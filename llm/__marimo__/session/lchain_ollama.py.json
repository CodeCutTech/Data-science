{
  "version": "1",
  "metadata": {
    "marimo_version": "0.13.7"
  },
  "cells": [
    {
      "id": "Hbol",
      "code_hash": "21aa5fe3971f8101af1bbadd28fe7a27",
      "outputs": [
        {
          "type": "error",
          "ename": "exception",
          "evalue": "No module named 'pandas'",
          "traceback": []
        }
      ],
      "console": [
        {
          "type": "stream",
          "name": "stderr",
          "text": "<span class=\"codehilite\"><div class=\"highlight\"><pre><span></span><span class=\"gt\">Traceback (most recent call last):</span>\n  File <span class=\"nb\">&quot;/Users/bexgboost/codecut/codecut-blog/.venv/lib/python3.12/site-packages/marimo/_runtime/executor.py&quot;</span>, line <span class=\"m\">122</span>, in <span class=\"n\">execute_cell</span>\n<span class=\"w\">    </span><span class=\"n\">exec</span><span class=\"p\">(</span><span class=\"n\">cell</span><span class=\"o\">.</span><span class=\"n\">body</span><span class=\"p\">,</span> <span class=\"n\">glbls</span><span class=\"p\">)</span>\n  File <span class=\"nb\">&quot;/var/folders/h6/xl7w9bcj3jz2p3s42_ygs4140000gn/T/marimo_7254/__marimo__cell_Hbol_.py&quot;</span>, line <span class=\"m\">2</span>, in <span class=\"n\">&lt;module&gt;</span>\n<span class=\"w\">    </span><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">pandas</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">pd</span>\n<span class=\"gr\">ModuleNotFoundError</span>: <span class=\"n\">No module named &#39;pandas&#39;</span>\n</pre></div>\n</span>"
        }
      ]
    },
    {
      "id": "MJUe",
      "code_hash": "681529fb42edb9e67d12f46e7c8de809",
      "outputs": [],
      "console": []
    },
    {
      "id": "vblA",
      "code_hash": "09179504e5ac1e6718f05a5c5340ba1f",
      "outputs": [],
      "console": []
    },
    {
      "id": "bkHC",
      "code_hash": "4b0b76fd52ba5c99d736cce9a1e64432",
      "outputs": [],
      "console": []
    },
    {
      "id": "DrOr",
      "code_hash": "1db60dbf222d46d5e10cca9fd8fefb0f",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/html": "<span class=\"markdown prose dark:prose-invert\"><h1 id=\"how-to-use-langchain-with-deepseek-and-ollama-step-by-step-integration-tutorial\">How to Use LangChain with DeepSeek and Ollama: Step-by-Step Integration Tutorial</h1>\n<h2 id=\"what-is-langchain-quick-overview\">What Is LangChain? (Quick Overview)</h2>\n<span class=\"paragraph\"><a href=\"https://www.langchain.com/\" rel=\"noopener\" target=\"_blank\">LangChain</a> is an open-source framework designed to make it easier to build applications powered by large language models (LLMs). It allows you to connect different components \u2014 like language models, tools, prompts, and memory \u2014 into structured, reusable pipelines.</span>\n<span class=\"paragraph\">In this guide, we'll walk you through how to use LangChain in combination with two powerful open-source model platforms:</span>\n<ul>\n<li><strong><a href=\"https://ollama.com/\" rel=\"noopener\" target=\"_blank\">Ollama</a></strong> \u2013 a local LLM runner for models like LLaMA 3, Mistral, and others, perfect for running models on your own machine.</li>\n<li><strong><a href=\"https://www.deepseek.com/\" rel=\"noopener\" target=\"_blank\">DeepSeek</a></strong> \u2013 a family of advanced transformer models known for strong reasoning, coding, and general-purpose performance.</li>\n</ul>\n<span class=\"paragraph\">You\u2019ll learn how to:</span>\n<ul>\n<li>Integrate <strong>LangChain with Ollama</strong> for local model serving</li>\n<li>Connect <strong>LangChain to DeepSeek</strong> via API or local inference</li>\n<li>Optionally combine <strong>LangChain, Ollama, and DeepSeek together</strong> in a single workflow for advanced use cases</li>\n</ul>\n<span class=\"paragraph\">Whether you came here for a <strong>LangChain Ollama tutorial</strong>, a <strong>LangChain DeepSeek tutorial</strong>, or a hybrid integration of all three, this article will guide you step-by-step.</span>\n<h2 id=\"introduction-to-ollama-and-deepseek\">Introduction to Ollama and DeepSeek</h2>\n<span class=\"paragraph\">Before diving into integration steps, let's understand the two key technologies we'll be working with in this tutorial.</span>\n<h3 id=\"what-is-ollama\">What is Ollama?</h3>\n<span class=\"paragraph\"><img alt=\"Ollama logo\" src=\"https://ollama.com/public/blog/embedding-models.png\" /></span>\n<span class=\"paragraph\">Ollama is an open-source framework designed to run large language models locally on your machine. It provides a simplified interface for downloading, running, and interacting with various open-source LLMs without needing extensive technical setup. Ollama handles the complex infrastructure requirements so developers can focus on using LLMs rather than managing them.</span>\n<ul>\n<li>Provides a simple CLI and REST API for running models locally</li>\n<li>Supports popular open-source models like DeepSeek, Llama, Mistral, and Gemma</li>\n<li>Optimized for consumer hardware with minimal setup requirements</li>\n<li>Offers customization through Modelfiles for fine-tuning behavior</li>\n</ul>\n<span class=\"paragraph\">With its focus on local execution, Ollama enables privacy-conscious applications and development workflows that don't depend on external APIs. This makes it particularly valuable for scenarios where data sensitivity is a concern or when working in environments with limited internet connectivity.</span>\n<h3 id=\"what-is-deepseek\">What is DeepSeek?</h3>\n<span class=\"paragraph\"><img alt=\"DeepSeek AI logo and visualization showing advanced language model capabilities for reasoning and code generation\" src=\"https://platform.theverge.com/wp-content/uploads/sites/2/chorus/uploads/chorus_asset/file/25848982/STKB320_DEEPSEEK_AI_CVIRGINIA_A.jpg?quality=90&amp;strip=all&amp;crop=0,0,100,100\" /></span>\n<span class=\"paragraph\">DeepSeek represents a family of transformer-based language models developed with a focus on reasoning capabilities and coding performance. With DeepSeek <a href=\"https://github.com/deepseek-ai/DeepSeek-R1\" rel=\"noopener\" target=\"_blank\">R1</a> and <a href=\"https://github.com/deepseek-ai/DeepSeek-V3\" rel=\"noopener\" target=\"_blank\">V3</a> garnering almost 200k stars on GitHub, DeepSeek AI has established itself as one of the leading open-source model ecosystems in the AI community. The project's popularity stems from its impressive performance across various benchmarks.</span>\n<ul>\n<li>DeepSeek-R1 models include both base versions and specialized distilled variants</li>\n<li>Achieves performance comparable to proprietary models on math, code, and reasoning tasks</li>\n<li>Released under MIT license that supports commercial use and modifications</li>\n<li>Available through Hugging Face and the DeepSeek platform</li>\n</ul>\n<span class=\"paragraph\">The DeepSeek model family continues to evolve, with ongoing research focused on improving reasoning capabilities through reinforcement learning approaches. By incorporating these models into LangChain workflows, developers can use their strengths in applications requiring complex reasoning or code generation while maintaining the flexibility of the LangChain framework.</span>\n<h2 id=\"langchain-ollama-integration-tutorial\">LangChain + Ollama: Integration Tutorial</h2>\n<span class=\"paragraph\">Now that we understand the core technologies, let's explore how to integrate LangChain with Ollama to run models locally.</span></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "IcVt",
      "code_hash": "dd2f1ed50392a05edea06155772fe8bc",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/html": "<span class=\"markdown prose dark:prose-invert\"><h3 id=\"installation-and-setup\">Installation and Setup</h3>\n<span class=\"paragraph\">First, install the required packages:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"n\">langchain</span> <span class=\"n\">langchain</span><span class=\"o\">-</span><span class=\"n\">community</span> <span class=\"n\">langchain</span><span class=\"o\">-</span><span class=\"n\">ollama</span>\n</code></pre></div>\n<span class=\"paragraph\">Ollama needs to be installed separately since it's a standalone service that runs locally:</span>\n<ul>\n<li>For macOS: Download from <a href=\"https://ollama.com\" rel=\"noopener\" target=\"_blank\">ollama.com</a> - this installs both the CLI tool and service</li>\n<li>For Linux: <code>curl -fsSL https://ollama.com/install.sh | sh</code> - this script sets up both the binary and system service</li>\n<li>For Windows: Download Windows (Preview) from <a href=\"https://ollama.com\" rel=\"noopener\" target=\"_blank\">ollama.com</a> - still in preview mode with some limitations</li>\n</ul>\n<span class=\"paragraph\">Ollama runs as a local server process that exposes a REST API on port 11434. This architecture allows any application to connect to it, not just the command line interface. When you start Ollama, it:</span>\n<ol>\n<li>Loads the GGUF model files from your local storage</li>\n<li>Creates an inference engine using optimized libraries like llama.cpp</li>\n<li>Exposes HTTP endpoints that LangChain will communicate with</li>\n</ol>\n<span class=\"paragraph\">Start the Ollama server:</span>\n<div class=\"codehilite\"><pre><span></span><code>ollama<span class=\"w\"> </span>serve\n</code></pre></div>\n<span class=\"paragraph\">The server will run in the background, handling model loading and inference requests. You can configure Ollama with environment variables:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"c1\"># Set custom model directory location</span>\n<span class=\"nv\">OLLAMA_MODELS</span><span class=\"o\">=</span>/path/to/models<span class=\"w\"> </span>ollama<span class=\"w\"> </span>serve\n\n<span class=\"c1\"># Limit GPU memory usage (in MiB)</span>\n<span class=\"nv\">OLLAMA_GPU_LAYERS</span><span class=\"o\">=</span><span class=\"m\">35</span><span class=\"w\"> </span>ollama<span class=\"w\"> </span>serve<span class=\"w\"> </span>\n</code></pre></div></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "FOzV",
      "code_hash": "89207e7e83c64f69805fac4f5069f9c4",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/html": "<span class=\"markdown prose dark:prose-invert\"></span>"
          }
        }
      ],
      "console": []
    },
    {
      "id": "Xbzj",
      "code_hash": "1d0db38904205bec4d6f6f6a1f6cec3e",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "SKll",
      "code_hash": "1d0db38904205bec4d6f6f6a1f6cec3e",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/plain": ""
          }
        }
      ],
      "console": []
    },
    {
      "id": "NdNv",
      "code_hash": "842dcbe047d64b15ea123da2253a28ee",
      "outputs": [
        {
          "type": "data",
          "data": {
            "text/html": "<span class=\"markdown prose dark:prose-invert\"><h3 id=\"pulling-models-with-ollama\">Pulling Models with Ollama</h3>\n<span class=\"paragraph\">Before using any model with LangChain, you need to pull it to your local machine:</span>\n<div class=\"codehilite\"><pre><span></span><code>ollama<span class=\"w\"> </span>pull<span class=\"w\"> </span>qwen3:0.6b\n</code></pre></div>\n<span class=\"paragraph\">When you run this command, Ollama:</span>\n<ol>\n<li>Downloads the model weights (often several GB in size)</li>\n<li>Optimizes the model for your specific hardware</li>\n<li>Stores the model in your local Ollama library (typically in <code>~/.ollama/models</code>)</li>\n</ol>\n<span class=\"paragraph\">Once it is downloaded, you can serve the model with the following command:</span>\n<div class=\"codehilite\"><pre><span></span><code>ollama<span class=\"w\"> </span>run<span class=\"w\"> </span>qwen3:0.6b\n</code></pre></div>\n<span class=\"paragraph\">Popular models with their characteristics:</span>\n<ul>\n<li><code>llama3</code> - Meta's Llama 3 model (8B parameters, good general purpose)</li>\n<li><code>llama3:70b</code> - Larger 70B parameter variant with stronger reasoning</li>\n<li><code>mistral</code> - Mistral AI's 7B parameter base model (efficient for its size)</li>\n<li><code>gemma:7b</code> - Google's Gemma model optimized for various tasks</li>\n<li><code>deepseek</code> - DeepSeek's model with strong code generation capabilities</li>\n<li><code>codellama</code> - Specialized for programming tasks and code completion</li>\n<li><code>nomic-embed-text</code> - Designed specifically for text embeddings</li>\n</ul>\n<span class=\"paragraph\">The model size has significant impact on performance and resource requirements:</span>\n<ul>\n<li>Smaller models (7B-8B) run well on most modern computers with 16GB+ RAM</li>\n<li>Medium models (13B-34B) need more RAM or GPU acceleration</li>\n<li>Large models (70B+) typically require a dedicated GPU with 24GB+ VRAM</li>\n</ul>\n<span class=\"paragraph\">For a full list of models you can serve locally, check out <a href=\"https://ollama.com/search\" rel=\"noopener\" target=\"_blank\">the Ollama model library</a>. Before pulling a model and potentially waste your hardware resources, check out <a href=\"https://apxml.com/tools/vram-calculator\" rel=\"noopener\" target=\"_blank\">the VRAM calculator</a> that tells you if you can run a specific model on your machine:</span>\n<span class=\"paragraph\"><img alt=\"VRAM Calculator showing memory requirements for different LLM models across various quantization levels\" src=\"images/vram.png\" /></span>\n<h3 id=\"basic-chat-integration-continue-from-here\">Basic Chat Integration - continue from here</h3>\n<span class=\"paragraph\">LangChain provides dedicated classes for working with Ollama chat models:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain_ollama</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">ChatOllama</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain_core.messages</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">HumanMessage</span><span class=\"p\">,</span> <span class=\"n\">SystemMessage</span>\n\n<span class=\"c1\"># Initialize the chat model with specific configurations</span>\n<span class=\"n\">chat_model</span> <span class=\"o\">=</span> <span class=\"n\">ChatOllama</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;qwen3:0.6b&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">temperature</span><span class=\"o\">=</span><span class=\"mf\">0.5</span><span class=\"p\">,</span>\n    <span class=\"n\">base_url</span><span class=\"o\">=</span><span class=\"s2\">&quot;http://localhost:11434&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># Can be changed for remote Ollama instances</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Create a conversation with system and user messages</span>\n<span class=\"n\">messages</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"n\">SystemMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"s2\">&quot;You are a helpful coding assistant specialized in Python.&quot;</span><span class=\"p\">),</span>\n    <span class=\"n\">HumanMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"s2\">&quot;Write a recursive Fibonacci function with memoization.&quot;</span><span class=\"p\">)</span>\n<span class=\"p\">]</span>\n\n<span class=\"c1\"># Invoke the model</span>\n<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">chat_model</span><span class=\"o\">.</span><span class=\"n\">invoke</span><span class=\"p\">(</span><span class=\"n\">messages</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">content</span><span class=\"p\">)</span>\n</code></pre></div>\n<span class=\"paragraph\">Make sure you have Ollama running on your machine and the model is already pulled and being served. If the model isn't available locally, Ollama will attempt to download it first, which may take some time depending on your internet connection and the model size.</span>\n<span class=\"paragraph\">Under the hood, <code>ChatOllama</code>:</span>\n<ol>\n<li>Converts LangChain message objects into Ollama API format</li>\n<li>Makes HTTP POST requests to the <code>/api/chat</code> endpoint</li>\n<li>Processes streaming responses when enabled</li>\n<li>Parses the response back into LangChain message objects</li>\n</ol>\n<span class=\"paragraph\">The <code>ChatOllama</code> class supports both synchronous and asynchronous operations, so for high-throughput applications, you can use:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">async</span> <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">generate_async</span><span class=\"p\">():</span>\n    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">chat_model</span><span class=\"o\">.</span><span class=\"n\">ainvoke</span><span class=\"p\">(</span><span class=\"n\">messages</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">content</span>\n\n<span class=\"c1\"># In async context</span>\n<span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">generate_async</span><span class=\"p\">()</span>\n</code></pre></div>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"nb\">print</span><span class=\"p\">(</span> <span class=\"n\">result</span><span class=\"p\">[:</span><span class=\"mi\">200</span><span class=\"p\">])</span>\n</code></pre></div>\n<div class=\"codehilite\"><pre><span></span><code>&lt;think&gt;\nOkay, I need to write a recursive Fibonacci function with memoization. Let me think about how to approach this. \n\nFirst, the Fibonacci sequence is defined such that each number is the sum of t\n</code></pre></div>\n<h3 id=\"using-completion-models\">Using Completion Models</h3>\n<span class=\"paragraph\">For traditional completion-style interactions, use the <code>Ollama</code> class:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain_ollama</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">OllamaLLM</span>\n\n<span class=\"c1\"># Initialize the LLM with specific options</span>\n<span class=\"n\">llm</span> <span class=\"o\">=</span> <span class=\"n\">OllamaLLM</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;qwen3:0.6b&quot;</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Generate text from a prompt</span>\n<span class=\"n\">text</span> <span class=\"o\">=</span> \\<span class=\"s2\">&quot;&quot;&quot;</span>\n<span class=\"s2\">Write a quick sort algorithm in Python with detailed comments:</span>\n<span class=\"s2\">```python</span>\n<span class=\"s2\">def quicksort(</span>\n<span class=\"se\">\\&quot;</span><span class=\"s2\">&quot;&quot;</span>\n\n<span class=\"s2\">response = llm.invoke(text)</span>\n<span class=\"s2\">print(response[:500])</span>\n</code></pre></div>\n<div class=\"codehilite\"><pre><span></span><code>&lt;think&gt;\nOkay, I need to write a quicksort algorithm in Python with detailed comments. Let me start by recalling how quicksort works. The basic idea is to choose a pivot element, partition the array into elements less than the pivot and greater than it, and then recursively sort each partition. The pivot can be chosen in different ways, like the first element, middle element, or random element.\n\nFirst, I should define the function signature. The parameters are the array, and maybe a left and ...\n</code></pre></div>\n<span class=\"paragraph\">The difference between <code>ChatOllama</code> and <code>OllamaLLM</code> classes:</span>\n<ul>\n<li><code>OllamaLLM</code> uses the <code>/api/generate</code> endpoint for text completion</li>\n<li><code>ChatOllama</code> uses the <code>/api/chat</code> endpoint for chat-style interactions</li>\n<li>Completion is better for code continuation, creative writing, and single-turn prompts</li>\n<li>Chat is better for multi-turn conversations and when using system prompts</li>\n</ul>\n<span class=\"paragraph\">For streaming responses (showing tokens as they're generated):</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">for</span> <span class=\"n\">chunk</span> <span class=\"ow\">in</span> <span class=\"n\">llm</span><span class=\"o\">.</span><span class=\"n\">stream</span><span class=\"p\">(</span><span class=\"s2\">&quot;Explain quantum computing in three sentences:&quot;</span><span class=\"p\">):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">chunk</span><span class=\"p\">,</span> <span class=\"n\">end</span><span class=\"o\">=</span><span class=\"s2\">&quot;&quot;</span><span class=\"p\">,</span> <span class=\"n\">flush</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</code></pre></div>\n<div class=\"codehilite\"><pre><span></span><code>&lt;think&gt;\nOkay, the user wants me to explain quantum computing in three sentences. Let me start by recalling what I know. Quantum computing uses qubits instead of classical bits. So first sentence should mention qubits and the difference from classical bits. Maybe say &quot;Quantum computing uses qubits, which can exist in multiple states at once, unlike classical bits that are either 0 or 1.&quot;\n\n...\n</code></pre></div>\n<h3 id=\"customizing-model-parameters\">Customizing Model Parameters</h3>\n<span class=\"paragraph\">Ollama offers fine-grained control over generation parameters:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">llm</span> <span class=\"o\">=</span> <span class=\"n\">OllamaLLM</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;deepseek&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">temperature</span><span class=\"o\">=</span><span class=\"mf\">0.7</span><span class=\"p\">,</span>      <span class=\"c1\"># Controls randomness (0.0 = deterministic, 1.0 = creative)</span>\n    <span class=\"n\">top_p</span><span class=\"o\">=</span><span class=\"mf\">0.9</span><span class=\"p\">,</span>            <span class=\"c1\"># Nucleus sampling parameter (lower = more focused)</span>\n    <span class=\"n\">top_k</span><span class=\"o\">=</span><span class=\"mi\">40</span><span class=\"p\">,</span>             <span class=\"c1\"># Limits vocabulary to top K tokens</span>\n    <span class=\"n\">num_ctx</span><span class=\"o\">=</span><span class=\"mi\">4096</span><span class=\"p\">,</span>         <span class=\"c1\"># Context window size in tokens</span>\n    <span class=\"n\">num_predict</span><span class=\"o\">=</span><span class=\"mi\">100</span><span class=\"p\">,</span>      <span class=\"c1\"># Maximum number of tokens to generate</span>\n    <span class=\"n\">stop</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">&quot;```&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;###&quot;</span><span class=\"p\">],</span>  <span class=\"c1\"># Stop sequences to end generation</span>\n    <span class=\"n\">repeat_penalty</span><span class=\"o\">=</span><span class=\"mf\">1.1</span><span class=\"p\">,</span>   <span class=\"c1\"># Penalizes repetition (&gt;1.0 reduces repetition)</span>\n    <span class=\"n\">num_thread</span><span class=\"o\">=</span><span class=\"mi\">4</span><span class=\"p\">,</span>         <span class=\"c1\"># CPU threads for computation</span>\n    <span class=\"n\">num_gpu</span><span class=\"o\">=</span><span class=\"mi\">1</span><span class=\"p\">,</span>            <span class=\"c1\"># Number of GPUs to use</span>\n    <span class=\"n\">mirostat</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>           <span class=\"c1\"># Alternative sampling method (0=disabled, 1=v1, 2=v2)</span>\n    <span class=\"n\">mirostat_eta</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span>     <span class=\"c1\"># Learning rate for mirostat</span>\n    <span class=\"n\">mirostat_tau</span><span class=\"o\">=</span><span class=\"mf\">5.0</span><span class=\"p\">,</span>     <span class=\"c1\"># Target entropy for mirostat</span>\n<span class=\"p\">)</span>\n</code></pre></div>\n<span class=\"paragraph\">Parameter recommendations:</span>\n<ul>\n<li>For factual or technical responses: Lower temperature (0.1-0.3) and higher <code>repeat_penalty</code> (1.1-1.2)</li>\n<li>For creative writing: Higher temperature (0.7-0.9) and lower <code>top_p</code> (0.8-0.9)</li>\n<li>For code generation: Medium temperature (0.3-0.6) with specific stop tokens like \"```\"</li>\n<li>For long-form content: Increase <code>num_predict</code> and use a larger <code>num_ctx</code></li>\n</ul>\n<span class=\"paragraph\">The model behavior changes dramatically with these settings. For example:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"c1\"># Scientific writing with precise output</span>\n<span class=\"n\">scientific_llm</span> <span class=\"o\">=</span> <span class=\"n\">Ollama</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;deepseek&quot;</span><span class=\"p\">,</span> <span class=\"n\">temperature</span><span class=\"o\">=</span><span class=\"mf\">0.1</span><span class=\"p\">,</span> <span class=\"n\">top_p</span><span class=\"o\">=</span><span class=\"mf\">0.95</span><span class=\"p\">,</span> <span class=\"n\">repeat_penalty</span><span class=\"o\">=</span><span class=\"mf\">1.2</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Creative storytelling</span>\n<span class=\"n\">creative_llm</span> <span class=\"o\">=</span> <span class=\"n\">Ollama</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;deepseek&quot;</span><span class=\"p\">,</span> <span class=\"n\">temperature</span><span class=\"o\">=</span><span class=\"mf\">0.9</span><span class=\"p\">,</span> <span class=\"n\">top_p</span><span class=\"o\">=</span><span class=\"mf\">0.8</span><span class=\"p\">,</span> <span class=\"n\">repeat_penalty</span><span class=\"o\">=</span><span class=\"mf\">1.0</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Code generation</span>\n<span class=\"n\">code_llm</span> <span class=\"o\">=</span> <span class=\"n\">Ollama</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;codellama&quot;</span><span class=\"p\">,</span> <span class=\"n\">temperature</span><span class=\"o\">=</span><span class=\"mf\">0.3</span><span class=\"p\">,</span> <span class=\"n\">top_p</span><span class=\"o\">=</span><span class=\"mf\">0.95</span><span class=\"p\">,</span> <span class=\"n\">stop</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">&quot;```&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;def &quot;</span><span class=\"p\">])</span>\n</code></pre></div>\n<h3 id=\"creating-langchain-chains\">Creating LangChain Chains</h3>\n<span class=\"paragraph\">LangChain's power comes from composing components into chains:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain_core.prompts</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">PromptTemplate</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain_core.output_parsers</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">StrOutputParser</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain_core.runnables</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">RunnablePassthrough</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain.schema</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">StrOutputParser</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">json</span>\n\n<span class=\"c1\"># Create a structured prompt template</span>\n<span class=\"n\">prompt</span> <span class=\"o\">=</span> <span class=\"n\">PromptTemplate</span><span class=\"o\">.</span><span class=\"n\">from_template</span><span class=\"p\">(</span>\\<span class=\"s2\">&quot;&quot;&quot;</span>\n<span class=\"s2\">You are an expert educator.</span>\n<span class=\"s2\">Explain the following concept in simple terms that a beginner would understand.</span>\n<span class=\"s2\">Make sure to provide:</span>\n<span class=\"s2\">1. A clear definition</span>\n<span class=\"s2\">2. A real-world analogy</span>\n<span class=\"s2\">3. A practical example</span>\n\n<span class=\"s2\">Concept: </span><span class=\"si\">{concept}</span>\n<span class=\"se\">\\&quot;</span><span class=\"s2\">&quot;&quot;)</span>\n\n<span class=\"s2\"># Create a parser that extracts structured data</span>\n<span class=\"s2\">class JsonOutputParser:</span>\n<span class=\"s2\">    def parse(self, text):</span>\n<span class=\"s2\">        try:</span>\n<span class=\"s2\">            # Find JSON blocks in the text</span>\n<span class=\"s2\">            if &quot;```json&quot; in text and &quot;```&quot; in text.split(&quot;```json&quot;)[1]:</span>\n<span class=\"s2\">                json_str = text.split(&quot;```json&quot;)[1].split(&quot;```&quot;)[0].strip()</span>\n<span class=\"s2\">                return json.loads(json_str)</span>\n<span class=\"s2\">            # Try to parse the whole text as JSON</span>\n<span class=\"s2\">            return json.loads(text)</span>\n<span class=\"s2\">        except:</span>\n<span class=\"s2\">            # Fall back to returning the raw text</span>\n<span class=\"s2\">            return {&quot;raw_output&quot;: text}</span>\n\n<span class=\"s2\"># Build a more complex chain</span>\n<span class=\"s2\">chain = (</span>\n<span class=\"s2\">    {&quot;concept&quot;: RunnablePassthrough()} </span>\n<span class=\"s2\">    | prompt </span>\n<span class=\"s2\">    | llm </span>\n<span class=\"s2\">    | StrOutputParser()</span>\n<span class=\"s2\">)</span>\n\n<span class=\"s2\"># Execute the chain with detailed tracking</span>\n<span class=\"s2\">result = chain.invoke(&quot;Recursive neural networks&quot;)</span>\n<span class=\"s2\">print(result[:500])</span>\n</code></pre></div>\n<div class=\"codehilite\"><pre><span></span><code>&lt;think&gt;\nOkay, so the user is asking for a simple explanation of recursive neural networks. Let me start by breaking down the concept. First, I need to define it clearly. Recursive neural networks... Hmm, I remember they&#39;re a type of neural network that can process data in multiple steps. Wait, maybe I should explain it as networks that can be broken down into smaller parts. Like, they can have multiple layers or multiple levels of processing. \n\nNow, the user wants a real-world analogy. Let me th...\n</code></pre></div>\n<span class=\"paragraph\">The chain architecture allows you to:</span>\n<ol>\n<li>Pre-process inputs before sending to the model</li>\n<li>Transform model outputs into structured data</li>\n<li>Chain multiple models together</li>\n<li>Add memory and context management</li>\n</ol>\n<span class=\"paragraph\">For more advanced use cases, create multi-step reasoning chains:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain.chains</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">SequentialChain</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain.chains</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">LLMChain</span>\n\n<span class=\"c1\"># First chain summarizes a concept</span>\n<span class=\"n\">summarize_prompt</span> <span class=\"o\">=</span> <span class=\"n\">PromptTemplate</span><span class=\"o\">.</span><span class=\"n\">from_template</span><span class=\"p\">(</span><span class=\"s2\">&quot;Summarize this concept: </span><span class=\"si\">{concept}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">summary_chain</span> <span class=\"o\">=</span> <span class=\"n\">LLMChain</span><span class=\"p\">(</span><span class=\"n\">llm</span><span class=\"o\">=</span><span class=\"n\">llm</span><span class=\"p\">,</span> <span class=\"n\">prompt</span><span class=\"o\">=</span><span class=\"n\">summarize_prompt</span><span class=\"p\">,</span> <span class=\"n\">output_key</span><span class=\"o\">=</span><span class=\"s2\">&quot;summary&quot;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Second chain explains it with examples</span>\n<span class=\"n\">explain_prompt</span> <span class=\"o\">=</span> <span class=\"n\">PromptTemplate</span><span class=\"o\">.</span><span class=\"n\">from_template</span><span class=\"p\">(</span><span class=\"s2\">&quot;Explain </span><span class=\"si\">{summary}</span><span class=\"s2\"> with examples&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">explanation_chain</span> <span class=\"o\">=</span> <span class=\"n\">LLMChain</span><span class=\"p\">(</span><span class=\"n\">llm</span><span class=\"o\">=</span><span class=\"n\">llm</span><span class=\"p\">,</span> <span class=\"n\">prompt</span><span class=\"o\">=</span><span class=\"n\">explain_prompt</span><span class=\"p\">,</span> <span class=\"n\">output_key</span><span class=\"o\">=</span><span class=\"s2\">&quot;explanation&quot;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Connect chains</span>\n<span class=\"n\">full_chain</span> <span class=\"o\">=</span> <span class=\"n\">SequentialChain</span><span class=\"p\">(</span>\n    <span class=\"n\">chains</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">summary_chain</span><span class=\"p\">,</span> <span class=\"n\">explanation_chain</span><span class=\"p\">],</span>\n    <span class=\"n\">input_variables</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">&quot;concept&quot;</span><span class=\"p\">],</span>\n    <span class=\"n\">output_variables</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"s2\">&quot;summary&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;explanation&quot;</span><span class=\"p\">]</span>\n<span class=\"p\">)</span>\n\n<span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">full_chain</span><span class=\"p\">({</span><span class=\"s2\">&quot;concept&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;Vector databases&quot;</span><span class=\"p\">})</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Summary: </span><span class=\"si\">{</span><span class=\"n\">result</span><span class=\"p\">[</span><span class=\"s1\">&#39;summary&#39;</span><span class=\"p\">]</span><span class=\"si\">}</span><span class=\"se\">\\n\\n</span><span class=\"s2\">Explanation: </span><span class=\"si\">{</span><span class=\"n\">result</span><span class=\"p\">[</span><span class=\"s1\">&#39;explanation&#39;</span><span class=\"p\">]</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n</code></pre></div>\n<h3 id=\"working-with-embeddings\">Working with Embeddings</h3>\n<span class=\"paragraph\">Embeddings convert text into numerical vectors that capture semantic meaning:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain_ollama</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">OllamaEmbeddings</span>\n<span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">numpy</span><span class=\"w\"> </span><span class=\"k\">as</span><span class=\"w\"> </span><span class=\"nn\">np</span>\n\n<span class=\"c1\"># Initialize embeddings model with specific parameters</span>\n<span class=\"n\">embeddings</span> <span class=\"o\">=</span> <span class=\"n\">OllamaEmbeddings</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;nomic-embed-text&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># Specialized embedding model that is also supported by Ollama</span>\n    <span class=\"n\">base_url</span><span class=\"o\">=</span><span class=\"s2\">&quot;http://localhost:11434&quot;</span><span class=\"p\">,</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Create embeddings for a query</span>\n<span class=\"n\">query</span> <span class=\"o\">=</span> <span class=\"s2\">&quot;How do neural networks learn?&quot;</span>\n<span class=\"n\">query_embedding</span> <span class=\"o\">=</span> <span class=\"n\">embeddings</span><span class=\"o\">.</span><span class=\"n\">embed_query</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Embedding dimension: </span><span class=\"si\">{</span><span class=\"nb\">len</span><span class=\"p\">(</span><span class=\"n\">query_embedding</span><span class=\"p\">)</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n</code></pre></div>\n<div class=\"codehilite\"><pre><span></span><code>Embedding dimension: 768\n</code></pre></div>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"c1\"># Create embeddings for multiple documents</span>\n<span class=\"n\">documents</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"s2\">&quot;Neural networks learn through backpropagation&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;Transformers use attention mechanisms&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;LLMs are trained on text data&quot;</span>\n<span class=\"p\">]</span>\n\n<span class=\"n\">doc_embeddings</span> <span class=\"o\">=</span> <span class=\"n\">embeddings</span><span class=\"o\">.</span><span class=\"n\">embed_documents</span><span class=\"p\">(</span><span class=\"n\">documents</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Calculate similarity between vectors</span>\n<span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">cosine_similarity</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">):</span>\n    <span class=\"k\">return</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">dot</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">,</span> <span class=\"n\">b</span><span class=\"p\">)</span> <span class=\"o\">/</span> <span class=\"p\">(</span><span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">a</span><span class=\"p\">)</span> <span class=\"o\">*</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">linalg</span><span class=\"o\">.</span><span class=\"n\">norm</span><span class=\"p\">(</span><span class=\"n\">b</span><span class=\"p\">))</span>\n\n<span class=\"c1\"># Find most similar document to query</span>\n<span class=\"n\">similarities</span> <span class=\"o\">=</span> <span class=\"p\">[</span><span class=\"n\">cosine_similarity</span><span class=\"p\">(</span><span class=\"n\">query_embedding</span><span class=\"p\">,</span> <span class=\"n\">doc_emb</span><span class=\"p\">)</span> <span class=\"k\">for</span> <span class=\"n\">doc_emb</span> <span class=\"ow\">in</span> <span class=\"n\">doc_embeddings</span><span class=\"p\">]</span>\n<span class=\"n\">most_similar_idx</span> <span class=\"o\">=</span> <span class=\"n\">np</span><span class=\"o\">.</span><span class=\"n\">argmax</span><span class=\"p\">(</span><span class=\"n\">similarities</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Most similar document: </span><span class=\"si\">{</span><span class=\"n\">documents</span><span class=\"p\">[</span><span class=\"n\">most_similar_idx</span><span class=\"p\">]</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n</code></pre></div>\n<div class=\"codehilite\"><pre><span></span><code>Most similar document: Neural networks learn through backpropagation\n</code></pre></div>\n<span class=\"paragraph\">These embeddings can be used for:</span>\n<ol>\n<li><strong>Semantic search</strong>: Find documents related to a query by meaning, not just keywords</li>\n<li><strong>Document clustering</strong>: Group similar documents together</li>\n<li><strong>Retrieval-Augmented Generation (RAG)</strong>: Retrieve relevant context before generating responses</li>\n<li><strong>Recommendation systems</strong>: Suggest similar items based on embeddings</li>\n</ol>\n<span class=\"paragraph\">When working with Ollama embeddings, it's important to understand:</span>\n<ul>\n<li>Different models produce embeddings with different dimensions (384, 768, 1024, etc.)</li>\n<li>The quality of embeddings varies by model (specialized embedding models usually perform better)</li>\n<li>Embedding generation is usually faster than text generation</li>\n</ul>\n<h3 id=\"common-issues-and-troubleshooting\">Common Issues and Troubleshooting</h3>\n<ol>\n<li>\n<span class=\"paragraph\"><strong>Ollama Server Not Running</strong>\n   - Error: <code>Failed to connect to Ollama server at http://localhost:11434/api/generate</code>\n   - Diagnosis: Check if the server is running with <code>ps aux | grep ollama</code>\n   - Solution: Start Ollama with <code>ollama serve</code> and ensure no firewall is blocking port 11434</span>\n</li>\n<li>\n<span class=\"paragraph\"><strong>Model Not Found</strong>\n   - Error: <code>no model found with name 'xyz'</code>\n   - Diagnosis: List available models with <code>ollama list</code>\n   - Solution: Pull the model first with <code>ollama pull xyz</code>\n   - Check model naming: Ollama is case-sensitive, so 'Llama' and 'llama' are different</span>\n</li>\n<li>\n<span class=\"paragraph\"><strong>Out of Memory</strong>\n   - Error: <code>Failed to load model: out of memory</code>\n   - Diagnosis: Check available system memory with <code>free -h</code> (Linux) or Activity Monitor (Mac)\n   - Solution:</span>\n<ul>\n<li>Try a smaller model (<code>llama3:8b</code> instead of <code>llama3:70b</code>)</li>\n<li>Reduce <code>num_ctx</code> parameter to use less memory</li>\n<li>Set <code>OLLAMA_GPU_LAYERS=0</code> to run only on CPU if GPU memory is insufficient</li>\n<li>Close other memory-intensive applications</li>\n</ul>\n</li>\n<li>\n<span class=\"paragraph\"><strong>Slow Responses</strong>\n   - Issue: First response after starting Ollama takes a long time\n   - Diagnosis: This is expected behavior as the model is loaded from disk into RAM/VRAM\n   - Solutions:</span>\n<ul>\n<li>For frequent use, keep Ollama running in the background</li>\n<li>Use <code>OLLAMA_KEEP_ALIVE=1h</code> to keep models in memory for one hour after last use</li>\n<li>For faster startup, use smaller models or quantized versions (e.g., <code>deepseek:7b-q4_0</code>)</li>\n</ul>\n</li>\n<li>\n<span class=\"paragraph\"><strong>Port Already in Use</strong>\n   - Error: <code>listen tcp 0.0.0.0:11434: bind: address already in use</code>\n   - Diagnosis: Find what's using the port with <code>lsof -i :11434</code> (Mac/Linux) or <code>netstat -ano | findstr 11434</code> (Windows)\n   - Solutions:</span>\n<ul>\n<li>Stop the existing Ollama process</li>\n<li>Use a different port with <code>OLLAMA_HOST=127.0.0.1:11435 ollama serve</code></li>\n<li>Update your LangChain code to use the new port: <code>base_url=\"http://localhost:11435\"</code></li>\n</ul>\n</li>\n<li>\n<span class=\"paragraph\"><strong>Incorrect Response Formats</strong>\n   - Issue: Model returns text when JSON was expected or vice versa\n   - Diagnosis: Check if your model supports the requested format and if your prompt clearly specifies the format\n   - Solution:</span>\n<ul>\n<li>Set <code>format=\"json\"</code> in ChatOllama constructor if the model supports it</li>\n<li>Add explicit format instructions in your prompt</li>\n<li>Use output parsers to structure the response regardless of format</li>\n</ul>\n</li>\n<li>\n<span class=\"paragraph\"><strong>Token Context Length Exceeded</strong>\n   - Error: <code>context window is full</code> or truncated responses\n   - Diagnosis: Your input + generated text is exceeding the model's context window\n   - Solutions:</span>\n<ul>\n<li>Reduce input prompt length</li>\n<li>Use a model with larger context window</li>\n<li>Split long documents into chunks</li>\n<li>Set appropriate <code>num_ctx</code> parameter (but requires more memory)</li>\n</ul>\n</li>\n</ol>\n<span class=\"paragraph\">By following this comprehensive tutorial, you now have the knowledge to use Ollama's local inference capabilities within LangChain's flexible framework for building sophisticated LLM-powered applications.</span>\n<h2 id=\"langchain-deepseek-integration-tutorial\">LangChain + DeepSeek: Integration Tutorial</h2>\n<span class=\"paragraph\">Now that we've explored how to use LangChain with Ollama for local model serving, let's turn our attention to DeepSeek. While Ollama gives us local model running capabilities, integrating with DeepSeek offers access to its specialized models known for reasoning capabilities and coding performance.</span>\n<h3 id=\"installation-and-setup-for-deepseek\">Installation and Setup For Deepseek</h3>\n<span class=\"paragraph\">First, install the required packages:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">pip</span> <span class=\"n\">install</span> <span class=\"n\">langchain</span> <span class=\"n\">langchain</span><span class=\"o\">-</span><span class=\"n\">deepseek</span> <span class=\"n\">python</span><span class=\"o\">-</span><span class=\"n\">dotenv</span>\n</code></pre></div>\n<span class=\"paragraph\">DeepSeek is primarily accessed through their API. You'll need to create a <a href=\"https://platform.deepseek.com\" rel=\"noopener\" target=\"_blank\">DeepSeek account</a>, generate an API key, and add a couple of dollars as balance. Once you have your API key, set it as an environment variable in a <code>.env</code> file:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"n\">touch</span> <span class=\"o\">.</span><span class=\"n\">env</span>\n\n<span class=\"c1\"># In the .env file</span>\n<span class=\"n\">DEEPSEEK_API_KEY</span><span class=\"o\">=</span><span class=\"s2\">&quot;sk-your-api-key-here&quot;</span>\n</code></pre></div>\n<h3 id=\"using-deepseek-chat-models\">Using DeepSeek Chat Models</h3>\n<span class=\"paragraph\">DeepSeek offers two main model families:</span>\n<ul>\n<li><strong>DeepSeek-V3</strong> (specified via <code>model=\"deepseek-chat\"</code>) - general purpose model with tool calling and structured output</li>\n<li><strong>DeepSeek-R1</strong> (specified via <code>model=\"deepseek-reasoner\"</code>) - specialized for reasoning tasks</li>\n</ul>\n<span class=\"paragraph\">Here's how to use them with LangChain:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">dotenv</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">load_dotenv</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain_deepseek</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">ChatDeepSeek</span>\n\n<span class=\"n\">load_dotenv</span><span class=\"p\">()</span>  <span class=\"c1\"># Load your api key</span>\n\n<span class=\"c1\"># Initialize the chat model</span>\n<span class=\"n\">llm</span> <span class=\"o\">=</span> <span class=\"n\">ChatDeepSeek</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;deepseek-chat&quot;</span><span class=\"p\">,</span>  <span class=\"c1\"># Can also use &quot;deepseek-reasoner&quot;</span>\n    <span class=\"n\">temperature</span><span class=\"o\">=</span><span class=\"mi\">0</span><span class=\"p\">,</span>  <span class=\"c1\"># 0 for more deterministic responses</span>\n    <span class=\"n\">max_tokens</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>  <span class=\"c1\"># None means model default</span>\n    <span class=\"n\">timeout</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">,</span>  <span class=\"c1\"># API request timeout</span>\n    <span class=\"n\">max_retries</span><span class=\"o\">=</span><span class=\"mi\">2</span>  <span class=\"c1\"># Retry failed requests</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Create a conversation with system and user messages</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain_core.messages</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">HumanMessage</span><span class=\"p\">,</span> <span class=\"n\">SystemMessage</span>\n\n<span class=\"n\">messages</span> <span class=\"o\">=</span> <span class=\"p\">[</span>\n    <span class=\"n\">SystemMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"s2\">&quot;You are a helpful assistant that translates English to French.&quot;</span><span class=\"p\">),</span>\n    <span class=\"n\">HumanMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"s2\">&quot;Translate this sentence: I love programming.&quot;</span><span class=\"p\">)</span>\n<span class=\"p\">]</span>\n\n<span class=\"c1\"># Generate a response</span>\n<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">llm</span><span class=\"o\">.</span><span class=\"n\">invoke</span><span class=\"p\">(</span><span class=\"n\">messages</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">content</span><span class=\"p\">)</span>\n</code></pre></div>\n<span class=\"paragraph\">For asynchronous operation, which is useful for handling multiple requests:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">async</span> <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">generate_async</span><span class=\"p\">():</span>\n    <span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">llm</span><span class=\"o\">.</span><span class=\"n\">ainvoke</span><span class=\"p\">(</span><span class=\"n\">messages</span><span class=\"p\">)</span>\n    <span class=\"k\">return</span> <span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">content</span>\n\n<span class=\"c1\"># In async context</span>\n<span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">generate_async</span><span class=\"p\">()</span>\n</code></pre></div>\n<h3 id=\"building-chains-with-deepseek\">Building Chains with DeepSeek</h3>\n<span class=\"paragraph\">LangChain's power comes from composing components into chains. Here's how to create a translation chain with DeepSeek:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain_core.prompts</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">ChatPromptTemplate</span>\n\n<span class=\"c1\"># Create a structured prompt template</span>\n<span class=\"n\">prompt</span> <span class=\"o\">=</span> <span class=\"n\">ChatPromptTemplate</span><span class=\"p\">(</span>\n    <span class=\"p\">[</span>\n        <span class=\"p\">(</span>\n            <span class=\"s2\">&quot;system&quot;</span><span class=\"p\">,</span>\n            <span class=\"s2\">&quot;You are a helpful assistant that translates </span><span class=\"si\">{input_language}</span><span class=\"s2\"> to </span><span class=\"si\">{output_language}</span><span class=\"s2\">.&quot;</span>\n        <span class=\"p\">),</span>\n        <span class=\"p\">(</span><span class=\"s2\">&quot;human&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;</span><span class=\"si\">{input}</span><span class=\"s2\">&quot;</span><span class=\"p\">),</span>\n    <span class=\"p\">]</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Build the chain</span>\n<span class=\"n\">chain</span> <span class=\"o\">=</span> <span class=\"n\">prompt</span> <span class=\"o\">|</span> <span class=\"n\">llm</span>\n\n<span class=\"c1\"># Execute the chain</span>\n<span class=\"n\">result</span> <span class=\"o\">=</span> <span class=\"n\">chain</span><span class=\"o\">.</span><span class=\"n\">invoke</span><span class=\"p\">({</span>\n    <span class=\"s2\">&quot;input_language&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;English&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;output_language&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;German&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;input&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;I love programming.&quot;</span>\n<span class=\"p\">})</span>\n\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">result</span><span class=\"o\">.</span><span class=\"n\">content</span><span class=\"p\">)</span>\n</code></pre></div>\n<h3 id=\"streaming-responses\">Streaming Responses</h3>\n<span class=\"paragraph\">For long-running generations, you might want to stream tokens as they're generated:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain_core.output_parsers</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">StrOutputParser</span>\n\n<span class=\"n\">streamed_chain</span> <span class=\"o\">=</span> <span class=\"n\">prompt</span> <span class=\"o\">|</span> <span class=\"n\">llm</span> <span class=\"o\">|</span> <span class=\"n\">StrOutputParser</span><span class=\"p\">()</span>\n\n<span class=\"k\">for</span> <span class=\"n\">chunk</span> <span class=\"ow\">in</span> <span class=\"n\">streamed_chain</span><span class=\"o\">.</span><span class=\"n\">stream</span><span class=\"p\">({</span>\n    <span class=\"s2\">&quot;input_language&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;English&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;output_language&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;Italian&quot;</span><span class=\"p\">,</span>\n    <span class=\"s2\">&quot;input&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;Machine learning is transforming the world.&quot;</span>\n<span class=\"p\">}):</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">chunk</span><span class=\"p\">,</span> <span class=\"n\">end</span><span class=\"o\">=</span><span class=\"s2\">&quot;&quot;</span><span class=\"p\">,</span> <span class=\"n\">flush</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">)</span>\n</code></pre></div>\n<h3 id=\"structured-output\">Structured Output</h3>\n<span class=\"paragraph\">When you need structured data instead of free text, you can use DeepSeek's structured output capability:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain_core.pydantic_v1</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">BaseModel</span><span class=\"p\">,</span> <span class=\"n\">Field</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">typing</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">List</span>\n\n<span class=\"c1\"># Define the output schema</span>\n<span class=\"k\">class</span><span class=\"w\"> </span><span class=\"nc\">MovieReview</span><span class=\"p\">(</span><span class=\"n\">BaseModel</span><span class=\"p\">):</span>\n    <span class=\"n\">title</span><span class=\"p\">:</span> <span class=\"nb\">str</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;The title of the movie&quot;</span><span class=\"p\">)</span>\n    <span class=\"n\">year</span><span class=\"p\">:</span> <span class=\"nb\">int</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;The year the movie was released&quot;</span><span class=\"p\">)</span>\n    <span class=\"n\">rating</span><span class=\"p\">:</span> <span class=\"nb\">float</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;Rating from 0-10&quot;</span><span class=\"p\">)</span>\n    <span class=\"n\">pros</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;List of positive aspects&quot;</span><span class=\"p\">)</span>\n    <span class=\"n\">cons</span><span class=\"p\">:</span> <span class=\"n\">List</span><span class=\"p\">[</span><span class=\"nb\">str</span><span class=\"p\">]</span> <span class=\"o\">=</span> <span class=\"n\">Field</span><span class=\"p\">(</span><span class=\"n\">description</span><span class=\"o\">=</span><span class=\"s2\">&quot;List of negative aspects&quot;</span><span class=\"p\">)</span>\n\n<span class=\"c1\"># Create a structured LLM</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain.output_parsers.openai_functions</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">PydanticOutputFunctionsParser</span>\n\n<span class=\"n\">structured_llm</span> <span class=\"o\">=</span> <span class=\"n\">llm</span><span class=\"o\">.</span><span class=\"n\">bind</span><span class=\"p\">(</span>\n    <span class=\"n\">functions</span><span class=\"o\">=</span><span class=\"p\">[</span><span class=\"n\">MovieReview</span><span class=\"p\">],</span>\n    <span class=\"n\">function_call</span><span class=\"o\">=</span><span class=\"p\">{</span><span class=\"s2\">&quot;name&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;MovieReview&quot;</span><span class=\"p\">}</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Create a chain</span>\n<span class=\"n\">chain</span> <span class=\"o\">=</span> <span class=\"p\">(</span>\n    <span class=\"n\">ChatPromptTemplate</span><span class=\"o\">.</span><span class=\"n\">from_messages</span><span class=\"p\">([</span>\n        <span class=\"p\">(</span><span class=\"s2\">&quot;system&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;You are a movie critic.&quot;</span><span class=\"p\">),</span>\n        <span class=\"p\">(</span><span class=\"s2\">&quot;human&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;Write a review for </span><span class=\"si\">{movie_title}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n    <span class=\"p\">])</span>\n    <span class=\"o\">|</span> <span class=\"n\">structured_llm</span>\n    <span class=\"o\">|</span> <span class=\"n\">PydanticOutputFunctionsParser</span><span class=\"p\">(</span><span class=\"n\">pydantic_schema</span><span class=\"o\">=</span><span class=\"n\">MovieReview</span><span class=\"p\">)</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Get structured output</span>\n<span class=\"n\">review</span> <span class=\"o\">=</span> <span class=\"n\">chain</span><span class=\"o\">.</span><span class=\"n\">invoke</span><span class=\"p\">({</span><span class=\"s2\">&quot;movie_title&quot;</span><span class=\"p\">:</span> <span class=\"s2\">&quot;The Matrix&quot;</span><span class=\"p\">})</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Title: </span><span class=\"si\">{</span><span class=\"n\">review</span><span class=\"o\">.</span><span class=\"n\">title</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Year: </span><span class=\"si\">{</span><span class=\"n\">review</span><span class=\"o\">.</span><span class=\"n\">year</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Rating: </span><span class=\"si\">{</span><span class=\"n\">review</span><span class=\"o\">.</span><span class=\"n\">rating</span><span class=\"si\">}</span><span class=\"s2\">/10&quot;</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Pros:&quot;</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">pro</span> <span class=\"ow\">in</span> <span class=\"n\">review</span><span class=\"o\">.</span><span class=\"n\">pros</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;- </span><span class=\"si\">{</span><span class=\"n\">pro</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"s2\">&quot;Cons:&quot;</span><span class=\"p\">)</span>\n<span class=\"k\">for</span> <span class=\"n\">con</span> <span class=\"ow\">in</span> <span class=\"n\">review</span><span class=\"o\">.</span><span class=\"n\">cons</span><span class=\"p\">:</span>\n    <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;- </span><span class=\"si\">{</span><span class=\"n\">con</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n</code></pre></div>\n<h3 id=\"common-issues-and-troubleshooting-with-deepseek\">Common Issues and Troubleshooting With DeepSeek</h3>\n<ol>\n<li>\n<span class=\"paragraph\"><strong>API Rate Limits</strong>\n   - Error: <code>429 Too Many Requests</code>\n   - Solution: Implement rate limiting in your code or upgrade your API plan</span>\n</li>\n<li>\n<span class=\"paragraph\"><strong>Authentication Errors</strong>\n   - Error: <code>401 Unauthorized</code>\n   - Solution: Check your API key is correct and properly set in the environment</span>\n</li>\n<li>\n<span class=\"paragraph\"><strong>Context Length Exceeded</strong>\n   - Error: <code>400 Bad Request: Maximum context length exceeded</code>\n   - Solution: Reduce your input length or try a model with a larger context window</span>\n</li>\n<li>\n<span class=\"paragraph\"><strong>Model Loading Time with Local Inference</strong>\n   - Issue: First request takes a long time\n   - Solution: This is normal as the model is loaded into memory; subsequent requests will be faster</span>\n</li>\n<li>\n<span class=\"paragraph\"><strong>Memory Issues with Local Models</strong>\n   - Error: <code>out of memory</code>\n   - Solution: Use a smaller model, reduce batch size, or upgrade your hardware</span>\n</li>\n<li>\n<span class=\"paragraph\"><strong>Structured Output Format Errors</strong>\n   - Issue: Model returns free text instead of structured output\n   - Solution: Use more explicit instructions in your prompt or try reducing temperature</span>\n</li>\n</ol>\n<span class=\"paragraph\">By following this guide, you now have the knowledge to use DeepSeek's powerful models within the LangChain framework, giving you access to state-of-the-art language capabilities for your applications.</span>\n<h2 id=\"combining-ollama-and-deepseek-in-langchain\">Combining Ollama and DeepSeek in LangChain</h2>\n<span class=\"paragraph\">Having explored both Ollama and DeepSeek individually, let's examine how to combine these powerful tools in a single LangChain workflow. One of the most practical setups is running DeepSeek locally through Ollama while maintaining the flexibility to call DeepSeek's API when needed.</span>\n<h3 id=\"running-deepseek-locally-with-ollama\">Running DeepSeek Locally with Ollama</h3>\n<span class=\"paragraph\">Ollama makes it simple to run DeepSeek models on your local machine:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"c1\"># Pull the DeepSeek model to your local machine</span>\nollama<span class=\"w\"> </span>pull<span class=\"w\"> </span>deepseek:7b\n</code></pre></div>\n<span class=\"paragraph\">Once downloaded, you can access it through LangChain just like any other Ollama model:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain_ollama</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">ChatOllama</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain_core.messages</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">HumanMessage</span>\n\n<span class=\"c1\"># Initialize the local DeepSeek model</span>\n<span class=\"n\">local_deepseek</span> <span class=\"o\">=</span> <span class=\"n\">ChatOllama</span><span class=\"p\">(</span>\n    <span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;deepseek:7b&quot;</span><span class=\"p\">,</span>\n    <span class=\"n\">temperature</span><span class=\"o\">=</span><span class=\"mf\">0.7</span><span class=\"p\">,</span>\n    <span class=\"n\">base_url</span><span class=\"o\">=</span><span class=\"s2\">&quot;http://localhost:11434&quot;</span>\n<span class=\"p\">)</span>\n\n<span class=\"c1\"># Use it just like any other LangChain model</span>\n<span class=\"n\">response</span> <span class=\"o\">=</span> <span class=\"n\">local_deepseek</span><span class=\"o\">.</span><span class=\"n\">invoke</span><span class=\"p\">([</span>\n    <span class=\"n\">HumanMessage</span><span class=\"p\">(</span><span class=\"n\">content</span><span class=\"o\">=</span><span class=\"s2\">&quot;Write a recursive function to calculate Fibonacci numbers.&quot;</span><span class=\"p\">)</span>\n<span class=\"p\">])</span>\n<span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"n\">response</span><span class=\"o\">.</span><span class=\"n\">content</span><span class=\"p\">)</span>\n</code></pre></div>\n<h3 id=\"why-use-both\">Why Use Both?</h3>\n<span class=\"paragraph\">Combining Ollama's local DeepSeek deployment with the official DeepSeek API gives you several advantages:</span>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain_deepseek</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">ChatDeepSeek</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain_ollama</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">ChatOllama</span>\n\n<span class=\"c1\"># Initialize both local and API models</span>\n<span class=\"n\">local_model</span> <span class=\"o\">=</span> <span class=\"n\">ChatOllama</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;deepseek:7b&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">cloud_model</span> <span class=\"o\">=</span> <span class=\"n\">ChatDeepSeek</span><span class=\"p\">(</span><span class=\"n\">model</span><span class=\"o\">=</span><span class=\"s2\">&quot;deepseek-chat&quot;</span><span class=\"p\">)</span>\n</code></pre></div>\n<ul>\n<li>\n<span class=\"paragraph\"><strong>Complementary strengths</strong>: Ollama offers speed and privacy for local inference, while DeepSeek's API provides access to their latest models with maximum accuracy and coding capabilities.</span>\n</li>\n<li>\n<span class=\"paragraph\"><strong>Fallback architecture</strong>: You can implement a fallback system that tries the local model first and defaults to the API if needed:</span>\n</li>\n</ul>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">get_response</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">,</span> <span class=\"n\">use_local</span><span class=\"o\">=</span><span class=\"kc\">True</span><span class=\"p\">):</span>\n    <span class=\"k\">try</span><span class=\"p\">:</span>\n        <span class=\"k\">if</span> <span class=\"n\">use_local</span><span class=\"p\">:</span>\n            <span class=\"k\">return</span> <span class=\"n\">local_model</span><span class=\"o\">.</span><span class=\"n\">invoke</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">)</span>\n        <span class=\"k\">else</span><span class=\"p\">:</span>\n            <span class=\"k\">return</span> <span class=\"n\">cloud_model</span><span class=\"o\">.</span><span class=\"n\">invoke</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">)</span>\n    <span class=\"k\">except</span> <span class=\"ne\">Exception</span> <span class=\"k\">as</span> <span class=\"n\">e</span><span class=\"p\">:</span>\n        <span class=\"nb\">print</span><span class=\"p\">(</span><span class=\"sa\">f</span><span class=\"s2\">&quot;Error with </span><span class=\"si\">{</span><span class=\"s1\">&#39;local&#39;</span><span class=\"w\"> </span><span class=\"k\">if</span><span class=\"w\"> </span><span class=\"n\">use_local</span><span class=\"w\"> </span><span class=\"k\">else</span><span class=\"w\"> </span><span class=\"s1\">&#39;cloud&#39;</span><span class=\"si\">}</span><span class=\"s2\"> model: </span><span class=\"si\">{</span><span class=\"n\">e</span><span class=\"si\">}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n        <span class=\"c1\"># Try the other model if the first one fails</span>\n        <span class=\"k\">return</span> <span class=\"n\">cloud_model</span><span class=\"o\">.</span><span class=\"n\">invoke</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">)</span> <span class=\"k\">if</span> <span class=\"n\">use_local</span> <span class=\"k\">else</span> <span class=\"n\">local_model</span><span class=\"o\">.</span><span class=\"n\">invoke</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">)</span>\n</code></pre></div>\n<ul>\n<li><strong>Cost optimization</strong>: Use local inference for development and testing, then switch to API calls for production or when higher quality is required.</li>\n</ul>\n<h3 id=\"performance-tips\">Performance Tips</h3>\n<span class=\"paragraph\">To get the most out of your hybrid Ollama-DeepSeek setup:</span>\n<ul>\n<li><strong>Prompt tuning per model</strong>: Different models respond better to different prompt formats. Create model-specific prompt templates:</li>\n</ul>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain_core.prompts</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">ChatPromptTemplate</span>\n\n<span class=\"c1\"># DeepSeek API model prompt template</span>\n<span class=\"n\">api_prompt</span> <span class=\"o\">=</span> <span class=\"n\">ChatPromptTemplate</span><span class=\"o\">.</span><span class=\"n\">from_messages</span><span class=\"p\">([</span>\n    <span class=\"p\">(</span><span class=\"s2\">&quot;system&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;You are DeepSeek, an advanced AI assistant with strong reasoning abilities.&quot;</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"s2\">&quot;human&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;</span><span class=\"si\">{input}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n<span class=\"p\">])</span>\n\n<span class=\"c1\"># Local Ollama model prompt template</span>\n<span class=\"n\">local_prompt</span> <span class=\"o\">=</span> <span class=\"n\">ChatPromptTemplate</span><span class=\"o\">.</span><span class=\"n\">from_messages</span><span class=\"p\">([</span>\n    <span class=\"p\">(</span><span class=\"s2\">&quot;system&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;You are a helpful coding assistant that gives concise answers.&quot;</span><span class=\"p\">),</span>\n    <span class=\"p\">(</span><span class=\"s2\">&quot;human&quot;</span><span class=\"p\">,</span> <span class=\"s2\">&quot;</span><span class=\"si\">{input}</span><span class=\"s2\">&quot;</span><span class=\"p\">)</span>\n<span class=\"p\">])</span>\n\n<span class=\"c1\"># Create model-specific chains</span>\n<span class=\"n\">api_chain</span> <span class=\"o\">=</span> <span class=\"n\">api_prompt</span> <span class=\"o\">|</span> <span class=\"n\">cloud_model</span>\n<span class=\"n\">local_chain</span> <span class=\"o\">=</span> <span class=\"n\">local_prompt</span> <span class=\"o\">|</span> <span class=\"n\">local_model</span>\n</code></pre></div>\n<ul>\n<li><strong>Parallel execution tradeoffs</strong>: You can run both models in parallel for critical tasks but be mindful of resource usage:</li>\n</ul>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">import</span><span class=\"w\"> </span><span class=\"nn\">asyncio</span>\n\n<span class=\"k\">async</span> <span class=\"k\">def</span><span class=\"w\"> </span><span class=\"nf\">get_both_responses</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">):</span>\n    <span class=\"c1\"># Run both models in parallel</span>\n    <span class=\"n\">local_task</span> <span class=\"o\">=</span> <span class=\"n\">local_model</span><span class=\"o\">.</span><span class=\"n\">ainvoke</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">)</span>\n    <span class=\"n\">cloud_task</span> <span class=\"o\">=</span> <span class=\"n\">cloud_model</span><span class=\"o\">.</span><span class=\"n\">ainvoke</span><span class=\"p\">(</span><span class=\"n\">query</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Wait for both to complete</span>\n    <span class=\"n\">local_response</span><span class=\"p\">,</span> <span class=\"n\">cloud_response</span> <span class=\"o\">=</span> <span class=\"k\">await</span> <span class=\"n\">asyncio</span><span class=\"o\">.</span><span class=\"n\">gather</span><span class=\"p\">(</span><span class=\"n\">local_task</span><span class=\"p\">,</span> <span class=\"n\">cloud_task</span><span class=\"p\">)</span>\n\n    <span class=\"c1\"># Compare or combine results</span>\n    <span class=\"k\">return</span> <span class=\"p\">{</span>\n        <span class=\"s2\">&quot;local&quot;</span><span class=\"p\">:</span> <span class=\"n\">local_response</span><span class=\"o\">.</span><span class=\"n\">content</span><span class=\"p\">,</span>\n        <span class=\"s2\">&quot;cloud&quot;</span><span class=\"p\">:</span> <span class=\"n\">cloud_response</span><span class=\"o\">.</span><span class=\"n\">content</span>\n    <span class=\"p\">}</span>\n</code></pre></div>\n<ul>\n<li><strong>Caching responses</strong>: Implement caching to avoid redundant computations and reduce API costs:</li>\n</ul>\n<div class=\"codehilite\"><pre><span></span><code><span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain.cache</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">InMemoryCache</span>\n<span class=\"kn\">from</span><span class=\"w\"> </span><span class=\"nn\">langchain.globals</span><span class=\"w\"> </span><span class=\"kn\">import</span> <span class=\"n\">set_llm_cache</span>\n\n<span class=\"c1\"># Set up caching</span>\n<span class=\"n\">set_llm_cache</span><span class=\"p\">(</span><span class=\"n\">InMemoryCache</span><span class=\"p\">())</span>\n\n<span class=\"c1\"># Now repeat queries will use cached responses</span>\n<span class=\"c1\"># This works for both local and API models</span>\n<span class=\"n\">response1</span> <span class=\"o\">=</span> <span class=\"n\">cloud_model</span><span class=\"o\">.</span><span class=\"n\">invoke</span><span class=\"p\">(</span><span class=\"s2\">&quot;What is quantum computing?&quot;</span><span class=\"p\">)</span>\n<span class=\"n\">response2</span> <span class=\"o\">=</span> <span class=\"n\">cloud_model</span><span class=\"o\">.</span><span class=\"n\">invoke</span><span class=\"p\">(</span><span class=\"s2\">&quot;What is quantum computing?&quot;</span><span class=\"p\">)</span>  <span class=\"c1\"># Uses cache</span>\n</code></pre></div>\n<span class=\"paragraph\">By thoughtfully combining Ollama and DeepSeek, you can create a flexible system that balances speed, cost, privacy, and quality based on your specific requirements. This hybrid approach gives you the best of both worlds while maintaining the composability and structure that LangChain provides.</span>\n<h2 id=\"final-thoughts-and-next-steps\">Final Thoughts And Next Steps</h2>\n<span class=\"paragraph\">In this tutorial, we've covered how to use LangChain with both Ollama and DeepSeek, giving you the tools to build powerful AI applications. You can now run models locally with Ollama for privacy and speed, access DeepSeek's advanced models through their API, or combine both approaches in a flexible workflow that meets your specific needs.</span>\n<span class=\"paragraph\">For your next steps, we recommend diving into the official documentation for <a href=\"https://www.langchain.com/\" rel=\"noopener\" target=\"_blank\">LangChain</a>, <a href=\"https://ollama.com\" rel=\"noopener\" target=\"_blank\">Ollama</a>, and <a href=\"https://www.deepseek.com/\" rel=\"noopener\" target=\"_blank\">DeepSeek</a>. Try building a simple project that chains these components together, such as a coding assistant that uses local models for rapid prototyping and API models for complex problems. As you gain comfort with these tools, you'll be able to create more advanced applications specific to your use cases.</span></span>"
          }
        }
      ],
      "console": []
    }
  ]
}