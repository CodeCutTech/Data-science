<!-- some code adapted from www.degeneratestate.org/static/metal_lyrics/metal_line.html -->
<!-- <!DOCTYPE html>
<meta content="utf-8"> -->
<style> /* set the CSS */

body {
  font: 12px Arial;
}

svg {
  font: 12px Helvetica;
}

path {
  stroke: steelblue;
  stroke-width: 2;
  fill: none;
}

.grid line {
  stroke: lightgrey;
  stroke-opacity: 0.4;
  shape-rendering: crispEdges;
}

.grid path {
  stroke-width: 0;
}

.axis path,
.axis lineper {
  fill: none;
  stroke: grey;
  stroke-width: 1;
  shape-rendering: crispEdges;
}

div.tooltip {
  position: absolute;
  text-align: center;
  width: 150px;
  height: 28px;
  padding: 2px;
  font: 12px sans-serif;
  background: lightsteelblue;
  border: 0px;
  border-radius: 8px;
  pointer-events: none;
}

div.tooltipscore {
  position: absolute;
  text-align: center;
  width: 150px;
  height: 50px;
  padding: 2px;
  font: 10px sans-serif;
  background: lightsteelblue;
  border: 0px;
  border-radius: 8px;
  pointer-events: none;
}

.category_header {
  font: 12px sans-serif;
  font-weight: bolder;
  text-decoration: underline;
}

div.label {
  color: rgb(252, 251, 253);
  color: rgb(63, 0, 125);
  color: rgb(158, 155, 201);

  position: absolute;
  text-align: left;
  padding: 1px;
  border-spacing: 1px;
  font: 10px sans-serif;
  font-family: Sans-Serif;
  border: 0;
  pointer-events: none;
}
/*
input {
  border: 1px dotted #ccc;
  background: white;
  font-family: monospace;
  padding: 10px 20px;
  font-size: 14px;
  margin: 20px 10px 30px 0;
  color: darkred;
}*/

.alert {
  font-family: monospace;
  padding: 10px 20px;
  font-size: 14px;
  margin: 20px 10px 30px 0;
  color: darkred;
}

ul.top_terms li {
  padding-right: 20px;
  font-size: 30pt;
  color: red;
}
/*
input:focus {
  background-color: lightyellow;
  outline: none;
}*/

.snippet {
  padding-bottom: 10px;
  padding-left: 5px;
  padding-right: 5px;
  white-space: pre-wrap;
}

.snippet_header {
  font-size: 20px;
  font-family: Helvetica, Arial, Sans-Serif;
  font-weight: bolder;
  #text-decoration: underline;
  text-align: center;
  border-bottom-width: 10px;
  border-bottom-color: #888888;
  padding-bottom: 10px;
}

.topic_preview {
  font-size: 12px;
  font-family: Helvetica, Arial, Sans-Serif;
  text-align: center;
  padding-bottom: 10px;
  font-weight: normal;
  text-decoration: none;
}


#d3-div-1-categoryinfo {
  font-size: 12px;
  font-family: Helvetica, Arial, Sans-Serif;
  text-align: center;
  padding-bottom: 10px;    

}


#d3-div-1-title-div {
  font-size: 20px;
  font-family: Helvetica, Arial, Sans-Serif;
  text-align: center;
}

.text_header {
  font: 18px sans-serif;
  font-size: 18px;
  font-family: Helvetica, Arial, Sans-Serif;

  font-weight: bolder;
  text-decoration: underline;
  text-align: center;
  color: darkblue;
  padding-bottom: 10px;
}

.text_subheader {
  font-size: 14px;
  font-family: Helvetica, Arial, Sans-Serif;

  text-align: center;
}

.snippet_meta {
  border-top: 3px solid #4588ba;
  font-size: 12px;
  font-family: Helvetica, Arial, Sans-Serif;
  color: darkblue;
}

.not_match {
    background-color: #F0F8FF;
}
    
.contexts {
  width: 45%;
  float: left;
}

.neut_display {
  display: none;
  float: left
}

.scattertext {
  font-size: 10px;
  font-family: Helvetica, Arial, Sans-Serif;
}

.label {
  font-size: 10px;
  font-family: Helvetica, Arial, Sans-Serif;
}

.obscured {
  /*font-size: 14px;
  font-weight: normal;
  color: dimgrey;
  font-family: Helvetica;*/
  text-align: center;
}

.small_label {
  font-size: 10px;
}

#d3-div-1-corpus-stats {
  text-align: center;
}

#d3-div-1-cat {
}

#d3-div-1-notcat {
}

#d3-div-1-neut {
}

#d3-div-1-neutcol {
  display: none;
}
/* Adapted from https://www.w3schools.com/howto/tryit.asp?filename=tryhow_js_autocomplete */

.autocomplete {
  position: relative;
  display: inline-block;
}

input {
  border: 1px solid transparent;
  background-color: #f1f1f1;
  padding: 10px;
  font-size: 16px;
}

input[type=text] {
  background-color: #f1f1f1;
  width: 100%;
}

input[type=submit] {
  background-color: DodgerBlue;
  color: #fff;
  cursor: pointer;
}

.autocomplete-items {
  position: absolute;
  border: 2px solid #d4d4d4;
  border-bottom: none;
  border-top: none;
  z-index: 99;
  /*position the autocomplete items to be the same width as the container:*/
  top: 100%;
  left: 0;
  right: 0;
}

.autocomplete-items div {
  padding: 10px;
  cursor: pointer;
  background-color: #fff;
  border-bottom: 2px solid #d4d4d4;
}

/*when hovering an item:*/
.autocomplete-items div:hover {
  background-color: #e9e9e9;
}

/*when navigating through the items using the arrow keys:*/
.autocomplete-active {
  background-color: DodgerBlue !important;
  color: #ffffff;
}
</style>

<script src="https://cdnjs.cloudflare.com/ajax/libs/d3/4.6.0/d3.min.js" charset="utf-8"></script>
<script src="https://d3js.org/d3-scale-chromatic.v1.min.js" charset="utf-8"></script>

<!-- INSERT SEMIOTIC SQUARE -->
<!--<a onclick="maxFreq = Math.log(data.map(d => d.cat + d.ncat).reduce((a,b) => Math.max(a,b))); plotInterface.redrawPoints(0.1, d => (Math.log(d.ncat + d.cat)/maxFreq), d => d.s, false); plotInterface.redrawPoints(0.1, d => (Math.log(d.ncat + d.cat)/maxFreq), d => d.s, true)">View Score Plot</a>-->
<span id="d3-div-1-title-div"></span>
<div class="scattertext" id="d3-div-1" style="float: left"></div>
<div style="floag: left;">
    <div autocomplete="off">
        <div class="autocomplete">
            <input id="searchInput" type="text" placeholder="Search the chart">
        </div>
    </div>
</div>
<br/>
<div id="d3-div-1-corpus-stats"></div>
<div id="d3-div-1-overlapped-terms"></div>
<a name="d3-div-1-snippets"></a>
<a name="d3-div-1-snippetsalt"></a>
<div id="d3-div-1-termstats"></div>
<div id="d3-div-1-overlapped-terms-clicked"></div>
<div id="d3-div-1-categoryinfo" style="display: hidden"></div>
<div id="d3-div-2">
  <div class="d3-div-1-contexts">
    <div class="snippet_header" id="d3-div-1-cathead"></div>
    <div class="snippet" id="d3-div-1-cat"></div>
  </div>
  <div id="d3-div-1-notcol" class="d3-div-1-contexts">
    <div class="snippet_header" id="d3-div-1-notcathead"></div>
    <div class="snippet" id="d3-div-1-notcat"></div>
  </div>
  <div id="d3-div-1-neutcol" class="d3-div-1-contexts">
    <div class="snippet_header" id="d3-div-1-neuthead"></div>
    <div class="snippet" id="d3-div-1-neut"></div>
  </div>
</div>
<script charset="utf-8">
    // Created using Cozy: github.com/uwplse/cozy
function Rectangle(ax1, ay1, ax2, ay2) {
    this.ax1 = ax1;
    this.ay1 = ay1;
    this.ax2 = ax2;
    this.ay2 = ay2;
    this._left7 = undefined;
    this._right8 = undefined;
    this._parent9 = undefined;
    this._min_ax12 = undefined;
    this._min_ay13 = undefined;
    this._max_ay24 = undefined;
    this._height10 = undefined;
}
function RectangleHolder() {
    this.my_size = 0;
    (this)._root1 = null;
}
RectangleHolder.prototype.size = function () {
    return this.my_size;
};
RectangleHolder.prototype.add = function (x) {
    ++this.my_size;
    var _idx69 = (x).ax2;
    (x)._left7 = null;
    (x)._right8 = null;
    (x)._min_ax12 = (x).ax1;
    (x)._min_ay13 = (x).ay1;
    (x)._max_ay24 = (x).ay2;
    (x)._height10 = 0;
    var _previous70 = null;
    var _current71 = (this)._root1;
    var _is_left72 = false;
    while (!((_current71) == null)) {
        _previous70 = _current71;
        if ((_idx69) < ((_current71).ax2)) {
            _current71 = (_current71)._left7;
            _is_left72 = true;
        } else {
            _current71 = (_current71)._right8;
            _is_left72 = false;
        }
    }
    if ((_previous70) == null) {
        (this)._root1 = x;
    } else {
        (x)._parent9 = _previous70;
        if (_is_left72) {
            (_previous70)._left7 = x;
        } else {
            (_previous70)._right8 = x;
        }
    }
    var _cursor73 = (x)._parent9;
    var _changed74 = true;
    while ((_changed74) && (!((_cursor73) == (null)))) {
        var _old__min_ax1275 = (_cursor73)._min_ax12;
        var _old__min_ay1376 = (_cursor73)._min_ay13;
        var _old__max_ay2477 = (_cursor73)._max_ay24;
        var _old_height78 = (_cursor73)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval79 = (_cursor73).ax1;
        var _child80 = (_cursor73)._left7;
        if (!((_child80) == null)) {
            var _val81 = (_child80)._min_ax12;
            _augval79 = ((_augval79) < (_val81)) ? (_augval79) : (_val81);
        }
        var _child82 = (_cursor73)._right8;
        if (!((_child82) == null)) {
            var _val83 = (_child82)._min_ax12;
            _augval79 = ((_augval79) < (_val83)) ? (_augval79) : (_val83);
        }
        (_cursor73)._min_ax12 = _augval79;
        /* _min_ay13 is min of ay1 */
        var _augval84 = (_cursor73).ay1;
        var _child85 = (_cursor73)._left7;
        if (!((_child85) == null)) {
            var _val86 = (_child85)._min_ay13;
            _augval84 = ((_augval84) < (_val86)) ? (_augval84) : (_val86);
        }
        var _child87 = (_cursor73)._right8;
        if (!((_child87) == null)) {
            var _val88 = (_child87)._min_ay13;
            _augval84 = ((_augval84) < (_val88)) ? (_augval84) : (_val88);
        }
        (_cursor73)._min_ay13 = _augval84;
        /* _max_ay24 is max of ay2 */
        var _augval89 = (_cursor73).ay2;
        var _child90 = (_cursor73)._left7;
        if (!((_child90) == null)) {
            var _val91 = (_child90)._max_ay24;
            _augval89 = ((_augval89) < (_val91)) ? (_val91) : (_augval89);
        }
        var _child92 = (_cursor73)._right8;
        if (!((_child92) == null)) {
            var _val93 = (_child92)._max_ay24;
            _augval89 = ((_augval89) < (_val93)) ? (_val93) : (_augval89);
        }
        (_cursor73)._max_ay24 = _augval89;
        (_cursor73)._height10 = 1 + ((((((_cursor73)._left7) == null) ? (-1) : (((_cursor73)._left7)._height10)) > ((((_cursor73)._right8) == null) ? (-1) : (((_cursor73)._right8)._height10))) ? ((((_cursor73)._left7) == null) ? (-1) : (((_cursor73)._left7)._height10)) : ((((_cursor73)._right8) == null) ? (-1) : (((_cursor73)._right8)._height10)));
        _changed74 = false;
        _changed74 = (_changed74) || (!((_old__min_ax1275) == ((_cursor73)._min_ax12)));
        _changed74 = (_changed74) || (!((_old__min_ay1376) == ((_cursor73)._min_ay13)));
        _changed74 = (_changed74) || (!((_old__max_ay2477) == ((_cursor73)._max_ay24)));
        _changed74 = (_changed74) || (!((_old_height78) == ((_cursor73)._height10)));
        _cursor73 = (_cursor73)._parent9;
    }
    /* rebalance AVL tree */
    var _cursor94 = x;
    var _imbalance95;
    while (!(((_cursor94)._parent9) == null)) {
        _cursor94 = (_cursor94)._parent9;
        (_cursor94)._height10 = 1 + ((((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) > ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10))) ? ((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) : ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10)));
        _imbalance95 = ((((_cursor94)._left7) == null) ? (-1) : (((_cursor94)._left7)._height10)) - ((((_cursor94)._right8) == null) ? (-1) : (((_cursor94)._right8)._height10));
        if ((_imbalance95) > (1)) {
            if ((((((_cursor94)._left7)._left7) == null) ? (-1) : ((((_cursor94)._left7)._left7)._height10)) < (((((_cursor94)._left7)._right8) == null) ? (-1) : ((((_cursor94)._left7)._right8)._height10))) {
                /* rotate ((_cursor94)._left7)._right8 */
                var _a96 = (_cursor94)._left7;
                var _b97 = (_a96)._right8;
                var _c98 = (_b97)._left7;
                /* replace _a96 with _b97 in (_a96)._parent9 */
                if (!(((_a96)._parent9) == null)) {
                    if ((((_a96)._parent9)._left7) == (_a96)) {
                        ((_a96)._parent9)._left7 = _b97;
                    } else {
                        ((_a96)._parent9)._right8 = _b97;
                    }
                }
                if (!((_b97) == null)) {
                    (_b97)._parent9 = (_a96)._parent9;
                }
                /* replace _c98 with _a96 in _b97 */
                (_b97)._left7 = _a96;
                if (!((_a96) == null)) {
                    (_a96)._parent9 = _b97;
                }
                /* replace _b97 with _c98 in _a96 */
                (_a96)._right8 = _c98;
                if (!((_c98) == null)) {
                    (_c98)._parent9 = _a96;
                }
                /* _min_ax12 is min of ax1 */
                var _augval99 = (_a96).ax1;
                var _child100 = (_a96)._left7;
                if (!((_child100) == null)) {
                    var _val101 = (_child100)._min_ax12;
                    _augval99 = ((_augval99) < (_val101)) ? (_augval99) : (_val101);
                }
                var _child102 = (_a96)._right8;
                if (!((_child102) == null)) {
                    var _val103 = (_child102)._min_ax12;
                    _augval99 = ((_augval99) < (_val103)) ? (_augval99) : (_val103);
                }
                (_a96)._min_ax12 = _augval99;
                /* _min_ay13 is min of ay1 */
                var _augval104 = (_a96).ay1;
                var _child105 = (_a96)._left7;
                if (!((_child105) == null)) {
                    var _val106 = (_child105)._min_ay13;
                    _augval104 = ((_augval104) < (_val106)) ? (_augval104) : (_val106);
                }
                var _child107 = (_a96)._right8;
                if (!((_child107) == null)) {
                    var _val108 = (_child107)._min_ay13;
                    _augval104 = ((_augval104) < (_val108)) ? (_augval104) : (_val108);
                }
                (_a96)._min_ay13 = _augval104;
                /* _max_ay24 is max of ay2 */
                var _augval109 = (_a96).ay2;
                var _child110 = (_a96)._left7;
                if (!((_child110) == null)) {
                    var _val111 = (_child110)._max_ay24;
                    _augval109 = ((_augval109) < (_val111)) ? (_val111) : (_augval109);
                }
                var _child112 = (_a96)._right8;
                if (!((_child112) == null)) {
                    var _val113 = (_child112)._max_ay24;
                    _augval109 = ((_augval109) < (_val113)) ? (_val113) : (_augval109);
                }
                (_a96)._max_ay24 = _augval109;
                (_a96)._height10 = 1 + ((((((_a96)._left7) == null) ? (-1) : (((_a96)._left7)._height10)) > ((((_a96)._right8) == null) ? (-1) : (((_a96)._right8)._height10))) ? ((((_a96)._left7) == null) ? (-1) : (((_a96)._left7)._height10)) : ((((_a96)._right8) == null) ? (-1) : (((_a96)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval114 = (_b97).ax1;
                var _child115 = (_b97)._left7;
                if (!((_child115) == null)) {
                    var _val116 = (_child115)._min_ax12;
                    _augval114 = ((_augval114) < (_val116)) ? (_augval114) : (_val116);
                }
                var _child117 = (_b97)._right8;
                if (!((_child117) == null)) {
                    var _val118 = (_child117)._min_ax12;
                    _augval114 = ((_augval114) < (_val118)) ? (_augval114) : (_val118);
                }
                (_b97)._min_ax12 = _augval114;
                /* _min_ay13 is min of ay1 */
                var _augval119 = (_b97).ay1;
                var _child120 = (_b97)._left7;
                if (!((_child120) == null)) {
                    var _val121 = (_child120)._min_ay13;
                    _augval119 = ((_augval119) < (_val121)) ? (_augval119) : (_val121);
                }
                var _child122 = (_b97)._right8;
                if (!((_child122) == null)) {
                    var _val123 = (_child122)._min_ay13;
                    _augval119 = ((_augval119) < (_val123)) ? (_augval119) : (_val123);
                }
                (_b97)._min_ay13 = _augval119;
                /* _max_ay24 is max of ay2 */
                var _augval124 = (_b97).ay2;
                var _child125 = (_b97)._left7;
                if (!((_child125) == null)) {
                    var _val126 = (_child125)._max_ay24;
                    _augval124 = ((_augval124) < (_val126)) ? (_val126) : (_augval124);
                }
                var _child127 = (_b97)._right8;
                if (!((_child127) == null)) {
                    var _val128 = (_child127)._max_ay24;
                    _augval124 = ((_augval124) < (_val128)) ? (_val128) : (_augval124);
                }
                (_b97)._max_ay24 = _augval124;
                (_b97)._height10 = 1 + ((((((_b97)._left7) == null) ? (-1) : (((_b97)._left7)._height10)) > ((((_b97)._right8) == null) ? (-1) : (((_b97)._right8)._height10))) ? ((((_b97)._left7) == null) ? (-1) : (((_b97)._left7)._height10)) : ((((_b97)._right8) == null) ? (-1) : (((_b97)._right8)._height10)));
                if (!(((_b97)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval129 = ((_b97)._parent9).ax1;
                    var _child130 = ((_b97)._parent9)._left7;
                    if (!((_child130) == null)) {
                        var _val131 = (_child130)._min_ax12;
                        _augval129 = ((_augval129) < (_val131)) ? (_augval129) : (_val131);
                    }
                    var _child132 = ((_b97)._parent9)._right8;
                    if (!((_child132) == null)) {
                        var _val133 = (_child132)._min_ax12;
                        _augval129 = ((_augval129) < (_val133)) ? (_augval129) : (_val133);
                    }
                    ((_b97)._parent9)._min_ax12 = _augval129;
                    /* _min_ay13 is min of ay1 */
                    var _augval134 = ((_b97)._parent9).ay1;
                    var _child135 = ((_b97)._parent9)._left7;
                    if (!((_child135) == null)) {
                        var _val136 = (_child135)._min_ay13;
                        _augval134 = ((_augval134) < (_val136)) ? (_augval134) : (_val136);
                    }
                    var _child137 = ((_b97)._parent9)._right8;
                    if (!((_child137) == null)) {
                        var _val138 = (_child137)._min_ay13;
                        _augval134 = ((_augval134) < (_val138)) ? (_augval134) : (_val138);
                    }
                    ((_b97)._parent9)._min_ay13 = _augval134;
                    /* _max_ay24 is max of ay2 */
                    var _augval139 = ((_b97)._parent9).ay2;
                    var _child140 = ((_b97)._parent9)._left7;
                    if (!((_child140) == null)) {
                        var _val141 = (_child140)._max_ay24;
                        _augval139 = ((_augval139) < (_val141)) ? (_val141) : (_augval139);
                    }
                    var _child142 = ((_b97)._parent9)._right8;
                    if (!((_child142) == null)) {
                        var _val143 = (_child142)._max_ay24;
                        _augval139 = ((_augval139) < (_val143)) ? (_val143) : (_augval139);
                    }
                    ((_b97)._parent9)._max_ay24 = _augval139;
                    ((_b97)._parent9)._height10 = 1 + (((((((_b97)._parent9)._left7) == null) ? (-1) : ((((_b97)._parent9)._left7)._height10)) > (((((_b97)._parent9)._right8) == null) ? (-1) : ((((_b97)._parent9)._right8)._height10))) ? (((((_b97)._parent9)._left7) == null) ? (-1) : ((((_b97)._parent9)._left7)._height10)) : (((((_b97)._parent9)._right8) == null) ? (-1) : ((((_b97)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b97;
                }
            }
            /* rotate (_cursor94)._left7 */
            var _a144 = _cursor94;
            var _b145 = (_a144)._left7;
            var _c146 = (_b145)._right8;
            /* replace _a144 with _b145 in (_a144)._parent9 */
            if (!(((_a144)._parent9) == null)) {
                if ((((_a144)._parent9)._left7) == (_a144)) {
                    ((_a144)._parent9)._left7 = _b145;
                } else {
                    ((_a144)._parent9)._right8 = _b145;
                }
            }
            if (!((_b145) == null)) {
                (_b145)._parent9 = (_a144)._parent9;
            }
            /* replace _c146 with _a144 in _b145 */
            (_b145)._right8 = _a144;
            if (!((_a144) == null)) {
                (_a144)._parent9 = _b145;
            }
            /* replace _b145 with _c146 in _a144 */
            (_a144)._left7 = _c146;
            if (!((_c146) == null)) {
                (_c146)._parent9 = _a144;
            }
            /* _min_ax12 is min of ax1 */
            var _augval147 = (_a144).ax1;
            var _child148 = (_a144)._left7;
            if (!((_child148) == null)) {
                var _val149 = (_child148)._min_ax12;
                _augval147 = ((_augval147) < (_val149)) ? (_augval147) : (_val149);
            }
            var _child150 = (_a144)._right8;
            if (!((_child150) == null)) {
                var _val151 = (_child150)._min_ax12;
                _augval147 = ((_augval147) < (_val151)) ? (_augval147) : (_val151);
            }
            (_a144)._min_ax12 = _augval147;
            /* _min_ay13 is min of ay1 */
            var _augval152 = (_a144).ay1;
            var _child153 = (_a144)._left7;
            if (!((_child153) == null)) {
                var _val154 = (_child153)._min_ay13;
                _augval152 = ((_augval152) < (_val154)) ? (_augval152) : (_val154);
            }
            var _child155 = (_a144)._right8;
            if (!((_child155) == null)) {
                var _val156 = (_child155)._min_ay13;
                _augval152 = ((_augval152) < (_val156)) ? (_augval152) : (_val156);
            }
            (_a144)._min_ay13 = _augval152;
            /* _max_ay24 is max of ay2 */
            var _augval157 = (_a144).ay2;
            var _child158 = (_a144)._left7;
            if (!((_child158) == null)) {
                var _val159 = (_child158)._max_ay24;
                _augval157 = ((_augval157) < (_val159)) ? (_val159) : (_augval157);
            }
            var _child160 = (_a144)._right8;
            if (!((_child160) == null)) {
                var _val161 = (_child160)._max_ay24;
                _augval157 = ((_augval157) < (_val161)) ? (_val161) : (_augval157);
            }
            (_a144)._max_ay24 = _augval157;
            (_a144)._height10 = 1 + ((((((_a144)._left7) == null) ? (-1) : (((_a144)._left7)._height10)) > ((((_a144)._right8) == null) ? (-1) : (((_a144)._right8)._height10))) ? ((((_a144)._left7) == null) ? (-1) : (((_a144)._left7)._height10)) : ((((_a144)._right8) == null) ? (-1) : (((_a144)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval162 = (_b145).ax1;
            var _child163 = (_b145)._left7;
            if (!((_child163) == null)) {
                var _val164 = (_child163)._min_ax12;
                _augval162 = ((_augval162) < (_val164)) ? (_augval162) : (_val164);
            }
            var _child165 = (_b145)._right8;
            if (!((_child165) == null)) {
                var _val166 = (_child165)._min_ax12;
                _augval162 = ((_augval162) < (_val166)) ? (_augval162) : (_val166);
            }
            (_b145)._min_ax12 = _augval162;
            /* _min_ay13 is min of ay1 */
            var _augval167 = (_b145).ay1;
            var _child168 = (_b145)._left7;
            if (!((_child168) == null)) {
                var _val169 = (_child168)._min_ay13;
                _augval167 = ((_augval167) < (_val169)) ? (_augval167) : (_val169);
            }
            var _child170 = (_b145)._right8;
            if (!((_child170) == null)) {
                var _val171 = (_child170)._min_ay13;
                _augval167 = ((_augval167) < (_val171)) ? (_augval167) : (_val171);
            }
            (_b145)._min_ay13 = _augval167;
            /* _max_ay24 is max of ay2 */
            var _augval172 = (_b145).ay2;
            var _child173 = (_b145)._left7;
            if (!((_child173) == null)) {
                var _val174 = (_child173)._max_ay24;
                _augval172 = ((_augval172) < (_val174)) ? (_val174) : (_augval172);
            }
            var _child175 = (_b145)._right8;
            if (!((_child175) == null)) {
                var _val176 = (_child175)._max_ay24;
                _augval172 = ((_augval172) < (_val176)) ? (_val176) : (_augval172);
            }
            (_b145)._max_ay24 = _augval172;
            (_b145)._height10 = 1 + ((((((_b145)._left7) == null) ? (-1) : (((_b145)._left7)._height10)) > ((((_b145)._right8) == null) ? (-1) : (((_b145)._right8)._height10))) ? ((((_b145)._left7) == null) ? (-1) : (((_b145)._left7)._height10)) : ((((_b145)._right8) == null) ? (-1) : (((_b145)._right8)._height10)));
            if (!(((_b145)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval177 = ((_b145)._parent9).ax1;
                var _child178 = ((_b145)._parent9)._left7;
                if (!((_child178) == null)) {
                    var _val179 = (_child178)._min_ax12;
                    _augval177 = ((_augval177) < (_val179)) ? (_augval177) : (_val179);
                }
                var _child180 = ((_b145)._parent9)._right8;
                if (!((_child180) == null)) {
                    var _val181 = (_child180)._min_ax12;
                    _augval177 = ((_augval177) < (_val181)) ? (_augval177) : (_val181);
                }
                ((_b145)._parent9)._min_ax12 = _augval177;
                /* _min_ay13 is min of ay1 */
                var _augval182 = ((_b145)._parent9).ay1;
                var _child183 = ((_b145)._parent9)._left7;
                if (!((_child183) == null)) {
                    var _val184 = (_child183)._min_ay13;
                    _augval182 = ((_augval182) < (_val184)) ? (_augval182) : (_val184);
                }
                var _child185 = ((_b145)._parent9)._right8;
                if (!((_child185) == null)) {
                    var _val186 = (_child185)._min_ay13;
                    _augval182 = ((_augval182) < (_val186)) ? (_augval182) : (_val186);
                }
                ((_b145)._parent9)._min_ay13 = _augval182;
                /* _max_ay24 is max of ay2 */
                var _augval187 = ((_b145)._parent9).ay2;
                var _child188 = ((_b145)._parent9)._left7;
                if (!((_child188) == null)) {
                    var _val189 = (_child188)._max_ay24;
                    _augval187 = ((_augval187) < (_val189)) ? (_val189) : (_augval187);
                }
                var _child190 = ((_b145)._parent9)._right8;
                if (!((_child190) == null)) {
                    var _val191 = (_child190)._max_ay24;
                    _augval187 = ((_augval187) < (_val191)) ? (_val191) : (_augval187);
                }
                ((_b145)._parent9)._max_ay24 = _augval187;
                ((_b145)._parent9)._height10 = 1 + (((((((_b145)._parent9)._left7) == null) ? (-1) : ((((_b145)._parent9)._left7)._height10)) > (((((_b145)._parent9)._right8) == null) ? (-1) : ((((_b145)._parent9)._right8)._height10))) ? (((((_b145)._parent9)._left7) == null) ? (-1) : ((((_b145)._parent9)._left7)._height10)) : (((((_b145)._parent9)._right8) == null) ? (-1) : ((((_b145)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b145;
            }
            _cursor94 = (_cursor94)._parent9;
        } else if ((_imbalance95) < (-1)) {
            if ((((((_cursor94)._right8)._left7) == null) ? (-1) : ((((_cursor94)._right8)._left7)._height10)) > (((((_cursor94)._right8)._right8) == null) ? (-1) : ((((_cursor94)._right8)._right8)._height10))) {
                /* rotate ((_cursor94)._right8)._left7 */
                var _a192 = (_cursor94)._right8;
                var _b193 = (_a192)._left7;
                var _c194 = (_b193)._right8;
                /* replace _a192 with _b193 in (_a192)._parent9 */
                if (!(((_a192)._parent9) == null)) {
                    if ((((_a192)._parent9)._left7) == (_a192)) {
                        ((_a192)._parent9)._left7 = _b193;
                    } else {
                        ((_a192)._parent9)._right8 = _b193;
                    }
                }
                if (!((_b193) == null)) {
                    (_b193)._parent9 = (_a192)._parent9;
                }
                /* replace _c194 with _a192 in _b193 */
                (_b193)._right8 = _a192;
                if (!((_a192) == null)) {
                    (_a192)._parent9 = _b193;
                }
                /* replace _b193 with _c194 in _a192 */
                (_a192)._left7 = _c194;
                if (!((_c194) == null)) {
                    (_c194)._parent9 = _a192;
                }
                /* _min_ax12 is min of ax1 */
                var _augval195 = (_a192).ax1;
                var _child196 = (_a192)._left7;
                if (!((_child196) == null)) {
                    var _val197 = (_child196)._min_ax12;
                    _augval195 = ((_augval195) < (_val197)) ? (_augval195) : (_val197);
                }
                var _child198 = (_a192)._right8;
                if (!((_child198) == null)) {
                    var _val199 = (_child198)._min_ax12;
                    _augval195 = ((_augval195) < (_val199)) ? (_augval195) : (_val199);
                }
                (_a192)._min_ax12 = _augval195;
                /* _min_ay13 is min of ay1 */
                var _augval200 = (_a192).ay1;
                var _child201 = (_a192)._left7;
                if (!((_child201) == null)) {
                    var _val202 = (_child201)._min_ay13;
                    _augval200 = ((_augval200) < (_val202)) ? (_augval200) : (_val202);
                }
                var _child203 = (_a192)._right8;
                if (!((_child203) == null)) {
                    var _val204 = (_child203)._min_ay13;
                    _augval200 = ((_augval200) < (_val204)) ? (_augval200) : (_val204);
                }
                (_a192)._min_ay13 = _augval200;
                /* _max_ay24 is max of ay2 */
                var _augval205 = (_a192).ay2;
                var _child206 = (_a192)._left7;
                if (!((_child206) == null)) {
                    var _val207 = (_child206)._max_ay24;
                    _augval205 = ((_augval205) < (_val207)) ? (_val207) : (_augval205);
                }
                var _child208 = (_a192)._right8;
                if (!((_child208) == null)) {
                    var _val209 = (_child208)._max_ay24;
                    _augval205 = ((_augval205) < (_val209)) ? (_val209) : (_augval205);
                }
                (_a192)._max_ay24 = _augval205;
                (_a192)._height10 = 1 + ((((((_a192)._left7) == null) ? (-1) : (((_a192)._left7)._height10)) > ((((_a192)._right8) == null) ? (-1) : (((_a192)._right8)._height10))) ? ((((_a192)._left7) == null) ? (-1) : (((_a192)._left7)._height10)) : ((((_a192)._right8) == null) ? (-1) : (((_a192)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval210 = (_b193).ax1;
                var _child211 = (_b193)._left7;
                if (!((_child211) == null)) {
                    var _val212 = (_child211)._min_ax12;
                    _augval210 = ((_augval210) < (_val212)) ? (_augval210) : (_val212);
                }
                var _child213 = (_b193)._right8;
                if (!((_child213) == null)) {
                    var _val214 = (_child213)._min_ax12;
                    _augval210 = ((_augval210) < (_val214)) ? (_augval210) : (_val214);
                }
                (_b193)._min_ax12 = _augval210;
                /* _min_ay13 is min of ay1 */
                var _augval215 = (_b193).ay1;
                var _child216 = (_b193)._left7;
                if (!((_child216) == null)) {
                    var _val217 = (_child216)._min_ay13;
                    _augval215 = ((_augval215) < (_val217)) ? (_augval215) : (_val217);
                }
                var _child218 = (_b193)._right8;
                if (!((_child218) == null)) {
                    var _val219 = (_child218)._min_ay13;
                    _augval215 = ((_augval215) < (_val219)) ? (_augval215) : (_val219);
                }
                (_b193)._min_ay13 = _augval215;
                /* _max_ay24 is max of ay2 */
                var _augval220 = (_b193).ay2;
                var _child221 = (_b193)._left7;
                if (!((_child221) == null)) {
                    var _val222 = (_child221)._max_ay24;
                    _augval220 = ((_augval220) < (_val222)) ? (_val222) : (_augval220);
                }
                var _child223 = (_b193)._right8;
                if (!((_child223) == null)) {
                    var _val224 = (_child223)._max_ay24;
                    _augval220 = ((_augval220) < (_val224)) ? (_val224) : (_augval220);
                }
                (_b193)._max_ay24 = _augval220;
                (_b193)._height10 = 1 + ((((((_b193)._left7) == null) ? (-1) : (((_b193)._left7)._height10)) > ((((_b193)._right8) == null) ? (-1) : (((_b193)._right8)._height10))) ? ((((_b193)._left7) == null) ? (-1) : (((_b193)._left7)._height10)) : ((((_b193)._right8) == null) ? (-1) : (((_b193)._right8)._height10)));
                if (!(((_b193)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval225 = ((_b193)._parent9).ax1;
                    var _child226 = ((_b193)._parent9)._left7;
                    if (!((_child226) == null)) {
                        var _val227 = (_child226)._min_ax12;
                        _augval225 = ((_augval225) < (_val227)) ? (_augval225) : (_val227);
                    }
                    var _child228 = ((_b193)._parent9)._right8;
                    if (!((_child228) == null)) {
                        var _val229 = (_child228)._min_ax12;
                        _augval225 = ((_augval225) < (_val229)) ? (_augval225) : (_val229);
                    }
                    ((_b193)._parent9)._min_ax12 = _augval225;
                    /* _min_ay13 is min of ay1 */
                    var _augval230 = ((_b193)._parent9).ay1;
                    var _child231 = ((_b193)._parent9)._left7;
                    if (!((_child231) == null)) {
                        var _val232 = (_child231)._min_ay13;
                        _augval230 = ((_augval230) < (_val232)) ? (_augval230) : (_val232);
                    }
                    var _child233 = ((_b193)._parent9)._right8;
                    if (!((_child233) == null)) {
                        var _val234 = (_child233)._min_ay13;
                        _augval230 = ((_augval230) < (_val234)) ? (_augval230) : (_val234);
                    }
                    ((_b193)._parent9)._min_ay13 = _augval230;
                    /* _max_ay24 is max of ay2 */
                    var _augval235 = ((_b193)._parent9).ay2;
                    var _child236 = ((_b193)._parent9)._left7;
                    if (!((_child236) == null)) {
                        var _val237 = (_child236)._max_ay24;
                        _augval235 = ((_augval235) < (_val237)) ? (_val237) : (_augval235);
                    }
                    var _child238 = ((_b193)._parent9)._right8;
                    if (!((_child238) == null)) {
                        var _val239 = (_child238)._max_ay24;
                        _augval235 = ((_augval235) < (_val239)) ? (_val239) : (_augval235);
                    }
                    ((_b193)._parent9)._max_ay24 = _augval235;
                    ((_b193)._parent9)._height10 = 1 + (((((((_b193)._parent9)._left7) == null) ? (-1) : ((((_b193)._parent9)._left7)._height10)) > (((((_b193)._parent9)._right8) == null) ? (-1) : ((((_b193)._parent9)._right8)._height10))) ? (((((_b193)._parent9)._left7) == null) ? (-1) : ((((_b193)._parent9)._left7)._height10)) : (((((_b193)._parent9)._right8) == null) ? (-1) : ((((_b193)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b193;
                }
            }
            /* rotate (_cursor94)._right8 */
            var _a240 = _cursor94;
            var _b241 = (_a240)._right8;
            var _c242 = (_b241)._left7;
            /* replace _a240 with _b241 in (_a240)._parent9 */
            if (!(((_a240)._parent9) == null)) {
                if ((((_a240)._parent9)._left7) == (_a240)) {
                    ((_a240)._parent9)._left7 = _b241;
                } else {
                    ((_a240)._parent9)._right8 = _b241;
                }
            }
            if (!((_b241) == null)) {
                (_b241)._parent9 = (_a240)._parent9;
            }
            /* replace _c242 with _a240 in _b241 */
            (_b241)._left7 = _a240;
            if (!((_a240) == null)) {
                (_a240)._parent9 = _b241;
            }
            /* replace _b241 with _c242 in _a240 */
            (_a240)._right8 = _c242;
            if (!((_c242) == null)) {
                (_c242)._parent9 = _a240;
            }
            /* _min_ax12 is min of ax1 */
            var _augval243 = (_a240).ax1;
            var _child244 = (_a240)._left7;
            if (!((_child244) == null)) {
                var _val245 = (_child244)._min_ax12;
                _augval243 = ((_augval243) < (_val245)) ? (_augval243) : (_val245);
            }
            var _child246 = (_a240)._right8;
            if (!((_child246) == null)) {
                var _val247 = (_child246)._min_ax12;
                _augval243 = ((_augval243) < (_val247)) ? (_augval243) : (_val247);
            }
            (_a240)._min_ax12 = _augval243;
            /* _min_ay13 is min of ay1 */
            var _augval248 = (_a240).ay1;
            var _child249 = (_a240)._left7;
            if (!((_child249) == null)) {
                var _val250 = (_child249)._min_ay13;
                _augval248 = ((_augval248) < (_val250)) ? (_augval248) : (_val250);
            }
            var _child251 = (_a240)._right8;
            if (!((_child251) == null)) {
                var _val252 = (_child251)._min_ay13;
                _augval248 = ((_augval248) < (_val252)) ? (_augval248) : (_val252);
            }
            (_a240)._min_ay13 = _augval248;
            /* _max_ay24 is max of ay2 */
            var _augval253 = (_a240).ay2;
            var _child254 = (_a240)._left7;
            if (!((_child254) == null)) {
                var _val255 = (_child254)._max_ay24;
                _augval253 = ((_augval253) < (_val255)) ? (_val255) : (_augval253);
            }
            var _child256 = (_a240)._right8;
            if (!((_child256) == null)) {
                var _val257 = (_child256)._max_ay24;
                _augval253 = ((_augval253) < (_val257)) ? (_val257) : (_augval253);
            }
            (_a240)._max_ay24 = _augval253;
            (_a240)._height10 = 1 + ((((((_a240)._left7) == null) ? (-1) : (((_a240)._left7)._height10)) > ((((_a240)._right8) == null) ? (-1) : (((_a240)._right8)._height10))) ? ((((_a240)._left7) == null) ? (-1) : (((_a240)._left7)._height10)) : ((((_a240)._right8) == null) ? (-1) : (((_a240)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval258 = (_b241).ax1;
            var _child259 = (_b241)._left7;
            if (!((_child259) == null)) {
                var _val260 = (_child259)._min_ax12;
                _augval258 = ((_augval258) < (_val260)) ? (_augval258) : (_val260);
            }
            var _child261 = (_b241)._right8;
            if (!((_child261) == null)) {
                var _val262 = (_child261)._min_ax12;
                _augval258 = ((_augval258) < (_val262)) ? (_augval258) : (_val262);
            }
            (_b241)._min_ax12 = _augval258;
            /* _min_ay13 is min of ay1 */
            var _augval263 = (_b241).ay1;
            var _child264 = (_b241)._left7;
            if (!((_child264) == null)) {
                var _val265 = (_child264)._min_ay13;
                _augval263 = ((_augval263) < (_val265)) ? (_augval263) : (_val265);
            }
            var _child266 = (_b241)._right8;
            if (!((_child266) == null)) {
                var _val267 = (_child266)._min_ay13;
                _augval263 = ((_augval263) < (_val267)) ? (_augval263) : (_val267);
            }
            (_b241)._min_ay13 = _augval263;
            /* _max_ay24 is max of ay2 */
            var _augval268 = (_b241).ay2;
            var _child269 = (_b241)._left7;
            if (!((_child269) == null)) {
                var _val270 = (_child269)._max_ay24;
                _augval268 = ((_augval268) < (_val270)) ? (_val270) : (_augval268);
            }
            var _child271 = (_b241)._right8;
            if (!((_child271) == null)) {
                var _val272 = (_child271)._max_ay24;
                _augval268 = ((_augval268) < (_val272)) ? (_val272) : (_augval268);
            }
            (_b241)._max_ay24 = _augval268;
            (_b241)._height10 = 1 + ((((((_b241)._left7) == null) ? (-1) : (((_b241)._left7)._height10)) > ((((_b241)._right8) == null) ? (-1) : (((_b241)._right8)._height10))) ? ((((_b241)._left7) == null) ? (-1) : (((_b241)._left7)._height10)) : ((((_b241)._right8) == null) ? (-1) : (((_b241)._right8)._height10)));
            if (!(((_b241)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval273 = ((_b241)._parent9).ax1;
                var _child274 = ((_b241)._parent9)._left7;
                if (!((_child274) == null)) {
                    var _val275 = (_child274)._min_ax12;
                    _augval273 = ((_augval273) < (_val275)) ? (_augval273) : (_val275);
                }
                var _child276 = ((_b241)._parent9)._right8;
                if (!((_child276) == null)) {
                    var _val277 = (_child276)._min_ax12;
                    _augval273 = ((_augval273) < (_val277)) ? (_augval273) : (_val277);
                }
                ((_b241)._parent9)._min_ax12 = _augval273;
                /* _min_ay13 is min of ay1 */
                var _augval278 = ((_b241)._parent9).ay1;
                var _child279 = ((_b241)._parent9)._left7;
                if (!((_child279) == null)) {
                    var _val280 = (_child279)._min_ay13;
                    _augval278 = ((_augval278) < (_val280)) ? (_augval278) : (_val280);
                }
                var _child281 = ((_b241)._parent9)._right8;
                if (!((_child281) == null)) {
                    var _val282 = (_child281)._min_ay13;
                    _augval278 = ((_augval278) < (_val282)) ? (_augval278) : (_val282);
                }
                ((_b241)._parent9)._min_ay13 = _augval278;
                /* _max_ay24 is max of ay2 */
                var _augval283 = ((_b241)._parent9).ay2;
                var _child284 = ((_b241)._parent9)._left7;
                if (!((_child284) == null)) {
                    var _val285 = (_child284)._max_ay24;
                    _augval283 = ((_augval283) < (_val285)) ? (_val285) : (_augval283);
                }
                var _child286 = ((_b241)._parent9)._right8;
                if (!((_child286) == null)) {
                    var _val287 = (_child286)._max_ay24;
                    _augval283 = ((_augval283) < (_val287)) ? (_val287) : (_augval283);
                }
                ((_b241)._parent9)._max_ay24 = _augval283;
                ((_b241)._parent9)._height10 = 1 + (((((((_b241)._parent9)._left7) == null) ? (-1) : ((((_b241)._parent9)._left7)._height10)) > (((((_b241)._parent9)._right8) == null) ? (-1) : ((((_b241)._parent9)._right8)._height10))) ? (((((_b241)._parent9)._left7) == null) ? (-1) : ((((_b241)._parent9)._left7)._height10)) : (((((_b241)._parent9)._right8) == null) ? (-1) : ((((_b241)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b241;
            }
            _cursor94 = (_cursor94)._parent9;
        }
    }
};
RectangleHolder.prototype.remove = function (x) {
    --this.my_size;
    var _parent288 = (x)._parent9;
    var _left289 = (x)._left7;
    var _right290 = (x)._right8;
    var _new_x291;
    if (((_left289) == null) && ((_right290) == null)) {
        _new_x291 = null;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else if ((!((_left289) == null)) && ((_right290) == null)) {
        _new_x291 = _left289;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else if (((_left289) == null) && (!((_right290) == null))) {
        _new_x291 = _right290;
        /* replace x with _new_x291 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _new_x291;
            } else {
                (_parent288)._right8 = _new_x291;
            }
        }
        if (!((_new_x291) == null)) {
            (_new_x291)._parent9 = _parent288;
        }
    } else {
        var _root292 = (x)._right8;
        var _x293 = _root292;
        var _descend294 = true;
        var _from_left295 = true;
        while (true) {
            if ((_x293) == null) {
                _x293 = null;
                break;
            }
            if (_descend294) {
                /* too small? */
                if (false) {
                    if ((!(((_x293)._right8) == null)) && (true)) {
                        if ((_x293) == (_root292)) {
                            _root292 = (_x293)._right8;
                        }
                        _x293 = (_x293)._right8;
                    } else if ((_x293) == (_root292)) {
                        _x293 = null;
                        break;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                } else if ((!(((_x293)._left7) == null)) && (true)) {
                    _x293 = (_x293)._left7;
                    /* too large? */
                } else if (false) {
                    if ((_x293) == (_root292)) {
                        _x293 = null;
                        break;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                    /* node ok? */
                } else if (true) {
                    break;
                } else if ((_x293) == (_root292)) {
                    _root292 = (_x293)._right8;
                    _x293 = (_x293)._right8;
                } else {
                    if ((!(((_x293)._right8) == null)) && (true)) {
                        if ((_x293) == (_root292)) {
                            _root292 = (_x293)._right8;
                        }
                        _x293 = (_x293)._right8;
                    } else {
                        _descend294 = false;
                        _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                        _x293 = (_x293)._parent9;
                    }
                }
            } else if (_from_left295) {
                if (false) {
                    _x293 = null;
                    break;
                } else if (true) {
                    break;
                } else if ((!(((_x293)._right8) == null)) && (true)) {
                    _descend294 = true;
                    if ((_x293) == (_root292)) {
                        _root292 = (_x293)._right8;
                    }
                    _x293 = (_x293)._right8;
                } else if ((_x293) == (_root292)) {
                    _x293 = null;
                    break;
                } else {
                    _descend294 = false;
                    _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                    _x293 = (_x293)._parent9;
                }
            } else {
                if ((_x293) == (_root292)) {
                    _x293 = null;
                    break;
                } else {
                    _descend294 = false;
                    _from_left295 = (!(((_x293)._parent9) == null)) && ((_x293) == (((_x293)._parent9)._left7));
                    _x293 = (_x293)._parent9;
                }
            }
        }
        _new_x291 = _x293;
        var _mp296 = (_x293)._parent9;
        var _mr297 = (_x293)._right8;
        /* replace _x293 with _mr297 in _mp296 */
        if (!((_mp296) == null)) {
            if (((_mp296)._left7) == (_x293)) {
                (_mp296)._left7 = _mr297;
            } else {
                (_mp296)._right8 = _mr297;
            }
        }
        if (!((_mr297) == null)) {
            (_mr297)._parent9 = _mp296;
        }
        /* replace x with _x293 in _parent288 */
        if (!((_parent288) == null)) {
            if (((_parent288)._left7) == (x)) {
                (_parent288)._left7 = _x293;
            } else {
                (_parent288)._right8 = _x293;
            }
        }
        if (!((_x293) == null)) {
            (_x293)._parent9 = _parent288;
        }
        /* replace null with _left289 in _x293 */
        (_x293)._left7 = _left289;
        if (!((_left289) == null)) {
            (_left289)._parent9 = _x293;
        }
        /* replace _mr297 with (x)._right8 in _x293 */
        (_x293)._right8 = (x)._right8;
        if (!(((x)._right8) == null)) {
            ((x)._right8)._parent9 = _x293;
        }
        /* _min_ax12 is min of ax1 */
        var _augval298 = (_x293).ax1;
        var _child299 = (_x293)._left7;
        if (!((_child299) == null)) {
            var _val300 = (_child299)._min_ax12;
            _augval298 = ((_augval298) < (_val300)) ? (_augval298) : (_val300);
        }
        var _child301 = (_x293)._right8;
        if (!((_child301) == null)) {
            var _val302 = (_child301)._min_ax12;
            _augval298 = ((_augval298) < (_val302)) ? (_augval298) : (_val302);
        }
        (_x293)._min_ax12 = _augval298;
        /* _min_ay13 is min of ay1 */
        var _augval303 = (_x293).ay1;
        var _child304 = (_x293)._left7;
        if (!((_child304) == null)) {
            var _val305 = (_child304)._min_ay13;
            _augval303 = ((_augval303) < (_val305)) ? (_augval303) : (_val305);
        }
        var _child306 = (_x293)._right8;
        if (!((_child306) == null)) {
            var _val307 = (_child306)._min_ay13;
            _augval303 = ((_augval303) < (_val307)) ? (_augval303) : (_val307);
        }
        (_x293)._min_ay13 = _augval303;
        /* _max_ay24 is max of ay2 */
        var _augval308 = (_x293).ay2;
        var _child309 = (_x293)._left7;
        if (!((_child309) == null)) {
            var _val310 = (_child309)._max_ay24;
            _augval308 = ((_augval308) < (_val310)) ? (_val310) : (_augval308);
        }
        var _child311 = (_x293)._right8;
        if (!((_child311) == null)) {
            var _val312 = (_child311)._max_ay24;
            _augval308 = ((_augval308) < (_val312)) ? (_val312) : (_augval308);
        }
        (_x293)._max_ay24 = _augval308;
        (_x293)._height10 = 1 + ((((((_x293)._left7) == null) ? (-1) : (((_x293)._left7)._height10)) > ((((_x293)._right8) == null) ? (-1) : (((_x293)._right8)._height10))) ? ((((_x293)._left7) == null) ? (-1) : (((_x293)._left7)._height10)) : ((((_x293)._right8) == null) ? (-1) : (((_x293)._right8)._height10)));
        var _cursor313 = _mp296;
        var _changed314 = true;
        while ((_changed314) && (!((_cursor313) == (_parent288)))) {
            var _old__min_ax12315 = (_cursor313)._min_ax12;
            var _old__min_ay13316 = (_cursor313)._min_ay13;
            var _old__max_ay24317 = (_cursor313)._max_ay24;
            var _old_height318 = (_cursor313)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval319 = (_cursor313).ax1;
            var _child320 = (_cursor313)._left7;
            if (!((_child320) == null)) {
                var _val321 = (_child320)._min_ax12;
                _augval319 = ((_augval319) < (_val321)) ? (_augval319) : (_val321);
            }
            var _child322 = (_cursor313)._right8;
            if (!((_child322) == null)) {
                var _val323 = (_child322)._min_ax12;
                _augval319 = ((_augval319) < (_val323)) ? (_augval319) : (_val323);
            }
            (_cursor313)._min_ax12 = _augval319;
            /* _min_ay13 is min of ay1 */
            var _augval324 = (_cursor313).ay1;
            var _child325 = (_cursor313)._left7;
            if (!((_child325) == null)) {
                var _val326 = (_child325)._min_ay13;
                _augval324 = ((_augval324) < (_val326)) ? (_augval324) : (_val326);
            }
            var _child327 = (_cursor313)._right8;
            if (!((_child327) == null)) {
                var _val328 = (_child327)._min_ay13;
                _augval324 = ((_augval324) < (_val328)) ? (_augval324) : (_val328);
            }
            (_cursor313)._min_ay13 = _augval324;
            /* _max_ay24 is max of ay2 */
            var _augval329 = (_cursor313).ay2;
            var _child330 = (_cursor313)._left7;
            if (!((_child330) == null)) {
                var _val331 = (_child330)._max_ay24;
                _augval329 = ((_augval329) < (_val331)) ? (_val331) : (_augval329);
            }
            var _child332 = (_cursor313)._right8;
            if (!((_child332) == null)) {
                var _val333 = (_child332)._max_ay24;
                _augval329 = ((_augval329) < (_val333)) ? (_val333) : (_augval329);
            }
            (_cursor313)._max_ay24 = _augval329;
            (_cursor313)._height10 = 1 + ((((((_cursor313)._left7) == null) ? (-1) : (((_cursor313)._left7)._height10)) > ((((_cursor313)._right8) == null) ? (-1) : (((_cursor313)._right8)._height10))) ? ((((_cursor313)._left7) == null) ? (-1) : (((_cursor313)._left7)._height10)) : ((((_cursor313)._right8) == null) ? (-1) : (((_cursor313)._right8)._height10)));
            _changed314 = false;
            _changed314 = (_changed314) || (!((_old__min_ax12315) == ((_cursor313)._min_ax12)));
            _changed314 = (_changed314) || (!((_old__min_ay13316) == ((_cursor313)._min_ay13)));
            _changed314 = (_changed314) || (!((_old__max_ay24317) == ((_cursor313)._max_ay24)));
            _changed314 = (_changed314) || (!((_old_height318) == ((_cursor313)._height10)));
            _cursor313 = (_cursor313)._parent9;
        }
    }
    var _cursor334 = _parent288;
    var _changed335 = true;
    while ((_changed335) && (!((_cursor334) == (null)))) {
        var _old__min_ax12336 = (_cursor334)._min_ax12;
        var _old__min_ay13337 = (_cursor334)._min_ay13;
        var _old__max_ay24338 = (_cursor334)._max_ay24;
        var _old_height339 = (_cursor334)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval340 = (_cursor334).ax1;
        var _child341 = (_cursor334)._left7;
        if (!((_child341) == null)) {
            var _val342 = (_child341)._min_ax12;
            _augval340 = ((_augval340) < (_val342)) ? (_augval340) : (_val342);
        }
        var _child343 = (_cursor334)._right8;
        if (!((_child343) == null)) {
            var _val344 = (_child343)._min_ax12;
            _augval340 = ((_augval340) < (_val344)) ? (_augval340) : (_val344);
        }
        (_cursor334)._min_ax12 = _augval340;
        /* _min_ay13 is min of ay1 */
        var _augval345 = (_cursor334).ay1;
        var _child346 = (_cursor334)._left7;
        if (!((_child346) == null)) {
            var _val347 = (_child346)._min_ay13;
            _augval345 = ((_augval345) < (_val347)) ? (_augval345) : (_val347);
        }
        var _child348 = (_cursor334)._right8;
        if (!((_child348) == null)) {
            var _val349 = (_child348)._min_ay13;
            _augval345 = ((_augval345) < (_val349)) ? (_augval345) : (_val349);
        }
        (_cursor334)._min_ay13 = _augval345;
        /* _max_ay24 is max of ay2 */
        var _augval350 = (_cursor334).ay2;
        var _child351 = (_cursor334)._left7;
        if (!((_child351) == null)) {
            var _val352 = (_child351)._max_ay24;
            _augval350 = ((_augval350) < (_val352)) ? (_val352) : (_augval350);
        }
        var _child353 = (_cursor334)._right8;
        if (!((_child353) == null)) {
            var _val354 = (_child353)._max_ay24;
            _augval350 = ((_augval350) < (_val354)) ? (_val354) : (_augval350);
        }
        (_cursor334)._max_ay24 = _augval350;
        (_cursor334)._height10 = 1 + ((((((_cursor334)._left7) == null) ? (-1) : (((_cursor334)._left7)._height10)) > ((((_cursor334)._right8) == null) ? (-1) : (((_cursor334)._right8)._height10))) ? ((((_cursor334)._left7) == null) ? (-1) : (((_cursor334)._left7)._height10)) : ((((_cursor334)._right8) == null) ? (-1) : (((_cursor334)._right8)._height10)));
        _changed335 = false;
        _changed335 = (_changed335) || (!((_old__min_ax12336) == ((_cursor334)._min_ax12)));
        _changed335 = (_changed335) || (!((_old__min_ay13337) == ((_cursor334)._min_ay13)));
        _changed335 = (_changed335) || (!((_old__max_ay24338) == ((_cursor334)._max_ay24)));
        _changed335 = (_changed335) || (!((_old_height339) == ((_cursor334)._height10)));
        _cursor334 = (_cursor334)._parent9;
    }
    if (((this)._root1) == (x)) {
        (this)._root1 = _new_x291;
    }
};
RectangleHolder.prototype.updateAx1 = function (__x, new_val) {
    if ((__x).ax1 != new_val) {
        /* _min_ax12 is min of ax1 */
        var _augval355 = new_val;
        var _child356 = (__x)._left7;
        if (!((_child356) == null)) {
            var _val357 = (_child356)._min_ax12;
            _augval355 = ((_augval355) < (_val357)) ? (_augval355) : (_val357);
        }
        var _child358 = (__x)._right8;
        if (!((_child358) == null)) {
            var _val359 = (_child358)._min_ax12;
            _augval355 = ((_augval355) < (_val359)) ? (_augval355) : (_val359);
        }
        (__x)._min_ax12 = _augval355;
        var _cursor360 = (__x)._parent9;
        var _changed361 = true;
        while ((_changed361) && (!((_cursor360) == (null)))) {
            var _old__min_ax12362 = (_cursor360)._min_ax12;
            var _old_height363 = (_cursor360)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval364 = (_cursor360).ax1;
            var _child365 = (_cursor360)._left7;
            if (!((_child365) == null)) {
                var _val366 = (_child365)._min_ax12;
                _augval364 = ((_augval364) < (_val366)) ? (_augval364) : (_val366);
            }
            var _child367 = (_cursor360)._right8;
            if (!((_child367) == null)) {
                var _val368 = (_child367)._min_ax12;
                _augval364 = ((_augval364) < (_val368)) ? (_augval364) : (_val368);
            }
            (_cursor360)._min_ax12 = _augval364;
            (_cursor360)._height10 = 1 + ((((((_cursor360)._left7) == null) ? (-1) : (((_cursor360)._left7)._height10)) > ((((_cursor360)._right8) == null) ? (-1) : (((_cursor360)._right8)._height10))) ? ((((_cursor360)._left7) == null) ? (-1) : (((_cursor360)._left7)._height10)) : ((((_cursor360)._right8) == null) ? (-1) : (((_cursor360)._right8)._height10)));
            _changed361 = false;
            _changed361 = (_changed361) || (!((_old__min_ax12362) == ((_cursor360)._min_ax12)));
            _changed361 = (_changed361) || (!((_old_height363) == ((_cursor360)._height10)));
            _cursor360 = (_cursor360)._parent9;
        }
        (__x).ax1 = new_val;
    }
}
RectangleHolder.prototype.updateAy1 = function (__x, new_val) {
    if ((__x).ay1 != new_val) {
        /* _min_ay13 is min of ay1 */
        var _augval369 = new_val;
        var _child370 = (__x)._left7;
        if (!((_child370) == null)) {
            var _val371 = (_child370)._min_ay13;
            _augval369 = ((_augval369) < (_val371)) ? (_augval369) : (_val371);
        }
        var _child372 = (__x)._right8;
        if (!((_child372) == null)) {
            var _val373 = (_child372)._min_ay13;
            _augval369 = ((_augval369) < (_val373)) ? (_augval369) : (_val373);
        }
        (__x)._min_ay13 = _augval369;
        var _cursor374 = (__x)._parent9;
        var _changed375 = true;
        while ((_changed375) && (!((_cursor374) == (null)))) {
            var _old__min_ay13376 = (_cursor374)._min_ay13;
            var _old_height377 = (_cursor374)._height10;
            /* _min_ay13 is min of ay1 */
            var _augval378 = (_cursor374).ay1;
            var _child379 = (_cursor374)._left7;
            if (!((_child379) == null)) {
                var _val380 = (_child379)._min_ay13;
                _augval378 = ((_augval378) < (_val380)) ? (_augval378) : (_val380);
            }
            var _child381 = (_cursor374)._right8;
            if (!((_child381) == null)) {
                var _val382 = (_child381)._min_ay13;
                _augval378 = ((_augval378) < (_val382)) ? (_augval378) : (_val382);
            }
            (_cursor374)._min_ay13 = _augval378;
            (_cursor374)._height10 = 1 + ((((((_cursor374)._left7) == null) ? (-1) : (((_cursor374)._left7)._height10)) > ((((_cursor374)._right8) == null) ? (-1) : (((_cursor374)._right8)._height10))) ? ((((_cursor374)._left7) == null) ? (-1) : (((_cursor374)._left7)._height10)) : ((((_cursor374)._right8) == null) ? (-1) : (((_cursor374)._right8)._height10)));
            _changed375 = false;
            _changed375 = (_changed375) || (!((_old__min_ay13376) == ((_cursor374)._min_ay13)));
            _changed375 = (_changed375) || (!((_old_height377) == ((_cursor374)._height10)));
            _cursor374 = (_cursor374)._parent9;
        }
        (__x).ay1 = new_val;
    }
}
RectangleHolder.prototype.updateAx2 = function (__x, new_val) {
    if ((__x).ax2 != new_val) {
        var _parent383 = (__x)._parent9;
        var _left384 = (__x)._left7;
        var _right385 = (__x)._right8;
        var _new_x386;
        if (((_left384) == null) && ((_right385) == null)) {
            _new_x386 = null;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else if ((!((_left384) == null)) && ((_right385) == null)) {
            _new_x386 = _left384;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else if (((_left384) == null) && (!((_right385) == null))) {
            _new_x386 = _right385;
            /* replace __x with _new_x386 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _new_x386;
                } else {
                    (_parent383)._right8 = _new_x386;
                }
            }
            if (!((_new_x386) == null)) {
                (_new_x386)._parent9 = _parent383;
            }
        } else {
            var _root387 = (__x)._right8;
            var _x388 = _root387;
            var _descend389 = true;
            var _from_left390 = true;
            while (true) {
                if ((_x388) == null) {
                    _x388 = null;
                    break;
                }
                if (_descend389) {
                    /* too small? */
                    if (false) {
                        if ((!(((_x388)._right8) == null)) && (true)) {
                            if ((_x388) == (_root387)) {
                                _root387 = (_x388)._right8;
                            }
                            _x388 = (_x388)._right8;
                        } else if ((_x388) == (_root387)) {
                            _x388 = null;
                            break;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                    } else if ((!(((_x388)._left7) == null)) && (true)) {
                        _x388 = (_x388)._left7;
                        /* too large? */
                    } else if (false) {
                        if ((_x388) == (_root387)) {
                            _x388 = null;
                            break;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                        /* node ok? */
                    } else if (true) {
                        break;
                    } else if ((_x388) == (_root387)) {
                        _root387 = (_x388)._right8;
                        _x388 = (_x388)._right8;
                    } else {
                        if ((!(((_x388)._right8) == null)) && (true)) {
                            if ((_x388) == (_root387)) {
                                _root387 = (_x388)._right8;
                            }
                            _x388 = (_x388)._right8;
                        } else {
                            _descend389 = false;
                            _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                            _x388 = (_x388)._parent9;
                        }
                    }
                } else if (_from_left390) {
                    if (false) {
                        _x388 = null;
                        break;
                    } else if (true) {
                        break;
                    } else if ((!(((_x388)._right8) == null)) && (true)) {
                        _descend389 = true;
                        if ((_x388) == (_root387)) {
                            _root387 = (_x388)._right8;
                        }
                        _x388 = (_x388)._right8;
                    } else if ((_x388) == (_root387)) {
                        _x388 = null;
                        break;
                    } else {
                        _descend389 = false;
                        _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                        _x388 = (_x388)._parent9;
                    }
                } else {
                    if ((_x388) == (_root387)) {
                        _x388 = null;
                        break;
                    } else {
                        _descend389 = false;
                        _from_left390 = (!(((_x388)._parent9) == null)) && ((_x388) == (((_x388)._parent9)._left7));
                        _x388 = (_x388)._parent9;
                    }
                }
            }
            _new_x386 = _x388;
            var _mp391 = (_x388)._parent9;
            var _mr392 = (_x388)._right8;
            /* replace _x388 with _mr392 in _mp391 */
            if (!((_mp391) == null)) {
                if (((_mp391)._left7) == (_x388)) {
                    (_mp391)._left7 = _mr392;
                } else {
                    (_mp391)._right8 = _mr392;
                }
            }
            if (!((_mr392) == null)) {
                (_mr392)._parent9 = _mp391;
            }
            /* replace __x with _x388 in _parent383 */
            if (!((_parent383) == null)) {
                if (((_parent383)._left7) == (__x)) {
                    (_parent383)._left7 = _x388;
                } else {
                    (_parent383)._right8 = _x388;
                }
            }
            if (!((_x388) == null)) {
                (_x388)._parent9 = _parent383;
            }
            /* replace null with _left384 in _x388 */
            (_x388)._left7 = _left384;
            if (!((_left384) == null)) {
                (_left384)._parent9 = _x388;
            }
            /* replace _mr392 with (__x)._right8 in _x388 */
            (_x388)._right8 = (__x)._right8;
            if (!(((__x)._right8) == null)) {
                ((__x)._right8)._parent9 = _x388;
            }
            /* _min_ax12 is min of ax1 */
            var _augval393 = (_x388).ax1;
            var _child394 = (_x388)._left7;
            if (!((_child394) == null)) {
                var _val395 = (_child394)._min_ax12;
                _augval393 = ((_augval393) < (_val395)) ? (_augval393) : (_val395);
            }
            var _child396 = (_x388)._right8;
            if (!((_child396) == null)) {
                var _val397 = (_child396)._min_ax12;
                _augval393 = ((_augval393) < (_val397)) ? (_augval393) : (_val397);
            }
            (_x388)._min_ax12 = _augval393;
            /* _min_ay13 is min of ay1 */
            var _augval398 = (_x388).ay1;
            var _child399 = (_x388)._left7;
            if (!((_child399) == null)) {
                var _val400 = (_child399)._min_ay13;
                _augval398 = ((_augval398) < (_val400)) ? (_augval398) : (_val400);
            }
            var _child401 = (_x388)._right8;
            if (!((_child401) == null)) {
                var _val402 = (_child401)._min_ay13;
                _augval398 = ((_augval398) < (_val402)) ? (_augval398) : (_val402);
            }
            (_x388)._min_ay13 = _augval398;
            /* _max_ay24 is max of ay2 */
            var _augval403 = (_x388).ay2;
            var _child404 = (_x388)._left7;
            if (!((_child404) == null)) {
                var _val405 = (_child404)._max_ay24;
                _augval403 = ((_augval403) < (_val405)) ? (_val405) : (_augval403);
            }
            var _child406 = (_x388)._right8;
            if (!((_child406) == null)) {
                var _val407 = (_child406)._max_ay24;
                _augval403 = ((_augval403) < (_val407)) ? (_val407) : (_augval403);
            }
            (_x388)._max_ay24 = _augval403;
            (_x388)._height10 = 1 + ((((((_x388)._left7) == null) ? (-1) : (((_x388)._left7)._height10)) > ((((_x388)._right8) == null) ? (-1) : (((_x388)._right8)._height10))) ? ((((_x388)._left7) == null) ? (-1) : (((_x388)._left7)._height10)) : ((((_x388)._right8) == null) ? (-1) : (((_x388)._right8)._height10)));
            var _cursor408 = _mp391;
            var _changed409 = true;
            while ((_changed409) && (!((_cursor408) == (_parent383)))) {
                var _old__min_ax12410 = (_cursor408)._min_ax12;
                var _old__min_ay13411 = (_cursor408)._min_ay13;
                var _old__max_ay24412 = (_cursor408)._max_ay24;
                var _old_height413 = (_cursor408)._height10;
                /* _min_ax12 is min of ax1 */
                var _augval414 = (_cursor408).ax1;
                var _child415 = (_cursor408)._left7;
                if (!((_child415) == null)) {
                    var _val416 = (_child415)._min_ax12;
                    _augval414 = ((_augval414) < (_val416)) ? (_augval414) : (_val416);
                }
                var _child417 = (_cursor408)._right8;
                if (!((_child417) == null)) {
                    var _val418 = (_child417)._min_ax12;
                    _augval414 = ((_augval414) < (_val418)) ? (_augval414) : (_val418);
                }
                (_cursor408)._min_ax12 = _augval414;
                /* _min_ay13 is min of ay1 */
                var _augval419 = (_cursor408).ay1;
                var _child420 = (_cursor408)._left7;
                if (!((_child420) == null)) {
                    var _val421 = (_child420)._min_ay13;
                    _augval419 = ((_augval419) < (_val421)) ? (_augval419) : (_val421);
                }
                var _child422 = (_cursor408)._right8;
                if (!((_child422) == null)) {
                    var _val423 = (_child422)._min_ay13;
                    _augval419 = ((_augval419) < (_val423)) ? (_augval419) : (_val423);
                }
                (_cursor408)._min_ay13 = _augval419;
                /* _max_ay24 is max of ay2 */
                var _augval424 = (_cursor408).ay2;
                var _child425 = (_cursor408)._left7;
                if (!((_child425) == null)) {
                    var _val426 = (_child425)._max_ay24;
                    _augval424 = ((_augval424) < (_val426)) ? (_val426) : (_augval424);
                }
                var _child427 = (_cursor408)._right8;
                if (!((_child427) == null)) {
                    var _val428 = (_child427)._max_ay24;
                    _augval424 = ((_augval424) < (_val428)) ? (_val428) : (_augval424);
                }
                (_cursor408)._max_ay24 = _augval424;
                (_cursor408)._height10 = 1 + ((((((_cursor408)._left7) == null) ? (-1) : (((_cursor408)._left7)._height10)) > ((((_cursor408)._right8) == null) ? (-1) : (((_cursor408)._right8)._height10))) ? ((((_cursor408)._left7) == null) ? (-1) : (((_cursor408)._left7)._height10)) : ((((_cursor408)._right8) == null) ? (-1) : (((_cursor408)._right8)._height10)));
                _changed409 = false;
                _changed409 = (_changed409) || (!((_old__min_ax12410) == ((_cursor408)._min_ax12)));
                _changed409 = (_changed409) || (!((_old__min_ay13411) == ((_cursor408)._min_ay13)));
                _changed409 = (_changed409) || (!((_old__max_ay24412) == ((_cursor408)._max_ay24)));
                _changed409 = (_changed409) || (!((_old_height413) == ((_cursor408)._height10)));
                _cursor408 = (_cursor408)._parent9;
            }
        }
        var _cursor429 = _parent383;
        var _changed430 = true;
        while ((_changed430) && (!((_cursor429) == (null)))) {
            var _old__min_ax12431 = (_cursor429)._min_ax12;
            var _old__min_ay13432 = (_cursor429)._min_ay13;
            var _old__max_ay24433 = (_cursor429)._max_ay24;
            var _old_height434 = (_cursor429)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval435 = (_cursor429).ax1;
            var _child436 = (_cursor429)._left7;
            if (!((_child436) == null)) {
                var _val437 = (_child436)._min_ax12;
                _augval435 = ((_augval435) < (_val437)) ? (_augval435) : (_val437);
            }
            var _child438 = (_cursor429)._right8;
            if (!((_child438) == null)) {
                var _val439 = (_child438)._min_ax12;
                _augval435 = ((_augval435) < (_val439)) ? (_augval435) : (_val439);
            }
            (_cursor429)._min_ax12 = _augval435;
            /* _min_ay13 is min of ay1 */
            var _augval440 = (_cursor429).ay1;
            var _child441 = (_cursor429)._left7;
            if (!((_child441) == null)) {
                var _val442 = (_child441)._min_ay13;
                _augval440 = ((_augval440) < (_val442)) ? (_augval440) : (_val442);
            }
            var _child443 = (_cursor429)._right8;
            if (!((_child443) == null)) {
                var _val444 = (_child443)._min_ay13;
                _augval440 = ((_augval440) < (_val444)) ? (_augval440) : (_val444);
            }
            (_cursor429)._min_ay13 = _augval440;
            /* _max_ay24 is max of ay2 */
            var _augval445 = (_cursor429).ay2;
            var _child446 = (_cursor429)._left7;
            if (!((_child446) == null)) {
                var _val447 = (_child446)._max_ay24;
                _augval445 = ((_augval445) < (_val447)) ? (_val447) : (_augval445);
            }
            var _child448 = (_cursor429)._right8;
            if (!((_child448) == null)) {
                var _val449 = (_child448)._max_ay24;
                _augval445 = ((_augval445) < (_val449)) ? (_val449) : (_augval445);
            }
            (_cursor429)._max_ay24 = _augval445;
            (_cursor429)._height10 = 1 + ((((((_cursor429)._left7) == null) ? (-1) : (((_cursor429)._left7)._height10)) > ((((_cursor429)._right8) == null) ? (-1) : (((_cursor429)._right8)._height10))) ? ((((_cursor429)._left7) == null) ? (-1) : (((_cursor429)._left7)._height10)) : ((((_cursor429)._right8) == null) ? (-1) : (((_cursor429)._right8)._height10)));
            _changed430 = false;
            _changed430 = (_changed430) || (!((_old__min_ax12431) == ((_cursor429)._min_ax12)));
            _changed430 = (_changed430) || (!((_old__min_ay13432) == ((_cursor429)._min_ay13)));
            _changed430 = (_changed430) || (!((_old__max_ay24433) == ((_cursor429)._max_ay24)));
            _changed430 = (_changed430) || (!((_old_height434) == ((_cursor429)._height10)));
            _cursor429 = (_cursor429)._parent9;
        }
        if (((this)._root1) == (__x)) {
            (this)._root1 = _new_x386;
        }
        (__x)._left7 = null;
        (__x)._right8 = null;
        (__x)._min_ax12 = (__x).ax1;
        (__x)._min_ay13 = (__x).ay1;
        (__x)._max_ay24 = (__x).ay2;
        (__x)._height10 = 0;
        var _previous450 = null;
        var _current451 = (this)._root1;
        var _is_left452 = false;
        while (!((_current451) == null)) {
            _previous450 = _current451;
            if ((new_val) < ((_current451).ax2)) {
                _current451 = (_current451)._left7;
                _is_left452 = true;
            } else {
                _current451 = (_current451)._right8;
                _is_left452 = false;
            }
        }
        if ((_previous450) == null) {
            (this)._root1 = __x;
        } else {
            (__x)._parent9 = _previous450;
            if (_is_left452) {
                (_previous450)._left7 = __x;
            } else {
                (_previous450)._right8 = __x;
            }
        }
        var _cursor453 = (__x)._parent9;
        var _changed454 = true;
        while ((_changed454) && (!((_cursor453) == (null)))) {
            var _old__min_ax12455 = (_cursor453)._min_ax12;
            var _old__min_ay13456 = (_cursor453)._min_ay13;
            var _old__max_ay24457 = (_cursor453)._max_ay24;
            var _old_height458 = (_cursor453)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval459 = (_cursor453).ax1;
            var _child460 = (_cursor453)._left7;
            if (!((_child460) == null)) {
                var _val461 = (_child460)._min_ax12;
                _augval459 = ((_augval459) < (_val461)) ? (_augval459) : (_val461);
            }
            var _child462 = (_cursor453)._right8;
            if (!((_child462) == null)) {
                var _val463 = (_child462)._min_ax12;
                _augval459 = ((_augval459) < (_val463)) ? (_augval459) : (_val463);
            }
            (_cursor453)._min_ax12 = _augval459;
            /* _min_ay13 is min of ay1 */
            var _augval464 = (_cursor453).ay1;
            var _child465 = (_cursor453)._left7;
            if (!((_child465) == null)) {
                var _val466 = (_child465)._min_ay13;
                _augval464 = ((_augval464) < (_val466)) ? (_augval464) : (_val466);
            }
            var _child467 = (_cursor453)._right8;
            if (!((_child467) == null)) {
                var _val468 = (_child467)._min_ay13;
                _augval464 = ((_augval464) < (_val468)) ? (_augval464) : (_val468);
            }
            (_cursor453)._min_ay13 = _augval464;
            /* _max_ay24 is max of ay2 */
            var _augval469 = (_cursor453).ay2;
            var _child470 = (_cursor453)._left7;
            if (!((_child470) == null)) {
                var _val471 = (_child470)._max_ay24;
                _augval469 = ((_augval469) < (_val471)) ? (_val471) : (_augval469);
            }
            var _child472 = (_cursor453)._right8;
            if (!((_child472) == null)) {
                var _val473 = (_child472)._max_ay24;
                _augval469 = ((_augval469) < (_val473)) ? (_val473) : (_augval469);
            }
            (_cursor453)._max_ay24 = _augval469;
            (_cursor453)._height10 = 1 + ((((((_cursor453)._left7) == null) ? (-1) : (((_cursor453)._left7)._height10)) > ((((_cursor453)._right8) == null) ? (-1) : (((_cursor453)._right8)._height10))) ? ((((_cursor453)._left7) == null) ? (-1) : (((_cursor453)._left7)._height10)) : ((((_cursor453)._right8) == null) ? (-1) : (((_cursor453)._right8)._height10)));
            _changed454 = false;
            _changed454 = (_changed454) || (!((_old__min_ax12455) == ((_cursor453)._min_ax12)));
            _changed454 = (_changed454) || (!((_old__min_ay13456) == ((_cursor453)._min_ay13)));
            _changed454 = (_changed454) || (!((_old__max_ay24457) == ((_cursor453)._max_ay24)));
            _changed454 = (_changed454) || (!((_old_height458) == ((_cursor453)._height10)));
            _cursor453 = (_cursor453)._parent9;
        }
        /* rebalance AVL tree */
        var _cursor474 = __x;
        var _imbalance475;
        while (!(((_cursor474)._parent9) == null)) {
            _cursor474 = (_cursor474)._parent9;
            (_cursor474)._height10 = 1 + ((((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) > ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10))) ? ((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) : ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10)));
            _imbalance475 = ((((_cursor474)._left7) == null) ? (-1) : (((_cursor474)._left7)._height10)) - ((((_cursor474)._right8) == null) ? (-1) : (((_cursor474)._right8)._height10));
            if ((_imbalance475) > (1)) {
                if ((((((_cursor474)._left7)._left7) == null) ? (-1) : ((((_cursor474)._left7)._left7)._height10)) < (((((_cursor474)._left7)._right8) == null) ? (-1) : ((((_cursor474)._left7)._right8)._height10))) {
                    /* rotate ((_cursor474)._left7)._right8 */
                    var _a476 = (_cursor474)._left7;
                    var _b477 = (_a476)._right8;
                    var _c478 = (_b477)._left7;
                    /* replace _a476 with _b477 in (_a476)._parent9 */
                    if (!(((_a476)._parent9) == null)) {
                        if ((((_a476)._parent9)._left7) == (_a476)) {
                            ((_a476)._parent9)._left7 = _b477;
                        } else {
                            ((_a476)._parent9)._right8 = _b477;
                        }
                    }
                    if (!((_b477) == null)) {
                        (_b477)._parent9 = (_a476)._parent9;
                    }
                    /* replace _c478 with _a476 in _b477 */
                    (_b477)._left7 = _a476;
                    if (!((_a476) == null)) {
                        (_a476)._parent9 = _b477;
                    }
                    /* replace _b477 with _c478 in _a476 */
                    (_a476)._right8 = _c478;
                    if (!((_c478) == null)) {
                        (_c478)._parent9 = _a476;
                    }
                    /* _min_ax12 is min of ax1 */
                    var _augval479 = (_a476).ax1;
                    var _child480 = (_a476)._left7;
                    if (!((_child480) == null)) {
                        var _val481 = (_child480)._min_ax12;
                        _augval479 = ((_augval479) < (_val481)) ? (_augval479) : (_val481);
                    }
                    var _child482 = (_a476)._right8;
                    if (!((_child482) == null)) {
                        var _val483 = (_child482)._min_ax12;
                        _augval479 = ((_augval479) < (_val483)) ? (_augval479) : (_val483);
                    }
                    (_a476)._min_ax12 = _augval479;
                    /* _min_ay13 is min of ay1 */
                    var _augval484 = (_a476).ay1;
                    var _child485 = (_a476)._left7;
                    if (!((_child485) == null)) {
                        var _val486 = (_child485)._min_ay13;
                        _augval484 = ((_augval484) < (_val486)) ? (_augval484) : (_val486);
                    }
                    var _child487 = (_a476)._right8;
                    if (!((_child487) == null)) {
                        var _val488 = (_child487)._min_ay13;
                        _augval484 = ((_augval484) < (_val488)) ? (_augval484) : (_val488);
                    }
                    (_a476)._min_ay13 = _augval484;
                    /* _max_ay24 is max of ay2 */
                    var _augval489 = (_a476).ay2;
                    var _child490 = (_a476)._left7;
                    if (!((_child490) == null)) {
                        var _val491 = (_child490)._max_ay24;
                        _augval489 = ((_augval489) < (_val491)) ? (_val491) : (_augval489);
                    }
                    var _child492 = (_a476)._right8;
                    if (!((_child492) == null)) {
                        var _val493 = (_child492)._max_ay24;
                        _augval489 = ((_augval489) < (_val493)) ? (_val493) : (_augval489);
                    }
                    (_a476)._max_ay24 = _augval489;
                    (_a476)._height10 = 1 + ((((((_a476)._left7) == null) ? (-1) : (((_a476)._left7)._height10)) > ((((_a476)._right8) == null) ? (-1) : (((_a476)._right8)._height10))) ? ((((_a476)._left7) == null) ? (-1) : (((_a476)._left7)._height10)) : ((((_a476)._right8) == null) ? (-1) : (((_a476)._right8)._height10)));
                    /* _min_ax12 is min of ax1 */
                    var _augval494 = (_b477).ax1;
                    var _child495 = (_b477)._left7;
                    if (!((_child495) == null)) {
                        var _val496 = (_child495)._min_ax12;
                        _augval494 = ((_augval494) < (_val496)) ? (_augval494) : (_val496);
                    }
                    var _child497 = (_b477)._right8;
                    if (!((_child497) == null)) {
                        var _val498 = (_child497)._min_ax12;
                        _augval494 = ((_augval494) < (_val498)) ? (_augval494) : (_val498);
                    }
                    (_b477)._min_ax12 = _augval494;
                    /* _min_ay13 is min of ay1 */
                    var _augval499 = (_b477).ay1;
                    var _child500 = (_b477)._left7;
                    if (!((_child500) == null)) {
                        var _val501 = (_child500)._min_ay13;
                        _augval499 = ((_augval499) < (_val501)) ? (_augval499) : (_val501);
                    }
                    var _child502 = (_b477)._right8;
                    if (!((_child502) == null)) {
                        var _val503 = (_child502)._min_ay13;
                        _augval499 = ((_augval499) < (_val503)) ? (_augval499) : (_val503);
                    }
                    (_b477)._min_ay13 = _augval499;
                    /* _max_ay24 is max of ay2 */
                    var _augval504 = (_b477).ay2;
                    var _child505 = (_b477)._left7;
                    if (!((_child505) == null)) {
                        var _val506 = (_child505)._max_ay24;
                        _augval504 = ((_augval504) < (_val506)) ? (_val506) : (_augval504);
                    }
                    var _child507 = (_b477)._right8;
                    if (!((_child507) == null)) {
                        var _val508 = (_child507)._max_ay24;
                        _augval504 = ((_augval504) < (_val508)) ? (_val508) : (_augval504);
                    }
                    (_b477)._max_ay24 = _augval504;
                    (_b477)._height10 = 1 + ((((((_b477)._left7) == null) ? (-1) : (((_b477)._left7)._height10)) > ((((_b477)._right8) == null) ? (-1) : (((_b477)._right8)._height10))) ? ((((_b477)._left7) == null) ? (-1) : (((_b477)._left7)._height10)) : ((((_b477)._right8) == null) ? (-1) : (((_b477)._right8)._height10)));
                    if (!(((_b477)._parent9) == null)) {
                        /* _min_ax12 is min of ax1 */
                        var _augval509 = ((_b477)._parent9).ax1;
                        var _child510 = ((_b477)._parent9)._left7;
                        if (!((_child510) == null)) {
                            var _val511 = (_child510)._min_ax12;
                            _augval509 = ((_augval509) < (_val511)) ? (_augval509) : (_val511);
                        }
                        var _child512 = ((_b477)._parent9)._right8;
                        if (!((_child512) == null)) {
                            var _val513 = (_child512)._min_ax12;
                            _augval509 = ((_augval509) < (_val513)) ? (_augval509) : (_val513);
                        }
                        ((_b477)._parent9)._min_ax12 = _augval509;
                        /* _min_ay13 is min of ay1 */
                        var _augval514 = ((_b477)._parent9).ay1;
                        var _child515 = ((_b477)._parent9)._left7;
                        if (!((_child515) == null)) {
                            var _val516 = (_child515)._min_ay13;
                            _augval514 = ((_augval514) < (_val516)) ? (_augval514) : (_val516);
                        }
                        var _child517 = ((_b477)._parent9)._right8;
                        if (!((_child517) == null)) {
                            var _val518 = (_child517)._min_ay13;
                            _augval514 = ((_augval514) < (_val518)) ? (_augval514) : (_val518);
                        }
                        ((_b477)._parent9)._min_ay13 = _augval514;
                        /* _max_ay24 is max of ay2 */
                        var _augval519 = ((_b477)._parent9).ay2;
                        var _child520 = ((_b477)._parent9)._left7;
                        if (!((_child520) == null)) {
                            var _val521 = (_child520)._max_ay24;
                            _augval519 = ((_augval519) < (_val521)) ? (_val521) : (_augval519);
                        }
                        var _child522 = ((_b477)._parent9)._right8;
                        if (!((_child522) == null)) {
                            var _val523 = (_child522)._max_ay24;
                            _augval519 = ((_augval519) < (_val523)) ? (_val523) : (_augval519);
                        }
                        ((_b477)._parent9)._max_ay24 = _augval519;
                        ((_b477)._parent9)._height10 = 1 + (((((((_b477)._parent9)._left7) == null) ? (-1) : ((((_b477)._parent9)._left7)._height10)) > (((((_b477)._parent9)._right8) == null) ? (-1) : ((((_b477)._parent9)._right8)._height10))) ? (((((_b477)._parent9)._left7) == null) ? (-1) : ((((_b477)._parent9)._left7)._height10)) : (((((_b477)._parent9)._right8) == null) ? (-1) : ((((_b477)._parent9)._right8)._height10)));
                    } else {
                        (this)._root1 = _b477;
                    }
                }
                /* rotate (_cursor474)._left7 */
                var _a524 = _cursor474;
                var _b525 = (_a524)._left7;
                var _c526 = (_b525)._right8;
                /* replace _a524 with _b525 in (_a524)._parent9 */
                if (!(((_a524)._parent9) == null)) {
                    if ((((_a524)._parent9)._left7) == (_a524)) {
                        ((_a524)._parent9)._left7 = _b525;
                    } else {
                        ((_a524)._parent9)._right8 = _b525;
                    }
                }
                if (!((_b525) == null)) {
                    (_b525)._parent9 = (_a524)._parent9;
                }
                /* replace _c526 with _a524 in _b525 */
                (_b525)._right8 = _a524;
                if (!((_a524) == null)) {
                    (_a524)._parent9 = _b525;
                }
                /* replace _b525 with _c526 in _a524 */
                (_a524)._left7 = _c526;
                if (!((_c526) == null)) {
                    (_c526)._parent9 = _a524;
                }
                /* _min_ax12 is min of ax1 */
                var _augval527 = (_a524).ax1;
                var _child528 = (_a524)._left7;
                if (!((_child528) == null)) {
                    var _val529 = (_child528)._min_ax12;
                    _augval527 = ((_augval527) < (_val529)) ? (_augval527) : (_val529);
                }
                var _child530 = (_a524)._right8;
                if (!((_child530) == null)) {
                    var _val531 = (_child530)._min_ax12;
                    _augval527 = ((_augval527) < (_val531)) ? (_augval527) : (_val531);
                }
                (_a524)._min_ax12 = _augval527;
                /* _min_ay13 is min of ay1 */
                var _augval532 = (_a524).ay1;
                var _child533 = (_a524)._left7;
                if (!((_child533) == null)) {
                    var _val534 = (_child533)._min_ay13;
                    _augval532 = ((_augval532) < (_val534)) ? (_augval532) : (_val534);
                }
                var _child535 = (_a524)._right8;
                if (!((_child535) == null)) {
                    var _val536 = (_child535)._min_ay13;
                    _augval532 = ((_augval532) < (_val536)) ? (_augval532) : (_val536);
                }
                (_a524)._min_ay13 = _augval532;
                /* _max_ay24 is max of ay2 */
                var _augval537 = (_a524).ay2;
                var _child538 = (_a524)._left7;
                if (!((_child538) == null)) {
                    var _val539 = (_child538)._max_ay24;
                    _augval537 = ((_augval537) < (_val539)) ? (_val539) : (_augval537);
                }
                var _child540 = (_a524)._right8;
                if (!((_child540) == null)) {
                    var _val541 = (_child540)._max_ay24;
                    _augval537 = ((_augval537) < (_val541)) ? (_val541) : (_augval537);
                }
                (_a524)._max_ay24 = _augval537;
                (_a524)._height10 = 1 + ((((((_a524)._left7) == null) ? (-1) : (((_a524)._left7)._height10)) > ((((_a524)._right8) == null) ? (-1) : (((_a524)._right8)._height10))) ? ((((_a524)._left7) == null) ? (-1) : (((_a524)._left7)._height10)) : ((((_a524)._right8) == null) ? (-1) : (((_a524)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval542 = (_b525).ax1;
                var _child543 = (_b525)._left7;
                if (!((_child543) == null)) {
                    var _val544 = (_child543)._min_ax12;
                    _augval542 = ((_augval542) < (_val544)) ? (_augval542) : (_val544);
                }
                var _child545 = (_b525)._right8;
                if (!((_child545) == null)) {
                    var _val546 = (_child545)._min_ax12;
                    _augval542 = ((_augval542) < (_val546)) ? (_augval542) : (_val546);
                }
                (_b525)._min_ax12 = _augval542;
                /* _min_ay13 is min of ay1 */
                var _augval547 = (_b525).ay1;
                var _child548 = (_b525)._left7;
                if (!((_child548) == null)) {
                    var _val549 = (_child548)._min_ay13;
                    _augval547 = ((_augval547) < (_val549)) ? (_augval547) : (_val549);
                }
                var _child550 = (_b525)._right8;
                if (!((_child550) == null)) {
                    var _val551 = (_child550)._min_ay13;
                    _augval547 = ((_augval547) < (_val551)) ? (_augval547) : (_val551);
                }
                (_b525)._min_ay13 = _augval547;
                /* _max_ay24 is max of ay2 */
                var _augval552 = (_b525).ay2;
                var _child553 = (_b525)._left7;
                if (!((_child553) == null)) {
                    var _val554 = (_child553)._max_ay24;
                    _augval552 = ((_augval552) < (_val554)) ? (_val554) : (_augval552);
                }
                var _child555 = (_b525)._right8;
                if (!((_child555) == null)) {
                    var _val556 = (_child555)._max_ay24;
                    _augval552 = ((_augval552) < (_val556)) ? (_val556) : (_augval552);
                }
                (_b525)._max_ay24 = _augval552;
                (_b525)._height10 = 1 + ((((((_b525)._left7) == null) ? (-1) : (((_b525)._left7)._height10)) > ((((_b525)._right8) == null) ? (-1) : (((_b525)._right8)._height10))) ? ((((_b525)._left7) == null) ? (-1) : (((_b525)._left7)._height10)) : ((((_b525)._right8) == null) ? (-1) : (((_b525)._right8)._height10)));
                if (!(((_b525)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval557 = ((_b525)._parent9).ax1;
                    var _child558 = ((_b525)._parent9)._left7;
                    if (!((_child558) == null)) {
                        var _val559 = (_child558)._min_ax12;
                        _augval557 = ((_augval557) < (_val559)) ? (_augval557) : (_val559);
                    }
                    var _child560 = ((_b525)._parent9)._right8;
                    if (!((_child560) == null)) {
                        var _val561 = (_child560)._min_ax12;
                        _augval557 = ((_augval557) < (_val561)) ? (_augval557) : (_val561);
                    }
                    ((_b525)._parent9)._min_ax12 = _augval557;
                    /* _min_ay13 is min of ay1 */
                    var _augval562 = ((_b525)._parent9).ay1;
                    var _child563 = ((_b525)._parent9)._left7;
                    if (!((_child563) == null)) {
                        var _val564 = (_child563)._min_ay13;
                        _augval562 = ((_augval562) < (_val564)) ? (_augval562) : (_val564);
                    }
                    var _child565 = ((_b525)._parent9)._right8;
                    if (!((_child565) == null)) {
                        var _val566 = (_child565)._min_ay13;
                        _augval562 = ((_augval562) < (_val566)) ? (_augval562) : (_val566);
                    }
                    ((_b525)._parent9)._min_ay13 = _augval562;
                    /* _max_ay24 is max of ay2 */
                    var _augval567 = ((_b525)._parent9).ay2;
                    var _child568 = ((_b525)._parent9)._left7;
                    if (!((_child568) == null)) {
                        var _val569 = (_child568)._max_ay24;
                        _augval567 = ((_augval567) < (_val569)) ? (_val569) : (_augval567);
                    }
                    var _child570 = ((_b525)._parent9)._right8;
                    if (!((_child570) == null)) {
                        var _val571 = (_child570)._max_ay24;
                        _augval567 = ((_augval567) < (_val571)) ? (_val571) : (_augval567);
                    }
                    ((_b525)._parent9)._max_ay24 = _augval567;
                    ((_b525)._parent9)._height10 = 1 + (((((((_b525)._parent9)._left7) == null) ? (-1) : ((((_b525)._parent9)._left7)._height10)) > (((((_b525)._parent9)._right8) == null) ? (-1) : ((((_b525)._parent9)._right8)._height10))) ? (((((_b525)._parent9)._left7) == null) ? (-1) : ((((_b525)._parent9)._left7)._height10)) : (((((_b525)._parent9)._right8) == null) ? (-1) : ((((_b525)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b525;
                }
                _cursor474 = (_cursor474)._parent9;
            } else if ((_imbalance475) < (-1)) {
                if ((((((_cursor474)._right8)._left7) == null) ? (-1) : ((((_cursor474)._right8)._left7)._height10)) > (((((_cursor474)._right8)._right8) == null) ? (-1) : ((((_cursor474)._right8)._right8)._height10))) {
                    /* rotate ((_cursor474)._right8)._left7 */
                    var _a572 = (_cursor474)._right8;
                    var _b573 = (_a572)._left7;
                    var _c574 = (_b573)._right8;
                    /* replace _a572 with _b573 in (_a572)._parent9 */
                    if (!(((_a572)._parent9) == null)) {
                        if ((((_a572)._parent9)._left7) == (_a572)) {
                            ((_a572)._parent9)._left7 = _b573;
                        } else {
                            ((_a572)._parent9)._right8 = _b573;
                        }
                    }
                    if (!((_b573) == null)) {
                        (_b573)._parent9 = (_a572)._parent9;
                    }
                    /* replace _c574 with _a572 in _b573 */
                    (_b573)._right8 = _a572;
                    if (!((_a572) == null)) {
                        (_a572)._parent9 = _b573;
                    }
                    /* replace _b573 with _c574 in _a572 */
                    (_a572)._left7 = _c574;
                    if (!((_c574) == null)) {
                        (_c574)._parent9 = _a572;
                    }
                    /* _min_ax12 is min of ax1 */
                    var _augval575 = (_a572).ax1;
                    var _child576 = (_a572)._left7;
                    if (!((_child576) == null)) {
                        var _val577 = (_child576)._min_ax12;
                        _augval575 = ((_augval575) < (_val577)) ? (_augval575) : (_val577);
                    }
                    var _child578 = (_a572)._right8;
                    if (!((_child578) == null)) {
                        var _val579 = (_child578)._min_ax12;
                        _augval575 = ((_augval575) < (_val579)) ? (_augval575) : (_val579);
                    }
                    (_a572)._min_ax12 = _augval575;
                    /* _min_ay13 is min of ay1 */
                    var _augval580 = (_a572).ay1;
                    var _child581 = (_a572)._left7;
                    if (!((_child581) == null)) {
                        var _val582 = (_child581)._min_ay13;
                        _augval580 = ((_augval580) < (_val582)) ? (_augval580) : (_val582);
                    }
                    var _child583 = (_a572)._right8;
                    if (!((_child583) == null)) {
                        var _val584 = (_child583)._min_ay13;
                        _augval580 = ((_augval580) < (_val584)) ? (_augval580) : (_val584);
                    }
                    (_a572)._min_ay13 = _augval580;
                    /* _max_ay24 is max of ay2 */
                    var _augval585 = (_a572).ay2;
                    var _child586 = (_a572)._left7;
                    if (!((_child586) == null)) {
                        var _val587 = (_child586)._max_ay24;
                        _augval585 = ((_augval585) < (_val587)) ? (_val587) : (_augval585);
                    }
                    var _child588 = (_a572)._right8;
                    if (!((_child588) == null)) {
                        var _val589 = (_child588)._max_ay24;
                        _augval585 = ((_augval585) < (_val589)) ? (_val589) : (_augval585);
                    }
                    (_a572)._max_ay24 = _augval585;
                    (_a572)._height10 = 1 + ((((((_a572)._left7) == null) ? (-1) : (((_a572)._left7)._height10)) > ((((_a572)._right8) == null) ? (-1) : (((_a572)._right8)._height10))) ? ((((_a572)._left7) == null) ? (-1) : (((_a572)._left7)._height10)) : ((((_a572)._right8) == null) ? (-1) : (((_a572)._right8)._height10)));
                    /* _min_ax12 is min of ax1 */
                    var _augval590 = (_b573).ax1;
                    var _child591 = (_b573)._left7;
                    if (!((_child591) == null)) {
                        var _val592 = (_child591)._min_ax12;
                        _augval590 = ((_augval590) < (_val592)) ? (_augval590) : (_val592);
                    }
                    var _child593 = (_b573)._right8;
                    if (!((_child593) == null)) {
                        var _val594 = (_child593)._min_ax12;
                        _augval590 = ((_augval590) < (_val594)) ? (_augval590) : (_val594);
                    }
                    (_b573)._min_ax12 = _augval590;
                    /* _min_ay13 is min of ay1 */
                    var _augval595 = (_b573).ay1;
                    var _child596 = (_b573)._left7;
                    if (!((_child596) == null)) {
                        var _val597 = (_child596)._min_ay13;
                        _augval595 = ((_augval595) < (_val597)) ? (_augval595) : (_val597);
                    }
                    var _child598 = (_b573)._right8;
                    if (!((_child598) == null)) {
                        var _val599 = (_child598)._min_ay13;
                        _augval595 = ((_augval595) < (_val599)) ? (_augval595) : (_val599);
                    }
                    (_b573)._min_ay13 = _augval595;
                    /* _max_ay24 is max of ay2 */
                    var _augval600 = (_b573).ay2;
                    var _child601 = (_b573)._left7;
                    if (!((_child601) == null)) {
                        var _val602 = (_child601)._max_ay24;
                        _augval600 = ((_augval600) < (_val602)) ? (_val602) : (_augval600);
                    }
                    var _child603 = (_b573)._right8;
                    if (!((_child603) == null)) {
                        var _val604 = (_child603)._max_ay24;
                        _augval600 = ((_augval600) < (_val604)) ? (_val604) : (_augval600);
                    }
                    (_b573)._max_ay24 = _augval600;
                    (_b573)._height10 = 1 + ((((((_b573)._left7) == null) ? (-1) : (((_b573)._left7)._height10)) > ((((_b573)._right8) == null) ? (-1) : (((_b573)._right8)._height10))) ? ((((_b573)._left7) == null) ? (-1) : (((_b573)._left7)._height10)) : ((((_b573)._right8) == null) ? (-1) : (((_b573)._right8)._height10)));
                    if (!(((_b573)._parent9) == null)) {
                        /* _min_ax12 is min of ax1 */
                        var _augval605 = ((_b573)._parent9).ax1;
                        var _child606 = ((_b573)._parent9)._left7;
                        if (!((_child606) == null)) {
                            var _val607 = (_child606)._min_ax12;
                            _augval605 = ((_augval605) < (_val607)) ? (_augval605) : (_val607);
                        }
                        var _child608 = ((_b573)._parent9)._right8;
                        if (!((_child608) == null)) {
                            var _val609 = (_child608)._min_ax12;
                            _augval605 = ((_augval605) < (_val609)) ? (_augval605) : (_val609);
                        }
                        ((_b573)._parent9)._min_ax12 = _augval605;
                        /* _min_ay13 is min of ay1 */
                        var _augval610 = ((_b573)._parent9).ay1;
                        var _child611 = ((_b573)._parent9)._left7;
                        if (!((_child611) == null)) {
                            var _val612 = (_child611)._min_ay13;
                            _augval610 = ((_augval610) < (_val612)) ? (_augval610) : (_val612);
                        }
                        var _child613 = ((_b573)._parent9)._right8;
                        if (!((_child613) == null)) {
                            var _val614 = (_child613)._min_ay13;
                            _augval610 = ((_augval610) < (_val614)) ? (_augval610) : (_val614);
                        }
                        ((_b573)._parent9)._min_ay13 = _augval610;
                        /* _max_ay24 is max of ay2 */
                        var _augval615 = ((_b573)._parent9).ay2;
                        var _child616 = ((_b573)._parent9)._left7;
                        if (!((_child616) == null)) {
                            var _val617 = (_child616)._max_ay24;
                            _augval615 = ((_augval615) < (_val617)) ? (_val617) : (_augval615);
                        }
                        var _child618 = ((_b573)._parent9)._right8;
                        if (!((_child618) == null)) {
                            var _val619 = (_child618)._max_ay24;
                            _augval615 = ((_augval615) < (_val619)) ? (_val619) : (_augval615);
                        }
                        ((_b573)._parent9)._max_ay24 = _augval615;
                        ((_b573)._parent9)._height10 = 1 + (((((((_b573)._parent9)._left7) == null) ? (-1) : ((((_b573)._parent9)._left7)._height10)) > (((((_b573)._parent9)._right8) == null) ? (-1) : ((((_b573)._parent9)._right8)._height10))) ? (((((_b573)._parent9)._left7) == null) ? (-1) : ((((_b573)._parent9)._left7)._height10)) : (((((_b573)._parent9)._right8) == null) ? (-1) : ((((_b573)._parent9)._right8)._height10)));
                    } else {
                        (this)._root1 = _b573;
                    }
                }
                /* rotate (_cursor474)._right8 */
                var _a620 = _cursor474;
                var _b621 = (_a620)._right8;
                var _c622 = (_b621)._left7;
                /* replace _a620 with _b621 in (_a620)._parent9 */
                if (!(((_a620)._parent9) == null)) {
                    if ((((_a620)._parent9)._left7) == (_a620)) {
                        ((_a620)._parent9)._left7 = _b621;
                    } else {
                        ((_a620)._parent9)._right8 = _b621;
                    }
                }
                if (!((_b621) == null)) {
                    (_b621)._parent9 = (_a620)._parent9;
                }
                /* replace _c622 with _a620 in _b621 */
                (_b621)._left7 = _a620;
                if (!((_a620) == null)) {
                    (_a620)._parent9 = _b621;
                }
                /* replace _b621 with _c622 in _a620 */
                (_a620)._right8 = _c622;
                if (!((_c622) == null)) {
                    (_c622)._parent9 = _a620;
                }
                /* _min_ax12 is min of ax1 */
                var _augval623 = (_a620).ax1;
                var _child624 = (_a620)._left7;
                if (!((_child624) == null)) {
                    var _val625 = (_child624)._min_ax12;
                    _augval623 = ((_augval623) < (_val625)) ? (_augval623) : (_val625);
                }
                var _child626 = (_a620)._right8;
                if (!((_child626) == null)) {
                    var _val627 = (_child626)._min_ax12;
                    _augval623 = ((_augval623) < (_val627)) ? (_augval623) : (_val627);
                }
                (_a620)._min_ax12 = _augval623;
                /* _min_ay13 is min of ay1 */
                var _augval628 = (_a620).ay1;
                var _child629 = (_a620)._left7;
                if (!((_child629) == null)) {
                    var _val630 = (_child629)._min_ay13;
                    _augval628 = ((_augval628) < (_val630)) ? (_augval628) : (_val630);
                }
                var _child631 = (_a620)._right8;
                if (!((_child631) == null)) {
                    var _val632 = (_child631)._min_ay13;
                    _augval628 = ((_augval628) < (_val632)) ? (_augval628) : (_val632);
                }
                (_a620)._min_ay13 = _augval628;
                /* _max_ay24 is max of ay2 */
                var _augval633 = (_a620).ay2;
                var _child634 = (_a620)._left7;
                if (!((_child634) == null)) {
                    var _val635 = (_child634)._max_ay24;
                    _augval633 = ((_augval633) < (_val635)) ? (_val635) : (_augval633);
                }
                var _child636 = (_a620)._right8;
                if (!((_child636) == null)) {
                    var _val637 = (_child636)._max_ay24;
                    _augval633 = ((_augval633) < (_val637)) ? (_val637) : (_augval633);
                }
                (_a620)._max_ay24 = _augval633;
                (_a620)._height10 = 1 + ((((((_a620)._left7) == null) ? (-1) : (((_a620)._left7)._height10)) > ((((_a620)._right8) == null) ? (-1) : (((_a620)._right8)._height10))) ? ((((_a620)._left7) == null) ? (-1) : (((_a620)._left7)._height10)) : ((((_a620)._right8) == null) ? (-1) : (((_a620)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval638 = (_b621).ax1;
                var _child639 = (_b621)._left7;
                if (!((_child639) == null)) {
                    var _val640 = (_child639)._min_ax12;
                    _augval638 = ((_augval638) < (_val640)) ? (_augval638) : (_val640);
                }
                var _child641 = (_b621)._right8;
                if (!((_child641) == null)) {
                    var _val642 = (_child641)._min_ax12;
                    _augval638 = ((_augval638) < (_val642)) ? (_augval638) : (_val642);
                }
                (_b621)._min_ax12 = _augval638;
                /* _min_ay13 is min of ay1 */
                var _augval643 = (_b621).ay1;
                var _child644 = (_b621)._left7;
                if (!((_child644) == null)) {
                    var _val645 = (_child644)._min_ay13;
                    _augval643 = ((_augval643) < (_val645)) ? (_augval643) : (_val645);
                }
                var _child646 = (_b621)._right8;
                if (!((_child646) == null)) {
                    var _val647 = (_child646)._min_ay13;
                    _augval643 = ((_augval643) < (_val647)) ? (_augval643) : (_val647);
                }
                (_b621)._min_ay13 = _augval643;
                /* _max_ay24 is max of ay2 */
                var _augval648 = (_b621).ay2;
                var _child649 = (_b621)._left7;
                if (!((_child649) == null)) {
                    var _val650 = (_child649)._max_ay24;
                    _augval648 = ((_augval648) < (_val650)) ? (_val650) : (_augval648);
                }
                var _child651 = (_b621)._right8;
                if (!((_child651) == null)) {
                    var _val652 = (_child651)._max_ay24;
                    _augval648 = ((_augval648) < (_val652)) ? (_val652) : (_augval648);
                }
                (_b621)._max_ay24 = _augval648;
                (_b621)._height10 = 1 + ((((((_b621)._left7) == null) ? (-1) : (((_b621)._left7)._height10)) > ((((_b621)._right8) == null) ? (-1) : (((_b621)._right8)._height10))) ? ((((_b621)._left7) == null) ? (-1) : (((_b621)._left7)._height10)) : ((((_b621)._right8) == null) ? (-1) : (((_b621)._right8)._height10)));
                if (!(((_b621)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval653 = ((_b621)._parent9).ax1;
                    var _child654 = ((_b621)._parent9)._left7;
                    if (!((_child654) == null)) {
                        var _val655 = (_child654)._min_ax12;
                        _augval653 = ((_augval653) < (_val655)) ? (_augval653) : (_val655);
                    }
                    var _child656 = ((_b621)._parent9)._right8;
                    if (!((_child656) == null)) {
                        var _val657 = (_child656)._min_ax12;
                        _augval653 = ((_augval653) < (_val657)) ? (_augval653) : (_val657);
                    }
                    ((_b621)._parent9)._min_ax12 = _augval653;
                    /* _min_ay13 is min of ay1 */
                    var _augval658 = ((_b621)._parent9).ay1;
                    var _child659 = ((_b621)._parent9)._left7;
                    if (!((_child659) == null)) {
                        var _val660 = (_child659)._min_ay13;
                        _augval658 = ((_augval658) < (_val660)) ? (_augval658) : (_val660);
                    }
                    var _child661 = ((_b621)._parent9)._right8;
                    if (!((_child661) == null)) {
                        var _val662 = (_child661)._min_ay13;
                        _augval658 = ((_augval658) < (_val662)) ? (_augval658) : (_val662);
                    }
                    ((_b621)._parent9)._min_ay13 = _augval658;
                    /* _max_ay24 is max of ay2 */
                    var _augval663 = ((_b621)._parent9).ay2;
                    var _child664 = ((_b621)._parent9)._left7;
                    if (!((_child664) == null)) {
                        var _val665 = (_child664)._max_ay24;
                        _augval663 = ((_augval663) < (_val665)) ? (_val665) : (_augval663);
                    }
                    var _child666 = ((_b621)._parent9)._right8;
                    if (!((_child666) == null)) {
                        var _val667 = (_child666)._max_ay24;
                        _augval663 = ((_augval663) < (_val667)) ? (_val667) : (_augval663);
                    }
                    ((_b621)._parent9)._max_ay24 = _augval663;
                    ((_b621)._parent9)._height10 = 1 + (((((((_b621)._parent9)._left7) == null) ? (-1) : ((((_b621)._parent9)._left7)._height10)) > (((((_b621)._parent9)._right8) == null) ? (-1) : ((((_b621)._parent9)._right8)._height10))) ? (((((_b621)._parent9)._left7) == null) ? (-1) : ((((_b621)._parent9)._left7)._height10)) : (((((_b621)._parent9)._right8) == null) ? (-1) : ((((_b621)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b621;
                }
                _cursor474 = (_cursor474)._parent9;
            }
        }
        (__x).ax2 = new_val;
    }
}
RectangleHolder.prototype.updateAy2 = function (__x, new_val) {
    if ((__x).ay2 != new_val) {
        /* _max_ay24 is max of ay2 */
        var _augval668 = new_val;
        var _child669 = (__x)._left7;
        if (!((_child669) == null)) {
            var _val670 = (_child669)._max_ay24;
            _augval668 = ((_augval668) < (_val670)) ? (_val670) : (_augval668);
        }
        var _child671 = (__x)._right8;
        if (!((_child671) == null)) {
            var _val672 = (_child671)._max_ay24;
            _augval668 = ((_augval668) < (_val672)) ? (_val672) : (_augval668);
        }
        (__x)._max_ay24 = _augval668;
        var _cursor673 = (__x)._parent9;
        var _changed674 = true;
        while ((_changed674) && (!((_cursor673) == (null)))) {
            var _old__max_ay24675 = (_cursor673)._max_ay24;
            var _old_height676 = (_cursor673)._height10;
            /* _max_ay24 is max of ay2 */
            var _augval677 = (_cursor673).ay2;
            var _child678 = (_cursor673)._left7;
            if (!((_child678) == null)) {
                var _val679 = (_child678)._max_ay24;
                _augval677 = ((_augval677) < (_val679)) ? (_val679) : (_augval677);
            }
            var _child680 = (_cursor673)._right8;
            if (!((_child680) == null)) {
                var _val681 = (_child680)._max_ay24;
                _augval677 = ((_augval677) < (_val681)) ? (_val681) : (_augval677);
            }
            (_cursor673)._max_ay24 = _augval677;
            (_cursor673)._height10 = 1 + ((((((_cursor673)._left7) == null) ? (-1) : (((_cursor673)._left7)._height10)) > ((((_cursor673)._right8) == null) ? (-1) : (((_cursor673)._right8)._height10))) ? ((((_cursor673)._left7) == null) ? (-1) : (((_cursor673)._left7)._height10)) : ((((_cursor673)._right8) == null) ? (-1) : (((_cursor673)._right8)._height10)));
            _changed674 = false;
            _changed674 = (_changed674) || (!((_old__max_ay24675) == ((_cursor673)._max_ay24)));
            _changed674 = (_changed674) || (!((_old_height676) == ((_cursor673)._height10)));
            _cursor673 = (_cursor673)._parent9;
        }
        (__x).ay2 = new_val;
    }
}
RectangleHolder.prototype.update = function (__x, ax1, ay1, ax2, ay2) {
    var _parent682 = (__x)._parent9;
    var _left683 = (__x)._left7;
    var _right684 = (__x)._right8;
    var _new_x685;
    if (((_left683) == null) && ((_right684) == null)) {
        _new_x685 = null;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else if ((!((_left683) == null)) && ((_right684) == null)) {
        _new_x685 = _left683;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else if (((_left683) == null) && (!((_right684) == null))) {
        _new_x685 = _right684;
        /* replace __x with _new_x685 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _new_x685;
            } else {
                (_parent682)._right8 = _new_x685;
            }
        }
        if (!((_new_x685) == null)) {
            (_new_x685)._parent9 = _parent682;
        }
    } else {
        var _root686 = (__x)._right8;
        var _x687 = _root686;
        var _descend688 = true;
        var _from_left689 = true;
        while (true) {
            if ((_x687) == null) {
                _x687 = null;
                break;
            }
            if (_descend688) {
                /* too small? */
                if (false) {
                    if ((!(((_x687)._right8) == null)) && (true)) {
                        if ((_x687) == (_root686)) {
                            _root686 = (_x687)._right8;
                        }
                        _x687 = (_x687)._right8;
                    } else if ((_x687) == (_root686)) {
                        _x687 = null;
                        break;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                } else if ((!(((_x687)._left7) == null)) && (true)) {
                    _x687 = (_x687)._left7;
                    /* too large? */
                } else if (false) {
                    if ((_x687) == (_root686)) {
                        _x687 = null;
                        break;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                    /* node ok? */
                } else if (true) {
                    break;
                } else if ((_x687) == (_root686)) {
                    _root686 = (_x687)._right8;
                    _x687 = (_x687)._right8;
                } else {
                    if ((!(((_x687)._right8) == null)) && (true)) {
                        if ((_x687) == (_root686)) {
                            _root686 = (_x687)._right8;
                        }
                        _x687 = (_x687)._right8;
                    } else {
                        _descend688 = false;
                        _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                        _x687 = (_x687)._parent9;
                    }
                }
            } else if (_from_left689) {
                if (false) {
                    _x687 = null;
                    break;
                } else if (true) {
                    break;
                } else if ((!(((_x687)._right8) == null)) && (true)) {
                    _descend688 = true;
                    if ((_x687) == (_root686)) {
                        _root686 = (_x687)._right8;
                    }
                    _x687 = (_x687)._right8;
                } else if ((_x687) == (_root686)) {
                    _x687 = null;
                    break;
                } else {
                    _descend688 = false;
                    _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                    _x687 = (_x687)._parent9;
                }
            } else {
                if ((_x687) == (_root686)) {
                    _x687 = null;
                    break;
                } else {
                    _descend688 = false;
                    _from_left689 = (!(((_x687)._parent9) == null)) && ((_x687) == (((_x687)._parent9)._left7));
                    _x687 = (_x687)._parent9;
                }
            }
        }
        _new_x685 = _x687;
        var _mp690 = (_x687)._parent9;
        var _mr691 = (_x687)._right8;
        /* replace _x687 with _mr691 in _mp690 */
        if (!((_mp690) == null)) {
            if (((_mp690)._left7) == (_x687)) {
                (_mp690)._left7 = _mr691;
            } else {
                (_mp690)._right8 = _mr691;
            }
        }
        if (!((_mr691) == null)) {
            (_mr691)._parent9 = _mp690;
        }
        /* replace __x with _x687 in _parent682 */
        if (!((_parent682) == null)) {
            if (((_parent682)._left7) == (__x)) {
                (_parent682)._left7 = _x687;
            } else {
                (_parent682)._right8 = _x687;
            }
        }
        if (!((_x687) == null)) {
            (_x687)._parent9 = _parent682;
        }
        /* replace null with _left683 in _x687 */
        (_x687)._left7 = _left683;
        if (!((_left683) == null)) {
            (_left683)._parent9 = _x687;
        }
        /* replace _mr691 with (__x)._right8 in _x687 */
        (_x687)._right8 = (__x)._right8;
        if (!(((__x)._right8) == null)) {
            ((__x)._right8)._parent9 = _x687;
        }
        /* _min_ax12 is min of ax1 */
        var _augval692 = (_x687).ax1;
        var _child693 = (_x687)._left7;
        if (!((_child693) == null)) {
            var _val694 = (_child693)._min_ax12;
            _augval692 = ((_augval692) < (_val694)) ? (_augval692) : (_val694);
        }
        var _child695 = (_x687)._right8;
        if (!((_child695) == null)) {
            var _val696 = (_child695)._min_ax12;
            _augval692 = ((_augval692) < (_val696)) ? (_augval692) : (_val696);
        }
        (_x687)._min_ax12 = _augval692;
        /* _min_ay13 is min of ay1 */
        var _augval697 = (_x687).ay1;
        var _child698 = (_x687)._left7;
        if (!((_child698) == null)) {
            var _val699 = (_child698)._min_ay13;
            _augval697 = ((_augval697) < (_val699)) ? (_augval697) : (_val699);
        }
        var _child700 = (_x687)._right8;
        if (!((_child700) == null)) {
            var _val701 = (_child700)._min_ay13;
            _augval697 = ((_augval697) < (_val701)) ? (_augval697) : (_val701);
        }
        (_x687)._min_ay13 = _augval697;
        /* _max_ay24 is max of ay2 */
        var _augval702 = (_x687).ay2;
        var _child703 = (_x687)._left7;
        if (!((_child703) == null)) {
            var _val704 = (_child703)._max_ay24;
            _augval702 = ((_augval702) < (_val704)) ? (_val704) : (_augval702);
        }
        var _child705 = (_x687)._right8;
        if (!((_child705) == null)) {
            var _val706 = (_child705)._max_ay24;
            _augval702 = ((_augval702) < (_val706)) ? (_val706) : (_augval702);
        }
        (_x687)._max_ay24 = _augval702;
        (_x687)._height10 = 1 + ((((((_x687)._left7) == null) ? (-1) : (((_x687)._left7)._height10)) > ((((_x687)._right8) == null) ? (-1) : (((_x687)._right8)._height10))) ? ((((_x687)._left7) == null) ? (-1) : (((_x687)._left7)._height10)) : ((((_x687)._right8) == null) ? (-1) : (((_x687)._right8)._height10)));
        var _cursor707 = _mp690;
        var _changed708 = true;
        while ((_changed708) && (!((_cursor707) == (_parent682)))) {
            var _old__min_ax12709 = (_cursor707)._min_ax12;
            var _old__min_ay13710 = (_cursor707)._min_ay13;
            var _old__max_ay24711 = (_cursor707)._max_ay24;
            var _old_height712 = (_cursor707)._height10;
            /* _min_ax12 is min of ax1 */
            var _augval713 = (_cursor707).ax1;
            var _child714 = (_cursor707)._left7;
            if (!((_child714) == null)) {
                var _val715 = (_child714)._min_ax12;
                _augval713 = ((_augval713) < (_val715)) ? (_augval713) : (_val715);
            }
            var _child716 = (_cursor707)._right8;
            if (!((_child716) == null)) {
                var _val717 = (_child716)._min_ax12;
                _augval713 = ((_augval713) < (_val717)) ? (_augval713) : (_val717);
            }
            (_cursor707)._min_ax12 = _augval713;
            /* _min_ay13 is min of ay1 */
            var _augval718 = (_cursor707).ay1;
            var _child719 = (_cursor707)._left7;
            if (!((_child719) == null)) {
                var _val720 = (_child719)._min_ay13;
                _augval718 = ((_augval718) < (_val720)) ? (_augval718) : (_val720);
            }
            var _child721 = (_cursor707)._right8;
            if (!((_child721) == null)) {
                var _val722 = (_child721)._min_ay13;
                _augval718 = ((_augval718) < (_val722)) ? (_augval718) : (_val722);
            }
            (_cursor707)._min_ay13 = _augval718;
            /* _max_ay24 is max of ay2 */
            var _augval723 = (_cursor707).ay2;
            var _child724 = (_cursor707)._left7;
            if (!((_child724) == null)) {
                var _val725 = (_child724)._max_ay24;
                _augval723 = ((_augval723) < (_val725)) ? (_val725) : (_augval723);
            }
            var _child726 = (_cursor707)._right8;
            if (!((_child726) == null)) {
                var _val727 = (_child726)._max_ay24;
                _augval723 = ((_augval723) < (_val727)) ? (_val727) : (_augval723);
            }
            (_cursor707)._max_ay24 = _augval723;
            (_cursor707)._height10 = 1 + ((((((_cursor707)._left7) == null) ? (-1) : (((_cursor707)._left7)._height10)) > ((((_cursor707)._right8) == null) ? (-1) : (((_cursor707)._right8)._height10))) ? ((((_cursor707)._left7) == null) ? (-1) : (((_cursor707)._left7)._height10)) : ((((_cursor707)._right8) == null) ? (-1) : (((_cursor707)._right8)._height10)));
            _changed708 = false;
            _changed708 = (_changed708) || (!((_old__min_ax12709) == ((_cursor707)._min_ax12)));
            _changed708 = (_changed708) || (!((_old__min_ay13710) == ((_cursor707)._min_ay13)));
            _changed708 = (_changed708) || (!((_old__max_ay24711) == ((_cursor707)._max_ay24)));
            _changed708 = (_changed708) || (!((_old_height712) == ((_cursor707)._height10)));
            _cursor707 = (_cursor707)._parent9;
        }
    }
    var _cursor728 = _parent682;
    var _changed729 = true;
    while ((_changed729) && (!((_cursor728) == (null)))) {
        var _old__min_ax12730 = (_cursor728)._min_ax12;
        var _old__min_ay13731 = (_cursor728)._min_ay13;
        var _old__max_ay24732 = (_cursor728)._max_ay24;
        var _old_height733 = (_cursor728)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval734 = (_cursor728).ax1;
        var _child735 = (_cursor728)._left7;
        if (!((_child735) == null)) {
            var _val736 = (_child735)._min_ax12;
            _augval734 = ((_augval734) < (_val736)) ? (_augval734) : (_val736);
        }
        var _child737 = (_cursor728)._right8;
        if (!((_child737) == null)) {
            var _val738 = (_child737)._min_ax12;
            _augval734 = ((_augval734) < (_val738)) ? (_augval734) : (_val738);
        }
        (_cursor728)._min_ax12 = _augval734;
        /* _min_ay13 is min of ay1 */
        var _augval739 = (_cursor728).ay1;
        var _child740 = (_cursor728)._left7;
        if (!((_child740) == null)) {
            var _val741 = (_child740)._min_ay13;
            _augval739 = ((_augval739) < (_val741)) ? (_augval739) : (_val741);
        }
        var _child742 = (_cursor728)._right8;
        if (!((_child742) == null)) {
            var _val743 = (_child742)._min_ay13;
            _augval739 = ((_augval739) < (_val743)) ? (_augval739) : (_val743);
        }
        (_cursor728)._min_ay13 = _augval739;
        /* _max_ay24 is max of ay2 */
        var _augval744 = (_cursor728).ay2;
        var _child745 = (_cursor728)._left7;
        if (!((_child745) == null)) {
            var _val746 = (_child745)._max_ay24;
            _augval744 = ((_augval744) < (_val746)) ? (_val746) : (_augval744);
        }
        var _child747 = (_cursor728)._right8;
        if (!((_child747) == null)) {
            var _val748 = (_child747)._max_ay24;
            _augval744 = ((_augval744) < (_val748)) ? (_val748) : (_augval744);
        }
        (_cursor728)._max_ay24 = _augval744;
        (_cursor728)._height10 = 1 + ((((((_cursor728)._left7) == null) ? (-1) : (((_cursor728)._left7)._height10)) > ((((_cursor728)._right8) == null) ? (-1) : (((_cursor728)._right8)._height10))) ? ((((_cursor728)._left7) == null) ? (-1) : (((_cursor728)._left7)._height10)) : ((((_cursor728)._right8) == null) ? (-1) : (((_cursor728)._right8)._height10)));
        _changed729 = false;
        _changed729 = (_changed729) || (!((_old__min_ax12730) == ((_cursor728)._min_ax12)));
        _changed729 = (_changed729) || (!((_old__min_ay13731) == ((_cursor728)._min_ay13)));
        _changed729 = (_changed729) || (!((_old__max_ay24732) == ((_cursor728)._max_ay24)));
        _changed729 = (_changed729) || (!((_old_height733) == ((_cursor728)._height10)));
        _cursor728 = (_cursor728)._parent9;
    }
    if (((this)._root1) == (__x)) {
        (this)._root1 = _new_x685;
    }
    (__x)._left7 = null;
    (__x)._right8 = null;
    (__x)._min_ax12 = (__x).ax1;
    (__x)._min_ay13 = (__x).ay1;
    (__x)._max_ay24 = (__x).ay2;
    (__x)._height10 = 0;
    var _previous749 = null;
    var _current750 = (this)._root1;
    var _is_left751 = false;
    while (!((_current750) == null)) {
        _previous749 = _current750;
        if ((ax2) < ((_current750).ax2)) {
            _current750 = (_current750)._left7;
            _is_left751 = true;
        } else {
            _current750 = (_current750)._right8;
            _is_left751 = false;
        }
    }
    if ((_previous749) == null) {
        (this)._root1 = __x;
    } else {
        (__x)._parent9 = _previous749;
        if (_is_left751) {
            (_previous749)._left7 = __x;
        } else {
            (_previous749)._right8 = __x;
        }
    }
    var _cursor752 = (__x)._parent9;
    var _changed753 = true;
    while ((_changed753) && (!((_cursor752) == (null)))) {
        var _old__min_ax12754 = (_cursor752)._min_ax12;
        var _old__min_ay13755 = (_cursor752)._min_ay13;
        var _old__max_ay24756 = (_cursor752)._max_ay24;
        var _old_height757 = (_cursor752)._height10;
        /* _min_ax12 is min of ax1 */
        var _augval758 = (_cursor752).ax1;
        var _child759 = (_cursor752)._left7;
        if (!((_child759) == null)) {
            var _val760 = (_child759)._min_ax12;
            _augval758 = ((_augval758) < (_val760)) ? (_augval758) : (_val760);
        }
        var _child761 = (_cursor752)._right8;
        if (!((_child761) == null)) {
            var _val762 = (_child761)._min_ax12;
            _augval758 = ((_augval758) < (_val762)) ? (_augval758) : (_val762);
        }
        (_cursor752)._min_ax12 = _augval758;
        /* _min_ay13 is min of ay1 */
        var _augval763 = (_cursor752).ay1;
        var _child764 = (_cursor752)._left7;
        if (!((_child764) == null)) {
            var _val765 = (_child764)._min_ay13;
            _augval763 = ((_augval763) < (_val765)) ? (_augval763) : (_val765);
        }
        var _child766 = (_cursor752)._right8;
        if (!((_child766) == null)) {
            var _val767 = (_child766)._min_ay13;
            _augval763 = ((_augval763) < (_val767)) ? (_augval763) : (_val767);
        }
        (_cursor752)._min_ay13 = _augval763;
        /* _max_ay24 is max of ay2 */
        var _augval768 = (_cursor752).ay2;
        var _child769 = (_cursor752)._left7;
        if (!((_child769) == null)) {
            var _val770 = (_child769)._max_ay24;
            _augval768 = ((_augval768) < (_val770)) ? (_val770) : (_augval768);
        }
        var _child771 = (_cursor752)._right8;
        if (!((_child771) == null)) {
            var _val772 = (_child771)._max_ay24;
            _augval768 = ((_augval768) < (_val772)) ? (_val772) : (_augval768);
        }
        (_cursor752)._max_ay24 = _augval768;
        (_cursor752)._height10 = 1 + ((((((_cursor752)._left7) == null) ? (-1) : (((_cursor752)._left7)._height10)) > ((((_cursor752)._right8) == null) ? (-1) : (((_cursor752)._right8)._height10))) ? ((((_cursor752)._left7) == null) ? (-1) : (((_cursor752)._left7)._height10)) : ((((_cursor752)._right8) == null) ? (-1) : (((_cursor752)._right8)._height10)));
        _changed753 = false;
        _changed753 = (_changed753) || (!((_old__min_ax12754) == ((_cursor752)._min_ax12)));
        _changed753 = (_changed753) || (!((_old__min_ay13755) == ((_cursor752)._min_ay13)));
        _changed753 = (_changed753) || (!((_old__max_ay24756) == ((_cursor752)._max_ay24)));
        _changed753 = (_changed753) || (!((_old_height757) == ((_cursor752)._height10)));
        _cursor752 = (_cursor752)._parent9;
    }
    /* rebalance AVL tree */
    var _cursor773 = __x;
    var _imbalance774;
    while (!(((_cursor773)._parent9) == null)) {
        _cursor773 = (_cursor773)._parent9;
        (_cursor773)._height10 = 1 + ((((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) > ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10))) ? ((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) : ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10)));
        _imbalance774 = ((((_cursor773)._left7) == null) ? (-1) : (((_cursor773)._left7)._height10)) - ((((_cursor773)._right8) == null) ? (-1) : (((_cursor773)._right8)._height10));
        if ((_imbalance774) > (1)) {
            if ((((((_cursor773)._left7)._left7) == null) ? (-1) : ((((_cursor773)._left7)._left7)._height10)) < (((((_cursor773)._left7)._right8) == null) ? (-1) : ((((_cursor773)._left7)._right8)._height10))) {
                /* rotate ((_cursor773)._left7)._right8 */
                var _a775 = (_cursor773)._left7;
                var _b776 = (_a775)._right8;
                var _c777 = (_b776)._left7;
                /* replace _a775 with _b776 in (_a775)._parent9 */
                if (!(((_a775)._parent9) == null)) {
                    if ((((_a775)._parent9)._left7) == (_a775)) {
                        ((_a775)._parent9)._left7 = _b776;
                    } else {
                        ((_a775)._parent9)._right8 = _b776;
                    }
                }
                if (!((_b776) == null)) {
                    (_b776)._parent9 = (_a775)._parent9;
                }
                /* replace _c777 with _a775 in _b776 */
                (_b776)._left7 = _a775;
                if (!((_a775) == null)) {
                    (_a775)._parent9 = _b776;
                }
                /* replace _b776 with _c777 in _a775 */
                (_a775)._right8 = _c777;
                if (!((_c777) == null)) {
                    (_c777)._parent9 = _a775;
                }
                /* _min_ax12 is min of ax1 */
                var _augval778 = (_a775).ax1;
                var _child779 = (_a775)._left7;
                if (!((_child779) == null)) {
                    var _val780 = (_child779)._min_ax12;
                    _augval778 = ((_augval778) < (_val780)) ? (_augval778) : (_val780);
                }
                var _child781 = (_a775)._right8;
                if (!((_child781) == null)) {
                    var _val782 = (_child781)._min_ax12;
                    _augval778 = ((_augval778) < (_val782)) ? (_augval778) : (_val782);
                }
                (_a775)._min_ax12 = _augval778;
                /* _min_ay13 is min of ay1 */
                var _augval783 = (_a775).ay1;
                var _child784 = (_a775)._left7;
                if (!((_child784) == null)) {
                    var _val785 = (_child784)._min_ay13;
                    _augval783 = ((_augval783) < (_val785)) ? (_augval783) : (_val785);
                }
                var _child786 = (_a775)._right8;
                if (!((_child786) == null)) {
                    var _val787 = (_child786)._min_ay13;
                    _augval783 = ((_augval783) < (_val787)) ? (_augval783) : (_val787);
                }
                (_a775)._min_ay13 = _augval783;
                /* _max_ay24 is max of ay2 */
                var _augval788 = (_a775).ay2;
                var _child789 = (_a775)._left7;
                if (!((_child789) == null)) {
                    var _val790 = (_child789)._max_ay24;
                    _augval788 = ((_augval788) < (_val790)) ? (_val790) : (_augval788);
                }
                var _child791 = (_a775)._right8;
                if (!((_child791) == null)) {
                    var _val792 = (_child791)._max_ay24;
                    _augval788 = ((_augval788) < (_val792)) ? (_val792) : (_augval788);
                }
                (_a775)._max_ay24 = _augval788;
                (_a775)._height10 = 1 + ((((((_a775)._left7) == null) ? (-1) : (((_a775)._left7)._height10)) > ((((_a775)._right8) == null) ? (-1) : (((_a775)._right8)._height10))) ? ((((_a775)._left7) == null) ? (-1) : (((_a775)._left7)._height10)) : ((((_a775)._right8) == null) ? (-1) : (((_a775)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval793 = (_b776).ax1;
                var _child794 = (_b776)._left7;
                if (!((_child794) == null)) {
                    var _val795 = (_child794)._min_ax12;
                    _augval793 = ((_augval793) < (_val795)) ? (_augval793) : (_val795);
                }
                var _child796 = (_b776)._right8;
                if (!((_child796) == null)) {
                    var _val797 = (_child796)._min_ax12;
                    _augval793 = ((_augval793) < (_val797)) ? (_augval793) : (_val797);
                }
                (_b776)._min_ax12 = _augval793;
                /* _min_ay13 is min of ay1 */
                var _augval798 = (_b776).ay1;
                var _child799 = (_b776)._left7;
                if (!((_child799) == null)) {
                    var _val800 = (_child799)._min_ay13;
                    _augval798 = ((_augval798) < (_val800)) ? (_augval798) : (_val800);
                }
                var _child801 = (_b776)._right8;
                if (!((_child801) == null)) {
                    var _val802 = (_child801)._min_ay13;
                    _augval798 = ((_augval798) < (_val802)) ? (_augval798) : (_val802);
                }
                (_b776)._min_ay13 = _augval798;
                /* _max_ay24 is max of ay2 */
                var _augval803 = (_b776).ay2;
                var _child804 = (_b776)._left7;
                if (!((_child804) == null)) {
                    var _val805 = (_child804)._max_ay24;
                    _augval803 = ((_augval803) < (_val805)) ? (_val805) : (_augval803);
                }
                var _child806 = (_b776)._right8;
                if (!((_child806) == null)) {
                    var _val807 = (_child806)._max_ay24;
                    _augval803 = ((_augval803) < (_val807)) ? (_val807) : (_augval803);
                }
                (_b776)._max_ay24 = _augval803;
                (_b776)._height10 = 1 + ((((((_b776)._left7) == null) ? (-1) : (((_b776)._left7)._height10)) > ((((_b776)._right8) == null) ? (-1) : (((_b776)._right8)._height10))) ? ((((_b776)._left7) == null) ? (-1) : (((_b776)._left7)._height10)) : ((((_b776)._right8) == null) ? (-1) : (((_b776)._right8)._height10)));
                if (!(((_b776)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval808 = ((_b776)._parent9).ax1;
                    var _child809 = ((_b776)._parent9)._left7;
                    if (!((_child809) == null)) {
                        var _val810 = (_child809)._min_ax12;
                        _augval808 = ((_augval808) < (_val810)) ? (_augval808) : (_val810);
                    }
                    var _child811 = ((_b776)._parent9)._right8;
                    if (!((_child811) == null)) {
                        var _val812 = (_child811)._min_ax12;
                        _augval808 = ((_augval808) < (_val812)) ? (_augval808) : (_val812);
                    }
                    ((_b776)._parent9)._min_ax12 = _augval808;
                    /* _min_ay13 is min of ay1 */
                    var _augval813 = ((_b776)._parent9).ay1;
                    var _child814 = ((_b776)._parent9)._left7;
                    if (!((_child814) == null)) {
                        var _val815 = (_child814)._min_ay13;
                        _augval813 = ((_augval813) < (_val815)) ? (_augval813) : (_val815);
                    }
                    var _child816 = ((_b776)._parent9)._right8;
                    if (!((_child816) == null)) {
                        var _val817 = (_child816)._min_ay13;
                        _augval813 = ((_augval813) < (_val817)) ? (_augval813) : (_val817);
                    }
                    ((_b776)._parent9)._min_ay13 = _augval813;
                    /* _max_ay24 is max of ay2 */
                    var _augval818 = ((_b776)._parent9).ay2;
                    var _child819 = ((_b776)._parent9)._left7;
                    if (!((_child819) == null)) {
                        var _val820 = (_child819)._max_ay24;
                        _augval818 = ((_augval818) < (_val820)) ? (_val820) : (_augval818);
                    }
                    var _child821 = ((_b776)._parent9)._right8;
                    if (!((_child821) == null)) {
                        var _val822 = (_child821)._max_ay24;
                        _augval818 = ((_augval818) < (_val822)) ? (_val822) : (_augval818);
                    }
                    ((_b776)._parent9)._max_ay24 = _augval818;
                    ((_b776)._parent9)._height10 = 1 + (((((((_b776)._parent9)._left7) == null) ? (-1) : ((((_b776)._parent9)._left7)._height10)) > (((((_b776)._parent9)._right8) == null) ? (-1) : ((((_b776)._parent9)._right8)._height10))) ? (((((_b776)._parent9)._left7) == null) ? (-1) : ((((_b776)._parent9)._left7)._height10)) : (((((_b776)._parent9)._right8) == null) ? (-1) : ((((_b776)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b776;
                }
            }
            /* rotate (_cursor773)._left7 */
            var _a823 = _cursor773;
            var _b824 = (_a823)._left7;
            var _c825 = (_b824)._right8;
            /* replace _a823 with _b824 in (_a823)._parent9 */
            if (!(((_a823)._parent9) == null)) {
                if ((((_a823)._parent9)._left7) == (_a823)) {
                    ((_a823)._parent9)._left7 = _b824;
                } else {
                    ((_a823)._parent9)._right8 = _b824;
                }
            }
            if (!((_b824) == null)) {
                (_b824)._parent9 = (_a823)._parent9;
            }
            /* replace _c825 with _a823 in _b824 */
            (_b824)._right8 = _a823;
            if (!((_a823) == null)) {
                (_a823)._parent9 = _b824;
            }
            /* replace _b824 with _c825 in _a823 */
            (_a823)._left7 = _c825;
            if (!((_c825) == null)) {
                (_c825)._parent9 = _a823;
            }
            /* _min_ax12 is min of ax1 */
            var _augval826 = (_a823).ax1;
            var _child827 = (_a823)._left7;
            if (!((_child827) == null)) {
                var _val828 = (_child827)._min_ax12;
                _augval826 = ((_augval826) < (_val828)) ? (_augval826) : (_val828);
            }
            var _child829 = (_a823)._right8;
            if (!((_child829) == null)) {
                var _val830 = (_child829)._min_ax12;
                _augval826 = ((_augval826) < (_val830)) ? (_augval826) : (_val830);
            }
            (_a823)._min_ax12 = _augval826;
            /* _min_ay13 is min of ay1 */
            var _augval831 = (_a823).ay1;
            var _child832 = (_a823)._left7;
            if (!((_child832) == null)) {
                var _val833 = (_child832)._min_ay13;
                _augval831 = ((_augval831) < (_val833)) ? (_augval831) : (_val833);
            }
            var _child834 = (_a823)._right8;
            if (!((_child834) == null)) {
                var _val835 = (_child834)._min_ay13;
                _augval831 = ((_augval831) < (_val835)) ? (_augval831) : (_val835);
            }
            (_a823)._min_ay13 = _augval831;
            /* _max_ay24 is max of ay2 */
            var _augval836 = (_a823).ay2;
            var _child837 = (_a823)._left7;
            if (!((_child837) == null)) {
                var _val838 = (_child837)._max_ay24;
                _augval836 = ((_augval836) < (_val838)) ? (_val838) : (_augval836);
            }
            var _child839 = (_a823)._right8;
            if (!((_child839) == null)) {
                var _val840 = (_child839)._max_ay24;
                _augval836 = ((_augval836) < (_val840)) ? (_val840) : (_augval836);
            }
            (_a823)._max_ay24 = _augval836;
            (_a823)._height10 = 1 + ((((((_a823)._left7) == null) ? (-1) : (((_a823)._left7)._height10)) > ((((_a823)._right8) == null) ? (-1) : (((_a823)._right8)._height10))) ? ((((_a823)._left7) == null) ? (-1) : (((_a823)._left7)._height10)) : ((((_a823)._right8) == null) ? (-1) : (((_a823)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval841 = (_b824).ax1;
            var _child842 = (_b824)._left7;
            if (!((_child842) == null)) {
                var _val843 = (_child842)._min_ax12;
                _augval841 = ((_augval841) < (_val843)) ? (_augval841) : (_val843);
            }
            var _child844 = (_b824)._right8;
            if (!((_child844) == null)) {
                var _val845 = (_child844)._min_ax12;
                _augval841 = ((_augval841) < (_val845)) ? (_augval841) : (_val845);
            }
            (_b824)._min_ax12 = _augval841;
            /* _min_ay13 is min of ay1 */
            var _augval846 = (_b824).ay1;
            var _child847 = (_b824)._left7;
            if (!((_child847) == null)) {
                var _val848 = (_child847)._min_ay13;
                _augval846 = ((_augval846) < (_val848)) ? (_augval846) : (_val848);
            }
            var _child849 = (_b824)._right8;
            if (!((_child849) == null)) {
                var _val850 = (_child849)._min_ay13;
                _augval846 = ((_augval846) < (_val850)) ? (_augval846) : (_val850);
            }
            (_b824)._min_ay13 = _augval846;
            /* _max_ay24 is max of ay2 */
            var _augval851 = (_b824).ay2;
            var _child852 = (_b824)._left7;
            if (!((_child852) == null)) {
                var _val853 = (_child852)._max_ay24;
                _augval851 = ((_augval851) < (_val853)) ? (_val853) : (_augval851);
            }
            var _child854 = (_b824)._right8;
            if (!((_child854) == null)) {
                var _val855 = (_child854)._max_ay24;
                _augval851 = ((_augval851) < (_val855)) ? (_val855) : (_augval851);
            }
            (_b824)._max_ay24 = _augval851;
            (_b824)._height10 = 1 + ((((((_b824)._left7) == null) ? (-1) : (((_b824)._left7)._height10)) > ((((_b824)._right8) == null) ? (-1) : (((_b824)._right8)._height10))) ? ((((_b824)._left7) == null) ? (-1) : (((_b824)._left7)._height10)) : ((((_b824)._right8) == null) ? (-1) : (((_b824)._right8)._height10)));
            if (!(((_b824)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval856 = ((_b824)._parent9).ax1;
                var _child857 = ((_b824)._parent9)._left7;
                if (!((_child857) == null)) {
                    var _val858 = (_child857)._min_ax12;
                    _augval856 = ((_augval856) < (_val858)) ? (_augval856) : (_val858);
                }
                var _child859 = ((_b824)._parent9)._right8;
                if (!((_child859) == null)) {
                    var _val860 = (_child859)._min_ax12;
                    _augval856 = ((_augval856) < (_val860)) ? (_augval856) : (_val860);
                }
                ((_b824)._parent9)._min_ax12 = _augval856;
                /* _min_ay13 is min of ay1 */
                var _augval861 = ((_b824)._parent9).ay1;
                var _child862 = ((_b824)._parent9)._left7;
                if (!((_child862) == null)) {
                    var _val863 = (_child862)._min_ay13;
                    _augval861 = ((_augval861) < (_val863)) ? (_augval861) : (_val863);
                }
                var _child864 = ((_b824)._parent9)._right8;
                if (!((_child864) == null)) {
                    var _val865 = (_child864)._min_ay13;
                    _augval861 = ((_augval861) < (_val865)) ? (_augval861) : (_val865);
                }
                ((_b824)._parent9)._min_ay13 = _augval861;
                /* _max_ay24 is max of ay2 */
                var _augval866 = ((_b824)._parent9).ay2;
                var _child867 = ((_b824)._parent9)._left7;
                if (!((_child867) == null)) {
                    var _val868 = (_child867)._max_ay24;
                    _augval866 = ((_augval866) < (_val868)) ? (_val868) : (_augval866);
                }
                var _child869 = ((_b824)._parent9)._right8;
                if (!((_child869) == null)) {
                    var _val870 = (_child869)._max_ay24;
                    _augval866 = ((_augval866) < (_val870)) ? (_val870) : (_augval866);
                }
                ((_b824)._parent9)._max_ay24 = _augval866;
                ((_b824)._parent9)._height10 = 1 + (((((((_b824)._parent9)._left7) == null) ? (-1) : ((((_b824)._parent9)._left7)._height10)) > (((((_b824)._parent9)._right8) == null) ? (-1) : ((((_b824)._parent9)._right8)._height10))) ? (((((_b824)._parent9)._left7) == null) ? (-1) : ((((_b824)._parent9)._left7)._height10)) : (((((_b824)._parent9)._right8) == null) ? (-1) : ((((_b824)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b824;
            }
            _cursor773 = (_cursor773)._parent9;
        } else if ((_imbalance774) < (-1)) {
            if ((((((_cursor773)._right8)._left7) == null) ? (-1) : ((((_cursor773)._right8)._left7)._height10)) > (((((_cursor773)._right8)._right8) == null) ? (-1) : ((((_cursor773)._right8)._right8)._height10))) {
                /* rotate ((_cursor773)._right8)._left7 */
                var _a871 = (_cursor773)._right8;
                var _b872 = (_a871)._left7;
                var _c873 = (_b872)._right8;
                /* replace _a871 with _b872 in (_a871)._parent9 */
                if (!(((_a871)._parent9) == null)) {
                    if ((((_a871)._parent9)._left7) == (_a871)) {
                        ((_a871)._parent9)._left7 = _b872;
                    } else {
                        ((_a871)._parent9)._right8 = _b872;
                    }
                }
                if (!((_b872) == null)) {
                    (_b872)._parent9 = (_a871)._parent9;
                }
                /* replace _c873 with _a871 in _b872 */
                (_b872)._right8 = _a871;
                if (!((_a871) == null)) {
                    (_a871)._parent9 = _b872;
                }
                /* replace _b872 with _c873 in _a871 */
                (_a871)._left7 = _c873;
                if (!((_c873) == null)) {
                    (_c873)._parent9 = _a871;
                }
                /* _min_ax12 is min of ax1 */
                var _augval874 = (_a871).ax1;
                var _child875 = (_a871)._left7;
                if (!((_child875) == null)) {
                    var _val876 = (_child875)._min_ax12;
                    _augval874 = ((_augval874) < (_val876)) ? (_augval874) : (_val876);
                }
                var _child877 = (_a871)._right8;
                if (!((_child877) == null)) {
                    var _val878 = (_child877)._min_ax12;
                    _augval874 = ((_augval874) < (_val878)) ? (_augval874) : (_val878);
                }
                (_a871)._min_ax12 = _augval874;
                /* _min_ay13 is min of ay1 */
                var _augval879 = (_a871).ay1;
                var _child880 = (_a871)._left7;
                if (!((_child880) == null)) {
                    var _val881 = (_child880)._min_ay13;
                    _augval879 = ((_augval879) < (_val881)) ? (_augval879) : (_val881);
                }
                var _child882 = (_a871)._right8;
                if (!((_child882) == null)) {
                    var _val883 = (_child882)._min_ay13;
                    _augval879 = ((_augval879) < (_val883)) ? (_augval879) : (_val883);
                }
                (_a871)._min_ay13 = _augval879;
                /* _max_ay24 is max of ay2 */
                var _augval884 = (_a871).ay2;
                var _child885 = (_a871)._left7;
                if (!((_child885) == null)) {
                    var _val886 = (_child885)._max_ay24;
                    _augval884 = ((_augval884) < (_val886)) ? (_val886) : (_augval884);
                }
                var _child887 = (_a871)._right8;
                if (!((_child887) == null)) {
                    var _val888 = (_child887)._max_ay24;
                    _augval884 = ((_augval884) < (_val888)) ? (_val888) : (_augval884);
                }
                (_a871)._max_ay24 = _augval884;
                (_a871)._height10 = 1 + ((((((_a871)._left7) == null) ? (-1) : (((_a871)._left7)._height10)) > ((((_a871)._right8) == null) ? (-1) : (((_a871)._right8)._height10))) ? ((((_a871)._left7) == null) ? (-1) : (((_a871)._left7)._height10)) : ((((_a871)._right8) == null) ? (-1) : (((_a871)._right8)._height10)));
                /* _min_ax12 is min of ax1 */
                var _augval889 = (_b872).ax1;
                var _child890 = (_b872)._left7;
                if (!((_child890) == null)) {
                    var _val891 = (_child890)._min_ax12;
                    _augval889 = ((_augval889) < (_val891)) ? (_augval889) : (_val891);
                }
                var _child892 = (_b872)._right8;
                if (!((_child892) == null)) {
                    var _val893 = (_child892)._min_ax12;
                    _augval889 = ((_augval889) < (_val893)) ? (_augval889) : (_val893);
                }
                (_b872)._min_ax12 = _augval889;
                /* _min_ay13 is min of ay1 */
                var _augval894 = (_b872).ay1;
                var _child895 = (_b872)._left7;
                if (!((_child895) == null)) {
                    var _val896 = (_child895)._min_ay13;
                    _augval894 = ((_augval894) < (_val896)) ? (_augval894) : (_val896);
                }
                var _child897 = (_b872)._right8;
                if (!((_child897) == null)) {
                    var _val898 = (_child897)._min_ay13;
                    _augval894 = ((_augval894) < (_val898)) ? (_augval894) : (_val898);
                }
                (_b872)._min_ay13 = _augval894;
                /* _max_ay24 is max of ay2 */
                var _augval899 = (_b872).ay2;
                var _child900 = (_b872)._left7;
                if (!((_child900) == null)) {
                    var _val901 = (_child900)._max_ay24;
                    _augval899 = ((_augval899) < (_val901)) ? (_val901) : (_augval899);
                }
                var _child902 = (_b872)._right8;
                if (!((_child902) == null)) {
                    var _val903 = (_child902)._max_ay24;
                    _augval899 = ((_augval899) < (_val903)) ? (_val903) : (_augval899);
                }
                (_b872)._max_ay24 = _augval899;
                (_b872)._height10 = 1 + ((((((_b872)._left7) == null) ? (-1) : (((_b872)._left7)._height10)) > ((((_b872)._right8) == null) ? (-1) : (((_b872)._right8)._height10))) ? ((((_b872)._left7) == null) ? (-1) : (((_b872)._left7)._height10)) : ((((_b872)._right8) == null) ? (-1) : (((_b872)._right8)._height10)));
                if (!(((_b872)._parent9) == null)) {
                    /* _min_ax12 is min of ax1 */
                    var _augval904 = ((_b872)._parent9).ax1;
                    var _child905 = ((_b872)._parent9)._left7;
                    if (!((_child905) == null)) {
                        var _val906 = (_child905)._min_ax12;
                        _augval904 = ((_augval904) < (_val906)) ? (_augval904) : (_val906);
                    }
                    var _child907 = ((_b872)._parent9)._right8;
                    if (!((_child907) == null)) {
                        var _val908 = (_child907)._min_ax12;
                        _augval904 = ((_augval904) < (_val908)) ? (_augval904) : (_val908);
                    }
                    ((_b872)._parent9)._min_ax12 = _augval904;
                    /* _min_ay13 is min of ay1 */
                    var _augval909 = ((_b872)._parent9).ay1;
                    var _child910 = ((_b872)._parent9)._left7;
                    if (!((_child910) == null)) {
                        var _val911 = (_child910)._min_ay13;
                        _augval909 = ((_augval909) < (_val911)) ? (_augval909) : (_val911);
                    }
                    var _child912 = ((_b872)._parent9)._right8;
                    if (!((_child912) == null)) {
                        var _val913 = (_child912)._min_ay13;
                        _augval909 = ((_augval909) < (_val913)) ? (_augval909) : (_val913);
                    }
                    ((_b872)._parent9)._min_ay13 = _augval909;
                    /* _max_ay24 is max of ay2 */
                    var _augval914 = ((_b872)._parent9).ay2;
                    var _child915 = ((_b872)._parent9)._left7;
                    if (!((_child915) == null)) {
                        var _val916 = (_child915)._max_ay24;
                        _augval914 = ((_augval914) < (_val916)) ? (_val916) : (_augval914);
                    }
                    var _child917 = ((_b872)._parent9)._right8;
                    if (!((_child917) == null)) {
                        var _val918 = (_child917)._max_ay24;
                        _augval914 = ((_augval914) < (_val918)) ? (_val918) : (_augval914);
                    }
                    ((_b872)._parent9)._max_ay24 = _augval914;
                    ((_b872)._parent9)._height10 = 1 + (((((((_b872)._parent9)._left7) == null) ? (-1) : ((((_b872)._parent9)._left7)._height10)) > (((((_b872)._parent9)._right8) == null) ? (-1) : ((((_b872)._parent9)._right8)._height10))) ? (((((_b872)._parent9)._left7) == null) ? (-1) : ((((_b872)._parent9)._left7)._height10)) : (((((_b872)._parent9)._right8) == null) ? (-1) : ((((_b872)._parent9)._right8)._height10)));
                } else {
                    (this)._root1 = _b872;
                }
            }
            /* rotate (_cursor773)._right8 */
            var _a919 = _cursor773;
            var _b920 = (_a919)._right8;
            var _c921 = (_b920)._left7;
            /* replace _a919 with _b920 in (_a919)._parent9 */
            if (!(((_a919)._parent9) == null)) {
                if ((((_a919)._parent9)._left7) == (_a919)) {
                    ((_a919)._parent9)._left7 = _b920;
                } else {
                    ((_a919)._parent9)._right8 = _b920;
                }
            }
            if (!((_b920) == null)) {
                (_b920)._parent9 = (_a919)._parent9;
            }
            /* replace _c921 with _a919 in _b920 */
            (_b920)._left7 = _a919;
            if (!((_a919) == null)) {
                (_a919)._parent9 = _b920;
            }
            /* replace _b920 with _c921 in _a919 */
            (_a919)._right8 = _c921;
            if (!((_c921) == null)) {
                (_c921)._parent9 = _a919;
            }
            /* _min_ax12 is min of ax1 */
            var _augval922 = (_a919).ax1;
            var _child923 = (_a919)._left7;
            if (!((_child923) == null)) {
                var _val924 = (_child923)._min_ax12;
                _augval922 = ((_augval922) < (_val924)) ? (_augval922) : (_val924);
            }
            var _child925 = (_a919)._right8;
            if (!((_child925) == null)) {
                var _val926 = (_child925)._min_ax12;
                _augval922 = ((_augval922) < (_val926)) ? (_augval922) : (_val926);
            }
            (_a919)._min_ax12 = _augval922;
            /* _min_ay13 is min of ay1 */
            var _augval927 = (_a919).ay1;
            var _child928 = (_a919)._left7;
            if (!((_child928) == null)) {
                var _val929 = (_child928)._min_ay13;
                _augval927 = ((_augval927) < (_val929)) ? (_augval927) : (_val929);
            }
            var _child930 = (_a919)._right8;
            if (!((_child930) == null)) {
                var _val931 = (_child930)._min_ay13;
                _augval927 = ((_augval927) < (_val931)) ? (_augval927) : (_val931);
            }
            (_a919)._min_ay13 = _augval927;
            /* _max_ay24 is max of ay2 */
            var _augval932 = (_a919).ay2;
            var _child933 = (_a919)._left7;
            if (!((_child933) == null)) {
                var _val934 = (_child933)._max_ay24;
                _augval932 = ((_augval932) < (_val934)) ? (_val934) : (_augval932);
            }
            var _child935 = (_a919)._right8;
            if (!((_child935) == null)) {
                var _val936 = (_child935)._max_ay24;
                _augval932 = ((_augval932) < (_val936)) ? (_val936) : (_augval932);
            }
            (_a919)._max_ay24 = _augval932;
            (_a919)._height10 = 1 + ((((((_a919)._left7) == null) ? (-1) : (((_a919)._left7)._height10)) > ((((_a919)._right8) == null) ? (-1) : (((_a919)._right8)._height10))) ? ((((_a919)._left7) == null) ? (-1) : (((_a919)._left7)._height10)) : ((((_a919)._right8) == null) ? (-1) : (((_a919)._right8)._height10)));
            /* _min_ax12 is min of ax1 */
            var _augval937 = (_b920).ax1;
            var _child938 = (_b920)._left7;
            if (!((_child938) == null)) {
                var _val939 = (_child938)._min_ax12;
                _augval937 = ((_augval937) < (_val939)) ? (_augval937) : (_val939);
            }
            var _child940 = (_b920)._right8;
            if (!((_child940) == null)) {
                var _val941 = (_child940)._min_ax12;
                _augval937 = ((_augval937) < (_val941)) ? (_augval937) : (_val941);
            }
            (_b920)._min_ax12 = _augval937;
            /* _min_ay13 is min of ay1 */
            var _augval942 = (_b920).ay1;
            var _child943 = (_b920)._left7;
            if (!((_child943) == null)) {
                var _val944 = (_child943)._min_ay13;
                _augval942 = ((_augval942) < (_val944)) ? (_augval942) : (_val944);
            }
            var _child945 = (_b920)._right8;
            if (!((_child945) == null)) {
                var _val946 = (_child945)._min_ay13;
                _augval942 = ((_augval942) < (_val946)) ? (_augval942) : (_val946);
            }
            (_b920)._min_ay13 = _augval942;
            /* _max_ay24 is max of ay2 */
            var _augval947 = (_b920).ay2;
            var _child948 = (_b920)._left7;
            if (!((_child948) == null)) {
                var _val949 = (_child948)._max_ay24;
                _augval947 = ((_augval947) < (_val949)) ? (_val949) : (_augval947);
            }
            var _child950 = (_b920)._right8;
            if (!((_child950) == null)) {
                var _val951 = (_child950)._max_ay24;
                _augval947 = ((_augval947) < (_val951)) ? (_val951) : (_augval947);
            }
            (_b920)._max_ay24 = _augval947;
            (_b920)._height10 = 1 + ((((((_b920)._left7) == null) ? (-1) : (((_b920)._left7)._height10)) > ((((_b920)._right8) == null) ? (-1) : (((_b920)._right8)._height10))) ? ((((_b920)._left7) == null) ? (-1) : (((_b920)._left7)._height10)) : ((((_b920)._right8) == null) ? (-1) : (((_b920)._right8)._height10)));
            if (!(((_b920)._parent9) == null)) {
                /* _min_ax12 is min of ax1 */
                var _augval952 = ((_b920)._parent9).ax1;
                var _child953 = ((_b920)._parent9)._left7;
                if (!((_child953) == null)) {
                    var _val954 = (_child953)._min_ax12;
                    _augval952 = ((_augval952) < (_val954)) ? (_augval952) : (_val954);
                }
                var _child955 = ((_b920)._parent9)._right8;
                if (!((_child955) == null)) {
                    var _val956 = (_child955)._min_ax12;
                    _augval952 = ((_augval952) < (_val956)) ? (_augval952) : (_val956);
                }
                ((_b920)._parent9)._min_ax12 = _augval952;
                /* _min_ay13 is min of ay1 */
                var _augval957 = ((_b920)._parent9).ay1;
                var _child958 = ((_b920)._parent9)._left7;
                if (!((_child958) == null)) {
                    var _val959 = (_child958)._min_ay13;
                    _augval957 = ((_augval957) < (_val959)) ? (_augval957) : (_val959);
                }
                var _child960 = ((_b920)._parent9)._right8;
                if (!((_child960) == null)) {
                    var _val961 = (_child960)._min_ay13;
                    _augval957 = ((_augval957) < (_val961)) ? (_augval957) : (_val961);
                }
                ((_b920)._parent9)._min_ay13 = _augval957;
                /* _max_ay24 is max of ay2 */
                var _augval962 = ((_b920)._parent9).ay2;
                var _child963 = ((_b920)._parent9)._left7;
                if (!((_child963) == null)) {
                    var _val964 = (_child963)._max_ay24;
                    _augval962 = ((_augval962) < (_val964)) ? (_val964) : (_augval962);
                }
                var _child965 = ((_b920)._parent9)._right8;
                if (!((_child965) == null)) {
                    var _val966 = (_child965)._max_ay24;
                    _augval962 = ((_augval962) < (_val966)) ? (_val966) : (_augval962);
                }
                ((_b920)._parent9)._max_ay24 = _augval962;
                ((_b920)._parent9)._height10 = 1 + (((((((_b920)._parent9)._left7) == null) ? (-1) : ((((_b920)._parent9)._left7)._height10)) > (((((_b920)._parent9)._right8) == null) ? (-1) : ((((_b920)._parent9)._right8)._height10))) ? (((((_b920)._parent9)._left7) == null) ? (-1) : ((((_b920)._parent9)._left7)._height10)) : (((((_b920)._parent9)._right8) == null) ? (-1) : ((((_b920)._parent9)._right8)._height10)));
            } else {
                (this)._root1 = _b920;
            }
            _cursor773 = (_cursor773)._parent9;
        }
    }
    (__x).ax1 = ax1;
    (__x).ay1 = ay1;
    (__x).ax2 = ax2;
    (__x).ay2 = ay2;
}
RectangleHolder.prototype.findMatchingRectangles = function (bx1, by1, bx2, by2, __callback) {
    var _root967 = (this)._root1;
    var _x968 = _root967;
    var _descend969 = true;
    var _from_left970 = true;
    while (true) {
        if ((_x968) == null) {
            _x968 = null;
            break;
        }
        if (_descend969) {
            /* too small? */
            if ((false) || (((_x968).ax2) <= (bx1))) {
                if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                    if ((_x968) == (_root967)) {
                        _root967 = (_x968)._right8;
                    }
                    _x968 = (_x968)._right8;
                } else if ((_x968) == (_root967)) {
                    _x968 = null;
                    break;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
            } else if ((!(((_x968)._left7) == null)) && ((((true) && ((((_x968)._left7)._min_ax12) < (bx2))) && ((((_x968)._left7)._min_ay13) < (by2))) && ((((_x968)._left7)._max_ay24) > (by1)))) {
                _x968 = (_x968)._left7;
                /* too large? */
            } else if (false) {
                if ((_x968) == (_root967)) {
                    _x968 = null;
                    break;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
                /* node ok? */
            } else if ((((true) && (((_x968).ax1) < (bx2))) && (((_x968).ay1) < (by2))) && (((_x968).ay2) > (by1))) {
                break;
            } else if ((_x968) == (_root967)) {
                _root967 = (_x968)._right8;
                _x968 = (_x968)._right8;
            } else {
                if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                    if ((_x968) == (_root967)) {
                        _root967 = (_x968)._right8;
                    }
                    _x968 = (_x968)._right8;
                } else {
                    _descend969 = false;
                    _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                    _x968 = (_x968)._parent9;
                }
            }
        } else if (_from_left970) {
            if (false) {
                _x968 = null;
                break;
            } else if ((((true) && (((_x968).ax1) < (bx2))) && (((_x968).ay1) < (by2))) && (((_x968).ay2) > (by1))) {
                break;
            } else if ((!(((_x968)._right8) == null)) && ((((true) && ((((_x968)._right8)._min_ax12) < (bx2))) && ((((_x968)._right8)._min_ay13) < (by2))) && ((((_x968)._right8)._max_ay24) > (by1)))) {
                _descend969 = true;
                if ((_x968) == (_root967)) {
                    _root967 = (_x968)._right8;
                }
                _x968 = (_x968)._right8;
            } else if ((_x968) == (_root967)) {
                _x968 = null;
                break;
            } else {
                _descend969 = false;
                _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                _x968 = (_x968)._parent9;
            }
        } else {
            if ((_x968) == (_root967)) {
                _x968 = null;
                break;
            } else {
                _descend969 = false;
                _from_left970 = (!(((_x968)._parent9) == null)) && ((_x968) == (((_x968)._parent9)._left7));
                _x968 = (_x968)._parent9;
            }
        }
    }
    var _prev_cursor5 = null;
    var _cursor6 = _x968;
    for (; ;) {
        if (!(!((_cursor6) == null))) break;
        var _name971 = _cursor6;
        /* ADVANCE */
        _prev_cursor5 = _cursor6;
        do {
            var _right_min972 = null;
            if ((!(((_cursor6)._right8) == null)) && ((((true) && ((((_cursor6)._right8)._min_ax12) < (bx2))) && ((((_cursor6)._right8)._min_ay13) < (by2))) && ((((_cursor6)._right8)._max_ay24) > (by1)))) {
                var _root973 = (_cursor6)._right8;
                var _x974 = _root973;
                var _descend975 = true;
                var _from_left976 = true;
                while (true) {
                    if ((_x974) == null) {
                        _x974 = null;
                        break;
                    }
                    if (_descend975) {
                        /* too small? */
                        if ((false) || (((_x974).ax2) <= (bx1))) {
                            if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                                if ((_x974) == (_root973)) {
                                    _root973 = (_x974)._right8;
                                }
                                _x974 = (_x974)._right8;
                            } else if ((_x974) == (_root973)) {
                                _x974 = null;
                                break;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                        } else if ((!(((_x974)._left7) == null)) && ((((true) && ((((_x974)._left7)._min_ax12) < (bx2))) && ((((_x974)._left7)._min_ay13) < (by2))) && ((((_x974)._left7)._max_ay24) > (by1)))) {
                            _x974 = (_x974)._left7;
                            /* too large? */
                        } else if (false) {
                            if ((_x974) == (_root973)) {
                                _x974 = null;
                                break;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                            /* node ok? */
                        } else if ((((true) && (((_x974).ax1) < (bx2))) && (((_x974).ay1) < (by2))) && (((_x974).ay2) > (by1))) {
                            break;
                        } else if ((_x974) == (_root973)) {
                            _root973 = (_x974)._right8;
                            _x974 = (_x974)._right8;
                        } else {
                            if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                                if ((_x974) == (_root973)) {
                                    _root973 = (_x974)._right8;
                                }
                                _x974 = (_x974)._right8;
                            } else {
                                _descend975 = false;
                                _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                                _x974 = (_x974)._parent9;
                            }
                        }
                    } else if (_from_left976) {
                        if (false) {
                            _x974 = null;
                            break;
                        } else if ((((true) && (((_x974).ax1) < (bx2))) && (((_x974).ay1) < (by2))) && (((_x974).ay2) > (by1))) {
                            break;
                        } else if ((!(((_x974)._right8) == null)) && ((((true) && ((((_x974)._right8)._min_ax12) < (bx2))) && ((((_x974)._right8)._min_ay13) < (by2))) && ((((_x974)._right8)._max_ay24) > (by1)))) {
                            _descend975 = true;
                            if ((_x974) == (_root973)) {
                                _root973 = (_x974)._right8;
                            }
                            _x974 = (_x974)._right8;
                        } else if ((_x974) == (_root973)) {
                            _x974 = null;
                            break;
                        } else {
                            _descend975 = false;
                            _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                            _x974 = (_x974)._parent9;
                        }
                    } else {
                        if ((_x974) == (_root973)) {
                            _x974 = null;
                            break;
                        } else {
                            _descend975 = false;
                            _from_left976 = (!(((_x974)._parent9) == null)) && ((_x974) == (((_x974)._parent9)._left7));
                            _x974 = (_x974)._parent9;
                        }
                    }
                }
                _right_min972 = _x974;
            }
            if (!((_right_min972) == null)) {
                _cursor6 = _right_min972;
                break;
            } else {
                while ((!(((_cursor6)._parent9) == null)) && ((_cursor6) == (((_cursor6)._parent9)._right8))) {
                    _cursor6 = (_cursor6)._parent9;
                }
                _cursor6 = (_cursor6)._parent9;
                if ((!((_cursor6) == null)) && (false)) {
                    _cursor6 = null;
                }
            }
        } while ((!((_cursor6) == null)) && (!((((true) && (((_cursor6).ax1) < (bx2))) && (((_cursor6).ay1) < (by2))) && (((_cursor6).ay2) > (by1)))));
        if (__callback(_name971)) {
            var _to_remove977 = _prev_cursor5;
            var _parent978 = (_to_remove977)._parent9;
            var _left979 = (_to_remove977)._left7;
            var _right980 = (_to_remove977)._right8;
            var _new_x981;
            if (((_left979) == null) && ((_right980) == null)) {
                _new_x981 = null;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else if ((!((_left979) == null)) && ((_right980) == null)) {
                _new_x981 = _left979;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else if (((_left979) == null) && (!((_right980) == null))) {
                _new_x981 = _right980;
                /* replace _to_remove977 with _new_x981 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _new_x981;
                    } else {
                        (_parent978)._right8 = _new_x981;
                    }
                }
                if (!((_new_x981) == null)) {
                    (_new_x981)._parent9 = _parent978;
                }
            } else {
                var _root982 = (_to_remove977)._right8;
                var _x983 = _root982;
                var _descend984 = true;
                var _from_left985 = true;
                while (true) {
                    if ((_x983) == null) {
                        _x983 = null;
                        break;
                    }
                    if (_descend984) {
                        /* too small? */
                        if (false) {
                            if ((!(((_x983)._right8) == null)) && (true)) {
                                if ((_x983) == (_root982)) {
                                    _root982 = (_x983)._right8;
                                }
                                _x983 = (_x983)._right8;
                            } else if ((_x983) == (_root982)) {
                                _x983 = null;
                                break;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                        } else if ((!(((_x983)._left7) == null)) && (true)) {
                            _x983 = (_x983)._left7;
                            /* too large? */
                        } else if (false) {
                            if ((_x983) == (_root982)) {
                                _x983 = null;
                                break;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                            /* node ok? */
                        } else if (true) {
                            break;
                        } else if ((_x983) == (_root982)) {
                            _root982 = (_x983)._right8;
                            _x983 = (_x983)._right8;
                        } else {
                            if ((!(((_x983)._right8) == null)) && (true)) {
                                if ((_x983) == (_root982)) {
                                    _root982 = (_x983)._right8;
                                }
                                _x983 = (_x983)._right8;
                            } else {
                                _descend984 = false;
                                _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                                _x983 = (_x983)._parent9;
                            }
                        }
                    } else if (_from_left985) {
                        if (false) {
                            _x983 = null;
                            break;
                        } else if (true) {
                            break;
                        } else if ((!(((_x983)._right8) == null)) && (true)) {
                            _descend984 = true;
                            if ((_x983) == (_root982)) {
                                _root982 = (_x983)._right8;
                            }
                            _x983 = (_x983)._right8;
                        } else if ((_x983) == (_root982)) {
                            _x983 = null;
                            break;
                        } else {
                            _descend984 = false;
                            _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                            _x983 = (_x983)._parent9;
                        }
                    } else {
                        if ((_x983) == (_root982)) {
                            _x983 = null;
                            break;
                        } else {
                            _descend984 = false;
                            _from_left985 = (!(((_x983)._parent9) == null)) && ((_x983) == (((_x983)._parent9)._left7));
                            _x983 = (_x983)._parent9;
                        }
                    }
                }
                _new_x981 = _x983;
                var _mp986 = (_x983)._parent9;
                var _mr987 = (_x983)._right8;
                /* replace _x983 with _mr987 in _mp986 */
                if (!((_mp986) == null)) {
                    if (((_mp986)._left7) == (_x983)) {
                        (_mp986)._left7 = _mr987;
                    } else {
                        (_mp986)._right8 = _mr987;
                    }
                }
                if (!((_mr987) == null)) {
                    (_mr987)._parent9 = _mp986;
                }
                /* replace _to_remove977 with _x983 in _parent978 */
                if (!((_parent978) == null)) {
                    if (((_parent978)._left7) == (_to_remove977)) {
                        (_parent978)._left7 = _x983;
                    } else {
                        (_parent978)._right8 = _x983;
                    }
                }
                if (!((_x983) == null)) {
                    (_x983)._parent9 = _parent978;
                }
                /* replace null with _left979 in _x983 */
                (_x983)._left7 = _left979;
                if (!((_left979) == null)) {
                    (_left979)._parent9 = _x983;
                }
                /* replace _mr987 with (_to_remove977)._right8 in _x983 */
                (_x983)._right8 = (_to_remove977)._right8;
                if (!(((_to_remove977)._right8) == null)) {
                    ((_to_remove977)._right8)._parent9 = _x983;
                }
                /* _min_ax12 is min of ax1 */
                var _augval988 = (_x983).ax1;
                var _child989 = (_x983)._left7;
                if (!((_child989) == null)) {
                    var _val990 = (_child989)._min_ax12;
                    _augval988 = ((_augval988) < (_val990)) ? (_augval988) : (_val990);
                }
                var _child991 = (_x983)._right8;
                if (!((_child991) == null)) {
                    var _val992 = (_child991)._min_ax12;
                    _augval988 = ((_augval988) < (_val992)) ? (_augval988) : (_val992);
                }
                (_x983)._min_ax12 = _augval988;
                /* _min_ay13 is min of ay1 */
                var _augval993 = (_x983).ay1;
                var _child994 = (_x983)._left7;
                if (!((_child994) == null)) {
                    var _val995 = (_child994)._min_ay13;
                    _augval993 = ((_augval993) < (_val995)) ? (_augval993) : (_val995);
                }
                var _child996 = (_x983)._right8;
                if (!((_child996) == null)) {
                    var _val997 = (_child996)._min_ay13;
                    _augval993 = ((_augval993) < (_val997)) ? (_augval993) : (_val997);
                }
                (_x983)._min_ay13 = _augval993;
                /* _max_ay24 is max of ay2 */
                var _augval998 = (_x983).ay2;
                var _child999 = (_x983)._left7;
                if (!((_child999) == null)) {
                    var _val1000 = (_child999)._max_ay24;
                    _augval998 = ((_augval998) < (_val1000)) ? (_val1000) : (_augval998);
                }
                var _child1001 = (_x983)._right8;
                if (!((_child1001) == null)) {
                    var _val1002 = (_child1001)._max_ay24;
                    _augval998 = ((_augval998) < (_val1002)) ? (_val1002) : (_augval998);
                }
                (_x983)._max_ay24 = _augval998;
                (_x983)._height10 = 1 + ((((((_x983)._left7) == null) ? (-1) : (((_x983)._left7)._height10)) > ((((_x983)._right8) == null) ? (-1) : (((_x983)._right8)._height10))) ? ((((_x983)._left7) == null) ? (-1) : (((_x983)._left7)._height10)) : ((((_x983)._right8) == null) ? (-1) : (((_x983)._right8)._height10)));
                var _cursor1003 = _mp986;
                var _changed1004 = true;
                while ((_changed1004) && (!((_cursor1003) == (_parent978)))) {
                    var _old__min_ax121005 = (_cursor1003)._min_ax12;
                    var _old__min_ay131006 = (_cursor1003)._min_ay13;
                    var _old__max_ay241007 = (_cursor1003)._max_ay24;
                    var _old_height1008 = (_cursor1003)._height10;
                    /* _min_ax12 is min of ax1 */
                    var _augval1009 = (_cursor1003).ax1;
                    var _child1010 = (_cursor1003)._left7;
                    if (!((_child1010) == null)) {
                        var _val1011 = (_child1010)._min_ax12;
                        _augval1009 = ((_augval1009) < (_val1011)) ? (_augval1009) : (_val1011);
                    }
                    var _child1012 = (_cursor1003)._right8;
                    if (!((_child1012) == null)) {
                        var _val1013 = (_child1012)._min_ax12;
                        _augval1009 = ((_augval1009) < (_val1013)) ? (_augval1009) : (_val1013);
                    }
                    (_cursor1003)._min_ax12 = _augval1009;
                    /* _min_ay13 is min of ay1 */
                    var _augval1014 = (_cursor1003).ay1;
                    var _child1015 = (_cursor1003)._left7;
                    if (!((_child1015) == null)) {
                        var _val1016 = (_child1015)._min_ay13;
                        _augval1014 = ((_augval1014) < (_val1016)) ? (_augval1014) : (_val1016);
                    }
                    var _child1017 = (_cursor1003)._right8;
                    if (!((_child1017) == null)) {
                        var _val1018 = (_child1017)._min_ay13;
                        _augval1014 = ((_augval1014) < (_val1018)) ? (_augval1014) : (_val1018);
                    }
                    (_cursor1003)._min_ay13 = _augval1014;
                    /* _max_ay24 is max of ay2 */
                    var _augval1019 = (_cursor1003).ay2;
                    var _child1020 = (_cursor1003)._left7;
                    if (!((_child1020) == null)) {
                        var _val1021 = (_child1020)._max_ay24;
                        _augval1019 = ((_augval1019) < (_val1021)) ? (_val1021) : (_augval1019);
                    }
                    var _child1022 = (_cursor1003)._right8;
                    if (!((_child1022) == null)) {
                        var _val1023 = (_child1022)._max_ay24;
                        _augval1019 = ((_augval1019) < (_val1023)) ? (_val1023) : (_augval1019);
                    }
                    (_cursor1003)._max_ay24 = _augval1019;
                    (_cursor1003)._height10 = 1 + ((((((_cursor1003)._left7) == null) ? (-1) : (((_cursor1003)._left7)._height10)) > ((((_cursor1003)._right8) == null) ? (-1) : (((_cursor1003)._right8)._height10))) ? ((((_cursor1003)._left7) == null) ? (-1) : (((_cursor1003)._left7)._height10)) : ((((_cursor1003)._right8) == null) ? (-1) : (((_cursor1003)._right8)._height10)));
                    _changed1004 = false;
                    _changed1004 = (_changed1004) || (!((_old__min_ax121005) == ((_cursor1003)._min_ax12)));
                    _changed1004 = (_changed1004) || (!((_old__min_ay131006) == ((_cursor1003)._min_ay13)));
                    _changed1004 = (_changed1004) || (!((_old__max_ay241007) == ((_cursor1003)._max_ay24)));
                    _changed1004 = (_changed1004) || (!((_old_height1008) == ((_cursor1003)._height10)));
                    _cursor1003 = (_cursor1003)._parent9;
                }
            }
            var _cursor1024 = _parent978;
            var _changed1025 = true;
            while ((_changed1025) && (!((_cursor1024) == (null)))) {
                var _old__min_ax121026 = (_cursor1024)._min_ax12;
                var _old__min_ay131027 = (_cursor1024)._min_ay13;
                var _old__max_ay241028 = (_cursor1024)._max_ay24;
                var _old_height1029 = (_cursor1024)._height10;
                /* _min_ax12 is min of ax1 */
                var _augval1030 = (_cursor1024).ax1;
                var _child1031 = (_cursor1024)._left7;
                if (!((_child1031) == null)) {
                    var _val1032 = (_child1031)._min_ax12;
                    _augval1030 = ((_augval1030) < (_val1032)) ? (_augval1030) : (_val1032);
                }
                var _child1033 = (_cursor1024)._right8;
                if (!((_child1033) == null)) {
                    var _val1034 = (_child1033)._min_ax12;
                    _augval1030 = ((_augval1030) < (_val1034)) ? (_augval1030) : (_val1034);
                }
                (_cursor1024)._min_ax12 = _augval1030;
                /* _min_ay13 is min of ay1 */
                var _augval1035 = (_cursor1024).ay1;
                var _child1036 = (_cursor1024)._left7;
                if (!((_child1036) == null)) {
                    var _val1037 = (_child1036)._min_ay13;
                    _augval1035 = ((_augval1035) < (_val1037)) ? (_augval1035) : (_val1037);
                }
                var _child1038 = (_cursor1024)._right8;
                if (!((_child1038) == null)) {
                    var _val1039 = (_child1038)._min_ay13;
                    _augval1035 = ((_augval1035) < (_val1039)) ? (_augval1035) : (_val1039);
                }
                (_cursor1024)._min_ay13 = _augval1035;
                /* _max_ay24 is max of ay2 */
                var _augval1040 = (_cursor1024).ay2;
                var _child1041 = (_cursor1024)._left7;
                if (!((_child1041) == null)) {
                    var _val1042 = (_child1041)._max_ay24;
                    _augval1040 = ((_augval1040) < (_val1042)) ? (_val1042) : (_augval1040);
                }
                var _child1043 = (_cursor1024)._right8;
                if (!((_child1043) == null)) {
                    var _val1044 = (_child1043)._max_ay24;
                    _augval1040 = ((_augval1040) < (_val1044)) ? (_val1044) : (_augval1040);
                }
                (_cursor1024)._max_ay24 = _augval1040;
                (_cursor1024)._height10 = 1 + ((((((_cursor1024)._left7) == null) ? (-1) : (((_cursor1024)._left7)._height10)) > ((((_cursor1024)._right8) == null) ? (-1) : (((_cursor1024)._right8)._height10))) ? ((((_cursor1024)._left7) == null) ? (-1) : (((_cursor1024)._left7)._height10)) : ((((_cursor1024)._right8) == null) ? (-1) : (((_cursor1024)._right8)._height10)));
                _changed1025 = false;
                _changed1025 = (_changed1025) || (!((_old__min_ax121026) == ((_cursor1024)._min_ax12)));
                _changed1025 = (_changed1025) || (!((_old__min_ay131027) == ((_cursor1024)._min_ay13)));
                _changed1025 = (_changed1025) || (!((_old__max_ay241028) == ((_cursor1024)._max_ay24)));
                _changed1025 = (_changed1025) || (!((_old_height1029) == ((_cursor1024)._height10)));
                _cursor1024 = (_cursor1024)._parent9;
            }
            if (((this)._root1) == (_to_remove977)) {
                (this)._root1 = _new_x981;
            }
            _prev_cursor5 = null;
        }
    };
}
; 
 
 buildViz = function (d3) {
    return function (widthInPixels = 1000,
                     heightInPixels = 600,
                     max_snippets = null,
                     color = null,
                     sortByDist = true,
                     useFullDoc = false,
                     greyZeroScores = false,
                     asianMode = false,
                     nonTextFeaturesMode = false,
                     showCharacteristic = true,
                     wordVecMaxPValue = false,
                     saveSvgButton = false,
                     reverseSortScoresForNotCategory = false,
                     minPVal = 0.1,
                     pValueColors = false,
                     xLabelText = null,
                     yLabelText = null,
                     fullData = null,
                     showTopTerms = true,
                     showNeutral = false,
                     getTooltipContent = null,
                     xAxisValues = null,
                     yAxisValues = null,
                     colorFunc = null,
                     showAxes = true,
                     showExtra = false,
                     doCensorPoints = true,
                     centerLabelsOverPoints = false,
                     xAxisLabels = null,
                     yAxisLabels = null,
                     topic_model_preview_size = 10,
                     verticalLines = null,
                     horizontal_line_y_position = null,
                     vertical_line_x_position = null,
                     unifiedContexts = false,
                     showCategoryHeadings = true,
                     showCrossAxes = true,
                     divName = 'd3-div-1',
                     alternativeTermFunc = null,
                     includeAllContexts = false,
                     showAxesAndCrossHairs = false,
                     x_axis_values_format = '.3f',
                     y_axis_values_format = '.3f',
                     matchFullLine = false,
                     maxOverlapping = -1,
                     showCorpusStats = true,
                     sortDocLabelsByName = false,
                     alwaysJump = true,
                     highlightSelectedCategory = false,
                     showDiagonal = false,
                     useGlobalScale = false,
                     enableTermCategoryDescription = true,
                     getCustomTermHtml = null,
                     headerNames = null,
                     headerSortingAlgos = null,
                     ignoreCategories = false,
                     backgroundLabels = null,
                     labelPriorityColumn = null,
                     textColorColumn = undefined,
                     suppressTextColumn = undefined,
                     backgroundColor = undefined,
                     censorPointColumn = undefined,
                     rightOrderColumn = undefined,
                     subwordEncoding = null,
                     topTermsLength = 14,
                     topTermsLeftBuffer = 0
    ) {
        function formatTermForDisplay(term) {
            if (subwordEncoding === 'RoBERTa' && (term.charCodeAt(0) === 288 || term.charCodeAt(0) === 289))
                term = '_' + term.substr(1, term.length - 1);
            return term;
        }

        //var divName = 'd3-div-1';
        // Set the dimensions of the canvas / graph
        var padding = {top: 30, right: 20, bottom: 30, left: 50};
        if (!showAxes) {
            padding = {top: 30, right: 20, bottom: 30, left: 50};
        }
        var margin = padding,
            width = widthInPixels - margin.left - margin.right,
            height = heightInPixels - margin.top - margin.bottom;
        fullData.data.forEach(function (x, i) {
            x.i = i
        });

        // Set the ranges
        var x = d3.scaleLinear().range([0, width]);
        var y = d3.scaleLinear().range([height, 0]);

        if (unifiedContexts) {
            document.querySelectorAll('#' + divName + '-' + 'notcol')
                .forEach(function (x) {
                    x.style.display = 'none'
                });
            document.querySelectorAll('.' + divName + '-' + 'contexts')
                .forEach(function (x) {
                    x.style.width = '90%'
                });
        } else if (showNeutral) {
            if (showExtra) {
                document.querySelectorAll('.' + divName + '-' + 'contexts')
                    .forEach(function (x) {
                        x.style.width = '25%'
                        x.style.float = 'left'
                    });

                ['notcol', 'neutcol', 'extracol'].forEach(function (columnName) {
                    document.querySelectorAll('#' + divName + '-' + columnName)
                        .forEach(function (x) {
                            x.style.display = 'inline'
                            x.style.float = 'left'
                            x.style.width = '25%'
                        });
                })

            } else {
                document.querySelectorAll('.' + divName + '-' + 'contexts')
                    .forEach(function (x) {
                        x.style.width = '33%'
                        x.style.float = 'left'
                    });

                ['notcol', 'neutcol'].forEach(function (columnName) {
                    document.querySelectorAll('#' + divName + '-' + columnName)
                        .forEach(function (x) {
                            x.style.display = 'inline'
                            x.style.float = 'left'
                            x.style.width = '33%'
                        });
                })


            }
        } else {
            document.querySelectorAll('.' + divName + '-' + 'contexts')
                .forEach(function (x) {
                    x.style.width = '45%'
                    //x.style.display = 'inline'
                    x.style.float = 'left'
                });

            ['notcol'].forEach(function (columnName) {
                document.querySelectorAll('#' + divName + '-' + columnName)
                    .forEach(function (x) {
                        //x.style.display = 'inline'
                        x.style.float = 'left'
                        x.style.width = '45%'
                    });
            })
        }

        var yAxis = null;
        var xAxis = null;

        function axisLabelerFactory(axis) {
            if ((axis == "x" && xLabelText == null)
                || (axis == "y" && yLabelText == null))
                return function (d, i) {
                    return ["Infrequent", "Average", "Frequent"][i];
                };

            return function (d, i) {
                return ["Low", "Medium", "High"][i];
            }
        }


        function bs(ar, x) {
            function bsa(s, e) {
                var mid = Math.floor((s + e) / 2);
                var midval = ar[mid];
                if (s == e) {
                    return s;
                }
                if (midval == x) {
                    return mid;
                } else if (midval < x) {
                    return bsa(mid + 1, e);
                } else {
                    return bsa(s, mid);
                }
            }

            return bsa(0, ar.length);
        }


        console.log("fullData");
        console.log(fullData);


        var sortedX = fullData.data.map(x => x).sort(function (a, b) {
            return a.x < b.x ? -1 : (a.x == b.x ? 0 : 1);
        }).map(function (x) {
            return x.x
        });

        var sortedOx = fullData.data.map(x => x).sort(function (a, b) {
            return a.ox < b.ox ? -1 : (a.ox == b.ox ? 0 : 1);
        }).map(function (x) {
            return x.ox
        });

        var sortedY = fullData.data.map(x => x).sort(function (a, b) {
            return a.y < b.y ? -1 : (a.y == b.y ? 0 : 1);
        }).map(function (x) {
            return x.y
        });

        var sortedOy = fullData.data.map(x => x).sort(function (a, b) {
            return a.oy < b.oy ? -1 : (a.oy == b.oy ? 0 : 1);
        }).map(function (x) {
            return x.oy
        });
        console.log(fullData.data[0])

        function labelWithZScore(axis, axisName, tickPoints, axis_values_format) {
            var myVals = axisName === 'x' ? sortedOx : sortedOy;
            var myPlotedVals = axisName === 'x' ? sortedX : sortedY;
            var ticks = tickPoints.map(function (x) {
                return myPlotedVals[bs(myVals, x)]
            });
            return axis.tickValues(ticks).tickFormat(
                function (d, i) {
                    return d3.format(axis_values_format)(tickPoints[i]);
                })
        }

        if (xAxisValues) {
            xAxis = labelWithZScore(d3.axisBottom(x), 'x', xAxisValues, x_axis_values_format);
        } else if (xAxisLabels) {
            xAxis = d3.axisBottom(x)
                .ticks(xAxisLabels.length)
                .tickFormat(function (d, i) {
                    return xAxisLabels[i];
                });
        } else {
            xAxis = d3.axisBottom(x).ticks(3).tickFormat(axisLabelerFactory('x'));
        }
        if (yAxisValues) {
            yAxis = labelWithZScore(d3.axisLeft(y), 'y', yAxisValues, y_axis_values_format);
        } else if (yAxisLabels) {
            yAxis = d3.axisLeft(y)
                .ticks(yAxisLabels.length)
                .tickFormat(function (d, i) {
                    return yAxisLabels[i];
                });
        } else {
            yAxis = d3.axisLeft(y).ticks(3).tickFormat(axisLabelerFactory('y'));
        }

        // var label = d3.select("body").append("div")
        var label = d3.select('#' + divName).append("div")
            .attr("class", "label");


        var interpolateLightGreys = d3.interpolate(d3.rgb(230, 230, 230),
            d3.rgb(130, 130, 130));
        // setup fill color
        if (color == null) {
            color = d3.interpolateRdYlBu;
        }
        if ((headerNames !== undefined && headerNames !== null)
            && (headerSortingAlgos !== undefined && headerSortingAlgos !== null)) {
            showTopTerms = true;
        }

        var pixelsToAddToWidth = 200;
        if (!showTopTerms && !showCharacteristic) {
            pixelsToAddToWidth = 0;
        }

        if (backgroundColor !== undefined) {
            document.body.style.backgroundColor = backgroundColor;
        }

        // Adds the svg canvas
        // var svg = d3.select("body")
        svg = d3.select('#' + divName)
            .append("svg")
            .attr("width", width + margin.left + margin.right + pixelsToAddToWidth)
            .attr("height", height + margin.top + margin.bottom)
            .append("g")
            .attr("transform",
                "translate(" + margin.left + "," + margin.top + ")");


        origSVGLeft = svg.node().getBoundingClientRect().left;
        origSVGTop = svg.node().getBoundingClientRect().top;
        var lastCircleSelected = null;

        function getCorpusWordCounts() {
            var binaryLabels = fullData.docs.labels.map(function (label) {
                return 1 * (fullData.docs.categories[label] != fullData.info.category_internal_name);
            });
            var wordCounts = {}; // word -> [cat counts, not-cat-counts]
            var wordCountSums = [0, 0];
            fullData.docs.texts.forEach(function (text, i) {
                text.toLowerCase().trim().split(/\W+/).forEach(function (word) {
                    if (word.trim() !== '') {
                        if (!(word in wordCounts))
                            wordCounts[word] = [0, 0];
                        wordCounts[word][binaryLabels[i]]++;
                        wordCountSums[binaryLabels[i]]++;
                    }
                })
            });
            return {
                avgDocLen: (wordCountSums[0] + wordCountSums[1]) / fullData.docs.texts.length,
                counts: wordCounts,
                sums: wordCountSums,
                uniques: [[0, 0]].concat(Object.keys(wordCounts).map(function (key) {
                    return wordCounts[key];
                })).reduce(function (a, b) {
                    return [a[0] + (b[0] > 0), a[1] + (b[1] > 0)]
                })
            };
        }

        function getContextWordCounts(query) {
            var wordCounts = {};
            var wordCountSums = [0, 0];
            var priorCountSums = [0, 0];
            gatherTermContexts(termDict[query])
                .contexts
                .forEach(function (contextSet, categoryIdx) {
                    contextSet.forEach(function (context) {
                        context.snippets.forEach(function (snippet) {
                            var tokens = snippet.toLowerCase().trim().replace('<b>', '').replace('</b>', '').split(/\W+/);
                            var matchIndices = [];
                            tokens.forEach(function (word, i) {
                                if (word === query) matchIndices.push(i)
                            });
                            tokens.forEach(function (word, i) {
                                if (word.trim() !== '') {
                                    var isValid = false;
                                    for (var matchI in matchIndices) {
                                        if (Math.abs(i - matchI) < 3) {
                                            isValid = true;
                                            break
                                        }
                                    }
                                    if (isValid) {
                                        //console.log([word, i, matchI, isValid]);
                                        if (!(word in wordCounts)) {
                                            var priorCounts = corpusWordCounts.counts[word]
                                            wordCounts[word] = [0, 0].concat(priorCounts);
                                            priorCountSums[0] += priorCounts[0];
                                            priorCountSums[1] += priorCounts[1];
                                        }
                                        wordCounts[word][categoryIdx]++;
                                        wordCountSums[categoryIdx]++;
                                    }
                                }
                            })
                        })
                    })
                });
            return {
                counts: wordCounts,
                priorSums: priorCountSums,
                sums: wordCountSums,
                uniques: [[0, 0]].concat(Object.keys(wordCounts).map(function (key) {
                    return wordCounts[key];
                })).reduce(function (a, b) {
                    return [a[0] + (b[0] > 0), a[1] + (b[1] > 0)];
                })
            }

        }

        function denseRank(ar) {
            var markedAr = ar.map((x, i) => [x, i]).sort((a, b) => a[0] - b[0]);
            var curRank = 1
            var rankedAr = markedAr.map(
                function (x, i) {
                    if (i > 0 && x[0] != markedAr[i - 1][0]) {
                        curRank++;
                    }
                    return [curRank, x[0], x[1]];
                }
            )
            return rankedAr.map(x => x).sort((a, b) => (a[2] - b[2])).map(x => x[0]);
        }


        function getDenseRanks(fullData, categoryNum) {
            console.log("GETTING DENSE RANKS")
            console.log("CAT NUM " + categoryNum)
            console.log(fullData)

            var fgFreqs = Array(fullData.data.length).fill(0);
            var bgFreqs = Array(fullData.data.length).fill(0);
            var categoryTermCounts = fullData.termCounts[categoryNum];

            Object.keys(categoryTermCounts).forEach(
                key => fgFreqs[key] = categoryTermCounts[key][0]
            )
            fullData.termCounts.forEach(
                function (categoryTermCounts, otherCategoryNum) {
                    if (otherCategoryNum != categoryNum) {
                        Object.keys(categoryTermCounts).forEach(
                            key => bgFreqs[key] += categoryTermCounts[key][0]
                        )
                    }
                }
            )
            var fgDenseRanks = denseRank(fgFreqs);
            var bgDenseRanks = denseRank(bgFreqs);

            var maxfgDenseRanks = Math.max(...fgDenseRanks);
            var minfgDenseRanks = Math.min(...fgDenseRanks);
            var scalefgDenseRanks = fgDenseRanks.map(
                x => (x - minfgDenseRanks) / (maxfgDenseRanks - minfgDenseRanks)
            )

            var maxbgDenseRanks = Math.max(...bgDenseRanks);
            var minbgDenseRanks = Math.min(...bgDenseRanks);
            var scalebgDenseRanks = bgDenseRanks.map(
                x => (x - minbgDenseRanks) / (maxbgDenseRanks - minbgDenseRanks)
            )

            return {'fg': scalefgDenseRanks,
                'bg': scalebgDenseRanks,
                'bgFreqs': bgFreqs,
                'fgFreqs': fgFreqs,
                'term': fullData.data.map((x)=>x.term)}
        }

        function getCategoryDenseRankScores(fullData, categoryNum) {
            var denseRanks = getDenseRanks(fullData, categoryNum)
            return denseRanks.fg.map((x, i) => x - denseRanks.bg[i]);
        }

        function getTermCounts(fullData) {
            var counts = Array(fullData.data.length).fill(0);
            fullData.termCounts.forEach(
                function (categoryTermCounts) {
                    Object.keys(categoryTermCounts).forEach(
                        key => counts[key] = categoryTermCounts[key][0]
                    )
                }
            )
            return counts;
        }

        function getContextWordLORIPs(query) {
            var contextWordCounts = getContextWordCounts(query);
            var ni_k = contextWordCounts.sums[0];
            var nj_k = contextWordCounts.sums[1];
            var n = ni_k + nj_k;
            //var ai_k0 = contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1];
            //var aj_k0 = contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1];
            var a0 = 0.00001 //corpusWordCounts.avgDocLen;
            var a_k0 = Object.keys(contextWordCounts.counts)
                .map(function (x) {
                    var counts = contextWordCounts.counts[x];
                    return a0 * (counts[2] + counts[3]) /
                        (contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1]);
                })
                .reduce(function (a, b) {
                    return a + b
                });
            var ai_k0 = a_k0 / ni_k;
            var aj_k0 = a_k0 / nj_k;
            var scores = Object.keys(contextWordCounts.counts).map(
                function (word) {
                    var countData = contextWordCounts.counts[word];
                    var yi = countData[0];
                    var yj = countData[1];
                    //var ai = countData[2];
                    //var aj = countData[3];
                    //var ai = countData[2] + countData[3];
                    //var aj = ai;
                    //var ai = (countData[2] + countData[3]) * a0/ni_k;
                    //var aj = (countData[2] + countData[3]) * a0/nj_k;
                    var ai = a0 * (countData[2] + countData[3]) /
                        (contextWordCounts.priorSums[0] + contextWordCounts.priorSums[1]);
                    var aj = ai;
                    var deltahat_i_j =
                        +Math.log((yi + ai) * 1. / (ni_k + ai_k0 - yi - ai))
                        - Math.log((yj + aj) * 1. / (nj_k + aj_k0 - yj - aj));
                    var var_deltahat_i_j = 1. / (yi + ai) + 1. / (ni_k + ai_k0 - yi - ai)
                        + 1. / (yj + aj) + 1. / (nj_k + aj_k0 - yj - aj);
                    var zeta_ij = deltahat_i_j / Math.sqrt(var_deltahat_i_j);
                    return [word, yi, yj, ai, aj, ai_k0, zeta_ij];
                }
            ).sort(function (a, b) {
                return b[5] - a[5];
            });
            return scores;
        }

        function getContextWordSFS(query) {
            // from https://stackoverflow.com/questions/14846767/std-normal-cdf-normal-cdf-or-error-function
            function cdf(x, mean, variance) {
                return 0.5 * (1 + erf((x - mean) / (Math.sqrt(2 * variance))));
            }

            function erf(x) {
                // save the sign of x
                var sign = (x >= 0) ? 1 : -1;
                x = Math.abs(x);

                // constants
                var a1 = 0.254829592;
                var a2 = -0.284496736;
                var a3 = 1.421413741;
                var a4 = -1.453152027;
                var a5 = 1.061405429;
                var p = 0.3275911;

                // A&S formula 7.1.26
                var t = 1.0 / (1.0 + p * x);
                var y = 1.0 - (((((a5 * t + a4) * t) + a3) * t + a2) * t + a1) * t * Math.exp(-x * x);
                return sign * y; // erf(-x) = -erf(x);
            }

            function scale(a) {
                return Math.log(a + 0.0000001);
            }

            var contextWordCounts = getContextWordCounts(query);
            var wordList = Object.keys(contextWordCounts.counts).map(function (word) {
                return contextWordCounts.counts[word].concat([word]);
            });
            var cat_freq_xbar = wordList.map(function (x) {
                return scale(x[0])
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var cat_freq_var = wordList.map(function (x) {
                return Math.pow((scale(x[0]) - cat_freq_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var cat_prec_xbar = wordList.map(function (x) {
                return scale(x[0] / (x[0] + x[1]));
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var cat_prec_var = wordList.map(function (x) {
                return Math.pow((scale(x[0] / (x[0] + x[1])) - cat_prec_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;

            var ncat_freq_xbar = wordList.map(function (x) {
                return scale(x[0])
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var ncat_freq_var = wordList.map(function (x) {
                return Math.pow((scale(x[0]) - ncat_freq_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var ncat_prec_xbar = wordList.map(function (x) {
                return scale(x[0] / (x[0] + x[1]));
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;
            var ncat_prec_var = wordList.map(function (x) {
                return Math.pow((scale(x[0] / (x[0] + x[1])) - ncat_prec_xbar), 2);
            }).reduce(function (a, b) {
                return a + b
            }) / wordList.length;

            function scaledFScore(cnt, other, freq_xbar, freq_var, prec_xbar, prec_var) {
                var beta = 1.5;
                var normFreq = cdf(scale(cnt), freq_xbar, freq_var);
                var normPrec = cdf(scale(cnt / (cnt + other)), prec_xbar, prec_var);
                return (1 + Math.pow(beta, 2)) * normFreq * normPrec / (Math.pow(beta, 2) * normFreq + normPrec);
            }

            var sfs = wordList.map(function (x) {
                cat_sfs = scaledFScore(x[0], x[1], cat_freq_xbar,
                    cat_freq_var, cat_prec_xbar, cat_prec_var);
                ncat_sfs = scaledFScore(x[1], x[0], ncat_freq_xbar,
                    ncat_freq_var, ncat_prec_xbar, ncat_prec_var);
                return [cat_sfs > ncat_sfs ? cat_sfs : -ncat_sfs].concat(x);

            }).sort(function (a, b) {
                return b[0] - a[0];
            });
            return sfs;
        }

        function deselectLastCircle() {
            if (lastCircleSelected) {
                lastCircleSelected.style["stroke"] = null;
                lastCircleSelected = null;
            }
        }

        function getSentenceBoundaries(text) {
            // !!! need to use spacy's sentence splitter
            if (asianMode) {
                var sentenceRe = /\n/gmi;
            } else {
                var sentenceRe = /\(?[^\.\?\!\n\b]+[\n\.!\?]\)?/g;
            }
            var offsets = [];
            var match;
            while ((match = sentenceRe.exec(text)) != null) {
                offsets.push(match.index);
            }
            offsets.push(text.length);
            return offsets;
        }

        function getMatchingSnippet(text, boundaries, start, end) {
            var sentenceStart = null;
            var sentenceEnd = null;
            for (var i in boundaries) {
                var position = boundaries[i];
                if (position <= start && (sentenceStart == null || position > sentenceStart)) {
                    sentenceStart = position;
                }
                if (position >= end) {
                    sentenceEnd = position;
                    break;
                }
            }
            var snippet = (text.slice(sentenceStart, start) + "<b>" + text.slice(start, end)
                + "</b>" + text.slice(end, sentenceEnd)).trim();
            if (sentenceStart == null) {
                sentenceStart = 0;
            }
            return {'snippet': snippet, 'sentenceStart': sentenceStart};
        }

        function gatherTermContexts(d, includeAll = true) {
            var category_name = fullData['info']['category_name'];
            var not_category_name = fullData['info']['not_category_name'];
            var matches = [[], [], [], []];
            console.log("searching")

            if (fullData.docs === undefined) return matches;
            if (!nonTextFeaturesMode) {
                return searchInText(d, includeAll);
            } else {
                return searchInExtraFeatures(d, includeAll);
            }
        }

        function searchInExtraFeatures(d) {
            var matches = [[], [], [], []];
            var term = d.term;
            var categoryNum = fullData.docs.categories.indexOf(fullData.info.category_internal_name);
            var notCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.not_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var neutralCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.neutral_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var extraCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.extra_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });

            var pattern = null;
            if ('metalists' in fullData && term in fullData.metalists) {
                // from https://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
                function escapeRegExp(str) {
                    return str.replace(/[\\?\-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|\']/g, "\\$&");
                }

                console.log('term');
                console.log(term);
                pattern = new RegExp(
                    '\\W(' + fullData.metalists[term].map(escapeRegExp).join('|') + ')\\W',
                    'gim'
                );
            }

            for (var i in fullData.docs.extra) {
                if (term in fullData.docs.extra[i]) {
                    var strength = fullData.docs.extra[i][term] /
                        Object.values(fullData.docs.extra[i]).reduce(
                            function (a, b) {
                                return a + b
                            });

                    var docLabel = fullData.docs.labels[i];
                    var numericLabel = -1;
                    if (docLabel == categoryNum) {
                        numericLabel = 0;
                    } else if (notCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 1;
                    } else if (neutralCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 2;
                    } else if (extraCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 3;
                    }
                    if (numericLabel == -1) {
                        continue;
                    }
                    var text = fullData.docs.texts[i];

                    if (fullData.offsets !== undefined) {

                        if (fullData.offsets[term] !== undefined && fullData.offsets[term][i] !== undefined) {
                            var curMatch = {
                                'id': i,
                                'snippets': [],
                                'strength': strength,
                                'docLabel': docLabel,
                                'meta': fullData.docs.meta ? fullData.docs.meta[i] : ""
                            }
                            for (const offset_i in fullData.offsets[term][i]) {
                                var offset = fullData.offsets[term][i][offset_i];
                                var spanStart = Math.max(offset[0] - 50, 0);
                                var spanEnd = Math.min(50, text.length-offset[1]);
                                var leftContext = text.substr(spanStart, offset[0] - spanStart);
                                var matchStr = text.substr(offset[0], offset[1] - offset[0]);
                                var rightContext = text.substr(offset[1], spanEnd);
                                var snippet = leftContext + '<b style="background-color: lightgoldenrodyellow">' + matchStr + '</b>' + rightContext;
                                if(spanStart > 0)
                                    snippet = '...' + snippet;
                                if(text.length - offset[1] > 50)
                                    snippet = snippet + '...'
                                curMatch.snippets.push(snippet)
                            }
                            matches[numericLabel].push(curMatch);
                        }
                    } else {

                        if (!useFullDoc)
                            text = text.slice(0, 300);
                        if (pattern !== null) {
                            text = text.replace(pattern, '<b>$&</b>');
                        }
                        var curMatch = {
                            'id': i,
                            'snippets': [text],
                            'strength': strength,
                            'docLabel': docLabel,
                            'meta': fullData.docs.meta ? fullData.docs.meta[i] : ""
                        }

                        matches[numericLabel].push(curMatch);
                    }
                }
            }
            for (var i in [0, 1]) {
                matches[i] = matches[i].sort(function (a, b) {
                    return a.strength < b.strength ? 1 : -1
                })
            }
            return {'contexts': matches, 'info': d};
        }

        // from https://mathiasbynens.be/notes/es-unicode-property-escapes#emoji
        var emojiRE = (/(?:[\u261D\u26F9\u270A-\u270D]|\uD83C[\uDF85\uDFC2-\uDFC4\uDFC7\uDFCA-\uDFCC]|\uD83D[\uDC42\uDC43\uDC46-\uDC50\uDC66-\uDC69\uDC6E\uDC70-\uDC78\uDC7C\uDC81-\uDC83\uDC85-\uDC87\uDCAA\uDD74\uDD75\uDD7A\uDD90\uDD95\uDD96\uDE45-\uDE47\uDE4B-\uDE4F\uDEA3\uDEB4-\uDEB6\uDEC0\uDECC]|\uD83E[\uDD18-\uDD1C\uDD1E\uDD1F\uDD26\uDD30-\uDD39\uDD3D\uDD3E\uDDD1-\uDDDD])(?:\uD83C[\uDFFB-\uDFFF])?|(?:[\u231A\u231B\u23E9-\u23EC\u23F0\u23F3\u25FD\u25FE\u2614\u2615\u2648-\u2653\u267F\u2693\u26A1\u26AA\u26AB\u26BD\u26BE\u26C4\u26C5\u26CE\u26D4\u26EA\u26F2\u26F3\u26F5\u26FA\u26FD\u2705\u270A\u270B\u2728\u274C\u274E\u2753-\u2755\u2757\u2795-\u2797\u27B0\u27BF\u2B1B\u2B1C\u2B50\u2B55]|\uD83C[\uDC04\uDCCF\uDD8E\uDD91-\uDD9A\uDDE6-\uDDFF\uDE01\uDE1A\uDE2F\uDE32-\uDE36\uDE38-\uDE3A\uDE50\uDE51\uDF00-\uDF20\uDF2D-\uDF35\uDF37-\uDF7C\uDF7E-\uDF93\uDFA0-\uDFCA\uDFCF-\uDFD3\uDFE0-\uDFF0\uDFF4\uDFF8-\uDFFF]|\uD83D[\uDC00-\uDC3E\uDC40\uDC42-\uDCFC\uDCFF-\uDD3D\uDD4B-\uDD4E\uDD50-\uDD67\uDD7A\uDD95\uDD96\uDDA4\uDDFB-\uDE4F\uDE80-\uDEC5\uDECC\uDED0-\uDED2\uDEEB\uDEEC\uDEF4-\uDEF8]|\uD83E[\uDD10-\uDD3A\uDD3C-\uDD3E\uDD40-\uDD45\uDD47-\uDD4C\uDD50-\uDD6B\uDD80-\uDD97\uDDC0\uDDD0-\uDDE6])|(?:[#\*0-9\xA9\xAE\u203C\u2049\u2122\u2139\u2194-\u2199\u21A9\u21AA\u231A\u231B\u2328\u23CF\u23E9-\u23F3\u23F8-\u23FA\u24C2\u25AA\u25AB\u25B6\u25C0\u25FB-\u25FE\u2600-\u2604\u260E\u2611\u2614\u2615\u2618\u261D\u2620\u2622\u2623\u2626\u262A\u262E\u262F\u2638-\u263A\u2640\u2642\u2648-\u2653\u2660\u2663\u2665\u2666\u2668\u267B\u267F\u2692-\u2697\u2699\u269B\u269C\u26A0\u26A1\u26AA\u26AB\u26B0\u26B1\u26BD\u26BE\u26C4\u26C5\u26C8\u26CE\u26CF\u26D1\u26D3\u26D4\u26E9\u26EA\u26F0-\u26F5\u26F7-\u26FA\u26FD\u2702\u2705\u2708-\u270D\u270F\u2712\u2714\u2716\u271D\u2721\u2728\u2733\u2734\u2744\u2747\u274C\u274E\u2753-\u2755\u2757\u2763\u2764\u2795-\u2797\u27A1\u27B0\u27BF\u2934\u2935\u2B05-\u2B07\u2B1B\u2B1C\u2B50\u2B55\u3030\u303D\u3297\u3299]|\uD83C[\uDC04\uDCCF\uDD70\uDD71\uDD7E\uDD7F\uDD8E\uDD91-\uDD9A\uDDE6-\uDDFF\uDE01\uDE02\uDE1A\uDE2F\uDE32-\uDE3A\uDE50\uDE51\uDF00-\uDF21\uDF24-\uDF93\uDF96\uDF97\uDF99-\uDF9B\uDF9E-\uDFF0\uDFF3-\uDFF5\uDFF7-\uDFFF]|\uD83D[\uDC00-\uDCFD\uDCFF-\uDD3D\uDD49-\uDD4E\uDD50-\uDD67\uDD6F\uDD70\uDD73-\uDD7A\uDD87\uDD8A-\uDD8D\uDD90\uDD95\uDD96\uDDA4\uDDA5\uDDA8\uDDB1\uDDB2\uDDBC\uDDC2-\uDDC4\uDDD1-\uDDD3\uDDDC-\uDDDE\uDDE1\uDDE3\uDDE8\uDDEF\uDDF3\uDDFA-\uDE4F\uDE80-\uDEC5\uDECB-\uDED2\uDEE0-\uDEE5\uDEE9\uDEEB\uDEEC\uDEF0\uDEF3-\uDEF8]|\uD83E[\uDD10-\uDD3A\uDD3C-\uDD3E\uDD40-\uDD45\uDD47-\uDD4C\uDD50-\uDD6B\uDD80-\uDD97\uDDC0\uDDD0-\uDDE6])\uFE0F/g);

        function isEmoji(str) {
            if (str.match(emojiRE)) return true;
            return false;
        }

        function displayObscuredTerms(obscuredTerms, data, term, termInfo, div = '#' + divName + '-' + 'overlapped-terms') {
            d3.select('#' + divName + '-' + 'overlapped-terms')
                .selectAll('div')
                .remove();
            d3.select(div)
                .selectAll('div')
                .remove();
            if (obscuredTerms.length > 1 && maxOverlapping !== 0) {
                var obscuredDiv = d3.select(div)
                    .append('div')
                    .attr("class", "obscured")
                    .style('align', 'center')
                    .style('text-align', 'center')
                    .html("<b>\"" + term + "\" obstructs</b>: ");
                obscuredTerms.map(
                    function (term, i) {
                        if (maxOverlapping === -1 || i < maxOverlapping) {
                            makeWordInteractive(
                                data,
                                svg,
                                obscuredDiv.append("text").text(term),
                                term,
                                data.filter(t => t.term === term)[0],//termInfo
                                false
                            );
                            if (i < obscuredTerms.length - 1
                                && (maxOverlapping === -1 || i < maxOverlapping - 1)) {
                                obscuredDiv.append("text").text(", ");
                            }
                        } else if (i === maxOverlapping && i !== obscuredTerms.length - 1) {
                            obscuredDiv.append("text").text("...");
                        }
                    }
                )
            }
        }

        function displayTermContexts(data, termInfo, jump = alwaysJump, includeAll = false) {
            var contexts = termInfo.contexts;
            var info = termInfo.info;
            var notmatches = termInfo.notmatches;
            if (contexts[0].length + contexts[1].length + contexts[2].length + contexts[3].length == 0) {
                //return null;
            }
            //!!! Future feature: context words
            //var contextWords = getContextWordSFS(info.term);
            //var contextWords = getContextWordLORIPs(info.term);
            //var categoryNames = [fullData.info.category_name,
            //    fullData.info.not_category_name];
            var catInternalName = fullData.info.category_internal_name;


            function addSnippets(contexts, divId, isMatch = true) {
                var meta = contexts.meta ? contexts.meta : '&nbsp;';
                var headClass = 'snippet_meta docLabel' + contexts.docLabel;
                var snippetClass = 'snippet docLabel' + contexts.docLabel;
                if (!isMatch) {
                    headClass = 'snippet_meta not_match docLabel' + contexts.docLabel;
                    snippetClass = 'snippet not_match docLabel' + contexts.docLabel;
                }
                d3.select(divId)
                    .append("div")
                    .attr('class', headClass)
                    .html(meta);
                contexts.snippets.forEach(function (snippet) {
                    d3.select(divId)
                        .append("div")
                        .attr('class', snippetClass)
                        .html(snippet);
                })
            }


            if (ignoreCategories) {
                divId = '#' + divName + '-' + 'cat';

                var numMatches = Object.create(null);
                var temp = d3.select(divId).selectAll("div").remove();
                var allContexts = contexts[0].concat(contexts[1]).concat(contexts[2]).concat(contexts[3]);
                var allNotMatches = [];
                if (notmatches !== undefined)
                    allNotMatches = notmatches[0].concat(notmatches[1]).concat(notmatches[2]).concat(notmatches[3]);
                d3.select('#' + divName + '-' + 'categoryinfo').selectAll("div").remove();
                var numDocs = fullData.docs.texts.length.toLocaleString('en');
                var numMatches = allContexts.length;
                d3.select(divId)
                    .append("div")
                    .attr('class', 'topic_preview')
                    .attr('text-align', "center")
                    .html(
                        "Matched " + numMatches + " out of " + numDocs + ' documents: '
                        + (100 * numMatches / numDocs).toFixed(2) + '%'
                    );

                if (allContexts.length > 0) {
                    var headerClassName = 'text_header';
                    allContexts.forEach(function (singleDoc) {
                        addSnippets(singleDoc, divId);
                    });
                    if (includeAll) {
                        allNotMatches.forEach(function (singleDoc) {
                            addSnippets(singleDoc, divId, false);
                        });
                    }
                }

            } else if (unifiedContexts) {
                divId = '#' + divName + '-' + 'cat';
                var docLabelCounts = fullData.docs.labels.reduce(
                    function (map, label) {
                        map[label] = (map[label] || 0) + 1;
                        return map;
                    },
                    Object.create(null)
                );
                var numMatches = Object.create(null);
                var temp = d3.select(divId).selectAll("div").remove();
                var allContexts = contexts[0].concat(contexts[1]).concat(contexts[2]).concat(contexts[3]);
                allContexts.forEach(function (singleDoc) {
                    numMatches[singleDoc.docLabel] = (numMatches[singleDoc.docLabel] || 0) + 1;
                });
                var allNotMatches = [];
                if (notmatches !== undefined)
                    allNotMatches = notmatches[0].concat(notmatches[1]).concat(notmatches[2]).concat(notmatches[3]);

                /*contexts.forEach(function(context) {
                     context.forEach(function (singleDoc) {
                         numMatches[singleDoc.docLabel] = (numMatches[singleDoc.docLabel]||0) + 1;
                         addSnippets(singleDoc, divId);
                     });
                 });*/
                console.log("ORDERING !!!!!");
                console.log(fullData.info.category_name);
                console.log(sortDocLabelsByName);
                var docLabelCountsSorted = Object.keys(docLabelCounts).map(key => (
                    {
                        "label": fullData.docs.categories[key],
                        "labelNum": key,
                        "matches": numMatches[key] || 0,
                        "overall": docLabelCounts[key],
                        'percent': (numMatches[key] || 0) * 100. / docLabelCounts[key]
                    }))
                    .sort(function (a, b) {
                        if (highlightSelectedCategory) {
                            if (a['label'] === fullData.info.category_name) {
                                return -1;
                            }
                            if (b['label'] === fullData.info.category_name) {
                                return 1;
                            }
                        }
                        if (sortDocLabelsByName) {
                            return a['label'] < b['label'] ? 1 : a['label'] > b['label'] ? -1 : 0;
                        } else {
                            return b.percent - a.percent;
                        }
                    });
                console.log("docLabelCountsSorted")
                console.log(docLabelCountsSorted);
                console.log(numMatches)
                console.log('#' + divName + '-' + 'categoryinfo')
                d3.select('#' + divName + '-' + 'categoryinfo').selectAll("div").remove();
                if (showCategoryHeadings) {
                    d3.select('#' + divName + '-' + 'categoryinfo').attr('display', 'inline');
                }

                function getCategoryStatsHTML(counts) {
                    return counts.matches + " document"
                        + (counts.matches == 1 ? "" : "s") + " out of " + counts.overall + ': '
                        + counts['percent'].toFixed(2) + '%';
                }

                function getCategoryInlineHeadingHTML(counts) {
                    return '<a name="' + divName + '-category'
                        + counts.labelNum + '"></a>'
                        + (ignoreCategories ? "" : counts.label + ": ") + "<span class=topic_preview>"
                        + getCategoryStatsHTML(counts)
                        + "</span>";
                }


                docLabelCountsSorted.forEach(function (counts) {

                    var htmlToAdd = "";
                    if (!ignoreCategories) {
                        htmlToAdd += "<b>" + counts.label + "</b>: " + getCategoryStatsHTML(counts);
                        ;
                    }

                    if (counts.matches > 0) {
                        var headerClassName = 'text_header';
                        if ((counts.label === fullData.info.category_name) && highlightSelectedCategory) {
                            d3.select(divId)
                                .append('div')
                                .attr('class', 'separator')
                                .html("<b>Selected category</b>");
                        }
                        d3.select(divId)
                            .append("div")
                            .attr('class', headerClassName)
                            .html(getCategoryInlineHeadingHTML(counts));

                        allContexts
                            .filter(singleDoc => singleDoc.docLabel == counts.labelNum)
                            .forEach(function (singleDoc) {
                                addSnippets(singleDoc, divId);
                            });
                        if (includeAll) {
                            allNotMatches
                                .filter(singleDoc => singleDoc.docLabel == counts.labelNum)
                                .forEach(function (singleDoc) {
                                    addSnippets(singleDoc, divId, false);
                                });
                        }
                        if ((counts.label === fullData.info.category_name) && highlightSelectedCategory) {
                            d3.select(divId).append('div').attr('class', 'separator').html("<b>End selected category</b>");
                            d3.select(divId).append('div').html("<br />");
                        }
                    }


                    if (showCategoryHeadings) {
                        d3.select('#' + divName + '-' + 'categoryinfo')
                            .attr('display', 'inline')
                            .append('div')
                            .html(htmlToAdd)
                            .on("click", function () {
                                window.location.hash = '#' + divName + '-' + 'category' + counts.labelNum
                            });
                    }

                })


            } else {
                var contextColumns = [
                    fullData.info.category_internal_name,
                    fullData.info.not_category_name
                ];
                if (showNeutral) {
                    if ('neutral_category_name' in fullData.info) {
                        contextColumns.push(fullData.info.neutral_category_name)
                    } else {
                        contextColumns.push("Neutral")
                    }
                    if (showExtra) {
                        if ('extra_category_name' in fullData.info) {
                            contextColumns.push(fullData.info.extra_category_name)
                        } else {
                            contextColumns.push("Extra")
                        }
                    }

                }
                contextColumns.map(
                    function (catName, catIndex) {
                        if (max_snippets != null) {
                            var contextsToDisplay = contexts[catIndex].slice(0, max_snippets);
                        }
                        //var divId = catName == catInternalName ? '#cat' : '#notcat';
                        var divId = null
                        if (fullData.info.category_internal_name == catName) {
                            divId = '#' + divName + '-' + 'cat'
                        } else if (fullData.info.not_category_name == catName) {
                            divId = '#' + divName + '-' + 'notcat'
                        } else if (fullData.info.neutral_category_name == catName) {
                            divId = '#' + divName + '-' + 'neut';
                        } else if (fullData.info.extra_category_name == catName) {
                            divId = '#' + divName + '-' + 'extra'
                        } else {
                            return;
                        }

                        var temp = d3.select(divId).selectAll("div").remove();
                        contexts[catIndex].forEach(function (context) {
                            addSnippets(context, divId);
                        });
                        if (includeAll) {
                            notmatches[catIndex].forEach(function (context) {
                                addSnippets(context, divId, false);
                            });
                        }
                    }
                );
            }

            var obscuredTerms = getObscuredTerms(data, termInfo.info);
            displayObscuredTerms(obscuredTerms, data, info.term, info, '#' + divName + '-' + 'overlapped-terms-clicked');

            d3.select('#' + divName + '-' + 'termstats')
                .selectAll("div")
                .remove();
            var termHtml = 'Term: <b>' + formatTermForDisplay(info.term) + '</b>';
            if ('metalists' in fullData && info.term in fullData.metalists) {
                termHtml = 'Topic: <b>' + formatTermForDisplay(info.term) + '</b>';
            }
            if (getCustomTermHtml !== null) {
                termHtml = getCustomTermHtml(info);
            }
            d3.select('#' + divName + '-' + 'termstats')
                .append('div')
                .attr("class", "snippet_header")
                .html(termHtml);
            if ('metalists' in fullData && info.term in fullData.metalists && topic_model_preview_size > 0) {
                d3.select('#' + divName + '-' + 'termstats')
                    .attr("class", "topic_preview")
                    .append('div')
                    .html("<b>Topic preview</b>: "
                        + fullData.metalists[info.term]
                            .slice(0, topic_model_preview_size)
                            .reduce(function (x, y) {
                                return x + ', ' + y
                            }));
            }
            if ('metadescriptions' in fullData && info.term in fullData.metadescriptions) {
                d3.select('#' + divName + '-' + 'termstats')
                    .attr("class", "topic_preview")
                    .append('div')
                    .html("<b>Description</b>: " + fullData.metadescriptions[info.term]);
            }
            var message = '';
            var cat_name = fullData.info.category_name;
            var ncat_name = fullData.info.not_category_name;


            var numCatDocs = fullData.docs.labels
                .map(function (x) {
                    return (x == fullData.docs.categories.indexOf(
                        fullData.info.category_internal_name)) + 0
                })
                .reduce(function (a, b) {
                    return a + b;
                }, 0);

            var notCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.not_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });


            var numNCatDocs = fullData.docs.labels
                .map(function (x) {
                    return notCategoryNumList.indexOf(x) > -1
                })
                .reduce(function (a, b) {
                    return a + b;
                }, 0);

            function getFrequencyDescription(name, count25k, count, ndocs) {
                var desc = name;
                if (!enableTermCategoryDescription) {
                    return desc + ':';
                }
                desc += ' frequency: <div class=text_subhead>' + count25k + ' per 25,000 terms</div>';
                if (!isNaN(Math.round(ndocs))) {
                    desc += '<div class=text_subhead>' + Math.round(ndocs) + ' per 1,000 docs</div>';
                }
                if (count == 0) {
                    desc += '<u>Not found in any ' + name + ' documents.</u>';
                } else {
                    if (!isNaN(Math.round(ndocs))) {
                        desc += '<u>Some of the ' + count + ' mentions:</u>';
                    } else {
                        desc += count + ' mentions';
                    }
                }
                /*
                desc += '<br><b>Discriminative:</b> ';

                desc += contextWords
                    .slice(cat_name === name ? 0 : contextWords.length - 3,
                        cat_name === name ? 3 : contextWords.length)
                    .filter(function (x) {
                        //return Math.abs(x[5]) > 1.96;
                        return true;
                    })
                    .map(function (x) {return x.join(', ')}).join('<br>');
                */
                return desc;
            }

            if (!unifiedContexts && !ignoreCategories) {
                console.log("NOT UNIFIED CONTEXTS")
                d3.select('#' + divName + '-' + 'cathead')
                    .style('fill', color(1))
                    .html(
                        getFrequencyDescription(cat_name,
                            info.cat25k,
                            info.cat,
                            termInfo.contexts[0].length * 1000 / numCatDocs
                        )
                    );
                d3.select('#' + divName + '-' + 'notcathead')
                    .style('fill', color(0))
                    .html(
                        getFrequencyDescription(ncat_name,
                            info.ncat25k,
                            info.ncat,
                            termInfo.contexts[1].length * 1000 / numNCatDocs)
                    );
                if (showNeutral) {
                    var numList = fullData.docs.categories.map(function (x, i) {
                        if (fullData.info.neutral_category_internal_names.indexOf(x) > -1) {
                            return i;
                        } else {
                            return -1;
                        }
                    }).filter(function (x) {
                        return x > -1
                    });

                    var numDocs = fullData.docs.labels
                        .map(function (x) {
                            return numList.indexOf(x) > -1
                        })
                        .reduce(function (a, b) {
                            return a + b;
                        }, 0);

                    d3.select("#" + divName + "-neuthead")
                        .style('fill', color(0))
                        .html(
                            getFrequencyDescription(fullData.info.neutral_category_name,
                                info.neut25k,
                                info.neut,
                                termInfo.contexts[2].length * 1000 / numDocs)
                        );

                    if (showExtra) {
                        var numList = fullData.docs.categories.map(function (x, i) {
                            if (fullData.info.extra_category_internal_names.indexOf(x) > -1) {
                                return i;
                            } else {
                                return -1;
                            }
                        }).filter(function (x) {
                            return x > -1
                        });

                        var numDocs = fullData.docs.labels
                            .map(function (x) {
                                return numList.indexOf(x) > -1
                            })
                            .reduce(function (a, b) {
                                return a + b;
                            }, 0);

                        d3.select("#" + divName + "-extrahead")
                            .style('fill', color(0))
                            .html(
                                getFrequencyDescription(fullData.info.extra_category_name,
                                    info.extra25k,
                                    info.extra,
                                    termInfo.contexts[3].length * 1000 / numDocs)
                            );

                    }
                }
            } else if (unifiedContexts && !ignoreCategories) {
                // extra unified context code goes here
                console.log("docLabelCountsSorted")
                console.log(docLabelCountsSorted)

                docLabelCountsSorted.forEach(function (counts) {
                    var htmlToAdd = (ignoreCategories ? "" : "<b>" + counts.label + "</b>: ") + getCategoryStatsHTML(counts);
                    if (showCategoryHeadings) {
                        d3.select('#' + divName + '-' + 'contexts')
                            .append('div')
                            .html(htmlToAdd)
                            .on("click", function () {
                                window.location.hash = '#' + divName + '-' + 'category' + counts.labelNum
                            });
                    }
                })
            }
            if (jump) {
                if (window.location.hash === '#' + divName + '-' + 'snippets') {
                    window.location.hash = '#' + divName + '-' + 'snippetsalt';
                } else {
                    window.location.hash = '#' + divName + '-' + 'snippets';
                }
            }
        }

        function searchInText(d, includeAll = true) {
            function stripNonWordChars(term) {
                //d.term.replace(" ", "[^\\w]+")
            }

            function removeUnderScoreJoin(term) {
                /*
                '_ _asjdklf_jaksdlf_jaksdfl skld_Jjskld asdfjkl_sjkdlf'
                  ->
                "_ _asjdklf jaksdlf jaksdfl skld Jjskld asdfjkl_sjkdlf"
                 */
                return term.replace(/(\w+)(_)(\w+)/, "$1 $3")
                    .replace(/(\w+)(_)(\w+)/, "$1 $3")
                    .replace(/(\w+)(_)(\w+)/, "$1 $3");
            }

            function buildMatcher(term) {


                var boundary = '(?:\\W|^|$)';
                var wordSep = "[^\\w]+";
                if (asianMode) {
                    boundary = '( |$|^)';
                    wordSep = ' ';
                }
                if (isEmoji(term)) {
                    boundary = '';
                    wordSep = '';
                }
                if (matchFullLine) {
                    boundary = '($|^)';
                }
                var termToRegex = term;


                // https://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
                function escapeRegExp(string) {
                    return string.replace(/[\-\[\]\/\{\}\(\)\*\+\?\.\,\\\^\$\|\'#?]/g, "\\$&");
                    //return string.replace(/[\?#.*+^${}()|[\]\\]'\%/g, '\\$&'); // $& means the whole matched string
                }

                /*
                ['[', ']', '(', ')', '{', '}', '^', '$', '|', '?', '"',
                    '*', '+', '-', '=', '~', '`', '{'].forEach(function (a) {
                    termToRegex = termToRegex.replace(a, '\\\\' + a)
                });
                ['.', '#'].forEach(function(a) {termToRegex = termToRegex.replace(a, '\\' + a)})
                */
                termToRegex = escapeRegExp(termToRegex);
                console.log("termToRegex")
                console.log(termToRegex)

                var regexp = new RegExp(boundary + '('
                    + removeUnderScoreJoin(
                        termToRegex.replace(' ', wordSep, 'gim')
                    ) + ')' + boundary, 'gim');
                console.log(regexp);

                if (subwordEncoding === 'RoBERTa') {
                    if (term.charCodeAt(0) === 288 || term.charCodeAt(0) === 289) {
                        // Starts with character Ġ indicating it's a word start
                        console.log("START")
                        regexp = new RegExp(boundary + escapeRegExp(term.substr(1, term.length)), 'gim');
                    } else {
                        regexp = new RegExp("\w" + escapeRegExp(term), 'gim');
                    }
                    console.log("SP")
                    console.log(regexp)
                }


                try {
                    regexp.exec('X');
                } catch (err) {
                    console.log("Can't search " + term);
                    console.log(err);
                    return null;
                }
                return regexp;
            }

            var matches = [[], [], [], []];
            var notmatches = [[], [], [], []];
            var pattern = buildMatcher(d.term);
            var categoryNum = fullData.docs.categories.indexOf(fullData.info.category_internal_name);
            var notCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.not_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var neutralCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.neutral_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            var extraCategoryNumList = fullData.docs.categories.map(function (x, i) {
                if (fullData.info.extra_category_internal_names.indexOf(x) > -1) {
                    return i;
                } else {
                    return -1;
                }
            }).filter(function (x) {
                return x > -1
            });
            console.log('extraCategoryNumList')
            console.log(extraCategoryNumList);
            console.log("categoryNum");
            console.log(categoryNum);
            console.log("categoryNum");
            if (pattern !== null) {
                for (var i in fullData.docs.texts) {
                    //var numericLabel = 1 * (fullData.docs.categories[fullData.docs.labels[i]] != fullData.info.category_internal_name);

                    var docLabel = fullData.docs.labels[i];
                    var numericLabel = -1;
                    if (docLabel == categoryNum) {
                        numericLabel = 0;
                    } else if (notCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 1;
                    } else if (neutralCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 2;
                    } else if (extraCategoryNumList.indexOf(docLabel) > -1) {
                        numericLabel = 3;
                    }
                    if (numericLabel == -1) {
                        continue;
                    }

                    var text = removeUnderScoreJoin(fullData.docs.texts[i]);
                    //var pattern = new RegExp("\\b(" + stripNonWordChars(d.term) + ")\\b", "gim");
                    var match;
                    var sentenceOffsets = null;
                    var lastSentenceStart = null;
                    var matchFound = false;
                    var curMatch = {'id': i, 'snippets': [], 'notsnippets': [], 'docLabel': docLabel};
                    if (fullData.docs.meta) {
                        curMatch['meta'] = fullData.docs.meta[i];
                    }

                    while ((match = pattern.exec(text)) != null) {
                        if (sentenceOffsets == null) {
                            sentenceOffsets = getSentenceBoundaries(text);
                        }
                        var foundSnippet = getMatchingSnippet(text, sentenceOffsets,
                            match.index, pattern.lastIndex);
                        if (foundSnippet.sentenceStart == lastSentenceStart) continue; // ensure we don't duplicate sentences
                        lastSentenceStart = foundSnippet.sentenceStart;
                        curMatch.snippets.push(foundSnippet.snippet);
                        matchFound = true;
                    }
                    if (matchFound) {
                        if (useFullDoc) {
                            curMatch.snippets = [
                                text
                                    .replace(/\n$/g, '\n\n')
                                    .replace(
                                        //new RegExp("\\b(" + d.term.replace(" ", "[^\\w]+") + ")\\b",
                                        //    'gim'),
                                        pattern,
                                        '<b>$&</b>')
                            ];
                        }
                        matches[numericLabel].push(curMatch);
                    } else {
                        if (includeAll) {
                            curMatch.snippets = [
                                text.replace(/\n$/g, '\n\n')
                            ];
                            notmatches[numericLabel].push(curMatch);
                        }

                    }
                }
            }
            var toRet = {
                'contexts': matches,
                'notmatches': notmatches,
                'info': d,
                'docLabel': docLabel
            };
            return toRet;
        }

        function getDefaultTooltipContent(d) {
            var term = formatTermForDisplay(d.term);

            var message = term + "<br/>" + d.cat25k + ":" + d.ncat25k + " per 25k words";
            message += '<br/>score: ' + d.os.toFixed(5);
            return message;
        }

        function getDefaultTooltipContentWithoutScore(d) {
            var term = formatTermForDisplay(d.term);

            var message = term + "<br/>" + d.cat25k + ":" + d.ncat25k + " per 25k words";
            return message;
        }

        function getObscuredTerms(data, d) {
            //data = fullData['data']
            var matches = (data.filter(function (term) {
                    return term.x === d.x && term.y === d.y && (term.display === undefined || term.display === true);
                }).map(function (term) {
                    return formatTermForDisplay(term.term)
                }).sort()
            );
            return matches;
        }

        function showTooltip(data, d, pageX, pageY, showObscured = true) {
            deselectLastCircle();

            var obscuredTerms = getObscuredTerms(data, d);
            var message = '';
            console.log("!!!!! " + obscuredTerms.length)
            console.log(showObscured)
            if (obscuredTerms.length > 1 && showObscured)
                displayObscuredTerms(obscuredTerms, data, d.term, d);
            if (getTooltipContent !== null) {
                message += getTooltipContent(d);
            } else {
                if (sortByDist) {
                    message += getDefaultTooltipContentWithoutScore(d);
                } else {
                    message += getDefaultTooltipContent(d);
                }
            }
            pageX -= (svg.node().getBoundingClientRect().left) - origSVGLeft;
            pageY -= (svg.node().getBoundingClientRect().top) - origSVGTop;
            tooltip.transition()
                .duration(0)
                .style("opacity", 1)
                .style("z-index", 10000000);
            tooltip.html(message)
                .style("left", (pageX - 40) + "px")
                .style("top", (pageY - 85 > 0 ? pageY - 85 : 0) + "px");
            tooltip.on('click', function () {
                tooltip.transition()
                    .style('opacity', 0)
            }).on('mouseout', function () {
                tooltip.transition().style('opacity', 0)
            });
        }

        handleSearch = function (event) {
            var searchTerm = document
                .getElementById(this.divName + "-searchTerm")
                .value;
            handleSearchTerm(searchTerm);
            return false;
        };

        function highlightTerm(searchTerm, showObscured) {
            deselectLastCircle();
            var cleanedTerm = searchTerm.toLowerCase()
                .replace("'", " '")
                .trim();
            if (this.termDict[cleanedTerm] === undefined) {
                cleanedTerm = searchTerm.replace("'", " '").trim();
            }
            if (this.termDict[cleanedTerm] !== undefined) {
                showToolTipForTerm(this.data, this.svg, cleanedTerm, this.termDict[cleanedTerm], showObscured);
            }
            return cleanedTerm;
        }

        function handleSearchTerm(searchTerm, jump = false) {
            console.log("Handle search term.");
            console.log(searchTerm);
            console.log("this");
            console.log(this)
            highlighted = highlightTerm.call(this, searchTerm, true);
            console.log("found searchTerm");
            console.log(searchTerm);
            if (this.termDict[searchTerm] != null) {
                var runDisplayTermContexts = true;
                if (alternativeTermFunc != null) {
                    runDisplayTermContexts = this.alternativeTermFunc(this.termDict[searchTerm]);
                }
                if (runDisplayTermContexts) {
                    displayTermContexts(
                        this.data,
                        this.gatherTermContexts(this.termDict[searchTerm], this.includeAllContexts),
                        alwaysJump,
                        this.includeAllContexts
                    );
                }
            }
        }

        function getCircleForSearchTerm(mysvg, searchTermInfo) {
            var circle = mysvg;
            if (circle.tagName !== "circle") { // need to clean this thing up
                circle = mysvg._groups[0][searchTermInfo.ci];
                if (circle === undefined || circle.tagName != 'circle') {
                    if (mysvg._groups[0].children !== undefined) {
                        circle = mysvg._groups[0].children[searchTermInfo.ci];
                    }
                }
                if (circle === undefined || circle.tagName != 'circle') {
                    if (mysvg._groups[0][0].children !== undefined) {
                        circle = Array.prototype.filter.call(
                            mysvg._groups[0][0].children,
                            x => (x.tagName == "circle" && x.__data__['term'] == searchTermInfo.term)
                        )[0];
                    }
                }
                if ((circle === undefined || circle.tagName != 'circle') && mysvg._groups[0][0].children !== undefined) {
                    circle = mysvg._groups[0][0].children[searchTermInfo.ci];
                }
            }
            return circle;
        }

        function showToolTipForTerm(data, mysvg, searchTerm, searchTermInfo, showObscured = true) {
            //var searchTermInfo = termDict[searchTerm];
            console.log("showing tool tip")
            console.log(searchTerm)
            console.log(searchTermInfo)
            if (searchTermInfo === undefined) {
                console.log("can't show")
                d3.select("#" + divName + "-alertMessage")
                    .text(searchTerm + " didn't make it into the visualization.");
            } else {
                d3.select("#" + divName + "-alertMessage").text("");
                var circle = getCircleForSearchTerm(mysvg, searchTermInfo);
                if (circle) {
                    var mySVGMatrix = circle.getScreenCTM().translate(circle.cx.baseVal.value, circle.cy.baseVal.value);
                    var pageX = mySVGMatrix.e;
                    var pageY = mySVGMatrix.f;
                    circle.style["stroke"] = "black";
                    //var circlePos = circle.position();
                    //var el = circle.node()
                    //showTooltip(searchTermInfo, pageX, pageY, circle.cx.baseVal.value, circle.cx.baseVal.value);
                    showTooltip(
                        data,
                        searchTermInfo,
                        pageX,
                        pageY,
                        showObscured
                    );

                    lastCircleSelected = circle;
                }

            }
        };


        function makeWordInteractive(data, svg, domObj, term, termInfo, showObscured = true) {
            return domObj
                .on("mouseover", function (d) {
                    showToolTipForTerm(data, svg, term, termInfo, showObscured);
                    d3.select(this).style("stroke", "black");
                })
                .on("mouseout", function (d) {
                    tooltip.transition()
                        .duration(0)
                        .style("opacity", 0);
                    d3.select(this).style("stroke", null);
                    if (showObscured) {
                        d3.select('#' + divName + '-' + 'overlapped-terms')
                            .selectAll('div')
                            .remove();
                    }
                })
                .on("click", function (d) {
                    var runDisplayTermContexts = true;
                    if (alternativeTermFunc != null) {
                        runDisplayTermContexts = alternativeTermFunc(termInfo);
                    }
                    if (runDisplayTermContexts) {
                        displayTermContexts(data, gatherTermContexts(termInfo, includeAllContexts), alwaysJump, includeAllContexts);
                    }
                });
        }



        function processData(fullData) {

            modelInfo = fullData['info'];
            /*
             categoryTermList.data(modelInfo['category_terms'])
             .enter()
             .append("li")
             .text(function(d) {return d;});
             */
            var data = fullData['data'];
            termDict = Object();
            data.forEach(function (x, i) {
                termDict[x.term] = x;
                //!!!
                //termDict[x.term].i = i;
            });

            var padding = 0.1;
            if (showAxes || showAxesAndCrossHairs) {
                padding = 0.1;
            }

            // Scale the range of the data.  Add some space on either end.
            if (useGlobalScale) {
                var axisMax = Math.max(
                    d3.max(data, function (d) {
                        return d.x;
                    }),
                    d3.max(data, function (d) {
                        return d.y;
                    }),
                )
                var axisMin = Math.min(
                    d3.min(data, function (d) {
                        return d.x;
                    }),
                    d3.min(data, function (d) {
                        return d.y;
                    }),
                )
                axisMin = axisMin - (axisMax - axisMin) * padding;
                axisMax = axisMax + (axisMax - axisMin) * padding;
                x.domain([axisMin, axisMax]);
                y.domain([axisMin, axisMax]);
            } else {
                var xMax = d3.max(data, function (d) {
                    return d.x;
                });
                var yMax = d3.max(data, function (d) {
                    return d.y;
                })
                x.domain([-1 * padding, xMax + padding]);
                y.domain([-1 * padding, yMax + padding]);
            }

            /*
             data.sort(function (a, b) {
             return Math.abs(b.os) - Math.abs(a.os)
             });
             */


            //var rangeTree = null; // keep boxes of all points and labels here
            var rectHolder = new RectangleHolder();
            var axisRectHolder = new RectangleHolder();
            // Add the scatterplot
            data.forEach(function (d, i) {
                d.ci = i
            });

            //console.log('XXXXX'); console.log(data)


            function getFilter(data) {
                return data.filter(d => d.display === undefined || d.display === true);
            }


            var mysvg = svg
                .selectAll("dot")
                .data(getFilter(data))
                //.filter(function (d) {return d.display === undefined || d.display === true})
                .enter()
                .append("circle")
                .attr("r", function (d) {
                    if (pValueColors && d.p) {
                        return (d.p >= 1 - minPVal || d.p <= minPVal) ? 2 : 1.75;
                    }
                    return 2;
                })
                .attr("cx", function (d) {
                    return x(d.x);
                })
                .attr("cy", function (d) {
                    return y(d.y);
                })
                .style("fill", function (d) {
                    //.attr("fill", function (d) {
                    if (colorFunc) {
                        return colorFunc(d);
                    } else if (greyZeroScores && d.os == 0) {
                        return d3.rgb(230, 230, 230);
                    } else if (pValueColors && d.p) {
                        if (d.p >= 1 - minPVal) {
                            return wordVecMaxPValue ? d3.interpolateYlGnBu(d.s) : color(d.s);
                        } else if (d.p <= minPVal) {
                            return wordVecMaxPValue ? d3.interpolateYlGnBu(d.s) : color(d.s);
                        } else {
                            return interpolateLightGreys(d.s);
                        }
                    } else {
                        if (d.term === "the") {
                            console.log("COLS " + d.s + " " + color(d.s) + " " + d.term)
                            console.log(d)
                            console.log(color)
                        }
                        return color(d.s);
                    }
                })
                .on("mouseover", function (d) {
                    /*var mySVGMatrix = circle.getScreenCTM()n
                        .translate(circle.cx.baseVal.value, circle.cy.baseVal.value);
                    var pageX = mySVGMatrix.e;
                    var pageY = mySVGMatrix.f;*/

                    /*showTooltip(
                        d,
                        d3.event.pageX,
                        d3.event.pageY
                    );*/
                    console.log("point MOUSOEVER")
                    console.log(d)
                    showToolTipForTerm(data, this, d.term, d, true);
                    d3.select(this).style("stroke", "black");
                })
                .on("click", function (d) {
                    var runDisplayTermContexts = true;
                    if (alternativeTermFunc != null) {
                        runDisplayTermContexts = alternativeTermFunc(d);
                    }
                    if (runDisplayTermContexts) {
                        displayTermContexts(data, gatherTermContexts(d), alwaysJump, includeAllContexts);
                    }
                })
                .on("mouseout", function (d) {
                    tooltip.transition()
                        .duration(0)
                        .style("opacity", 0);
                    d3.select(this).style("stroke", null);
                    d3.select('#' + divName + '-' + 'overlapped-terms')
                        .selectAll('div')
                        .remove();
                })


            coords = Object();

            var pointStore = [];
            var pointRects = [];

            function censorPoints(datum, getX, getY) {
                var term = datum.term;
                var curLabel = svg.append("text")
                    .attr("x", x(getX(datum)))
                    .attr("y", y(getY(datum)) + 3)
                    .attr("text-anchor", "middle")
                    .text("x");
                var bbox = curLabel.node().getBBox();
                var borderToRemove = .5;
                var x1 = bbox.x + borderToRemove,
                    y1 = bbox.y + borderToRemove,
                    x2 = bbox.x + bbox.width - borderToRemove,
                    y2 = bbox.y + bbox.height - borderToRemove;
                //rangeTree = insertRangeTree(rangeTree, x1, y1, x2, y2, '~~' + term);
                var pointRect = new Rectangle(x1, y1, x2, y2);
                pointRects.push(pointRect);
                rectHolder.add(pointRect);
                pointStore.push([x1, y1]);
                pointStore.push([x2, y1]);
                pointStore.push([x1, y2]);
                pointStore.push([x2, y2]);
                curLabel.remove();
            }

            function censorCircle(xCoord, yCoord) {
                var curLabel = svg.append("text")
                    .attr("x", x(xCoord))
                    .attr("y", y(yCoord) + 3)
                    .attr("text-anchor", "middle")
                    .text("x");
                var bbox = curLabel.node().getBBox();
                var borderToRemove = .5;
                var x1 = bbox.x + borderToRemove,
                    y1 = bbox.y + borderToRemove,
                    x2 = bbox.x + bbox.width - borderToRemove,
                    y2 = bbox.y + bbox.height - borderToRemove;
                var pointRect = new Rectangle(x1, y1, x2, y2);
                pointRects.push(pointRect);
                rectHolder.add(pointRect);
                pointStore.push([x1, y1]);
                pointStore.push([x2, y1]);
                pointStore.push([x1, y2]);
                pointStore.push([x2, y2]);
                curLabel.remove();
            }

            var configs = [
                {'anchor': 'end', 'group': 1, 'xoff': -5, 'yoff': -3, 'alignment-baseline': 'ideographic'},
                {'anchor': 'end', 'group': 1, 'xoff': -5, 'yoff': 10, 'alignment-baseline': 'ideographic'},

                {'anchor': 'end', 'group': 2, 'xoff': 10, 'yoff': 15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'end', 'group': 2, 'xoff': -10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'end', 'group': 2, 'xoff': 10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'end', 'group': 2, 'xoff': -10, 'yoff': 15, 'alignment-baseline': 'ideographic'},

                {'anchor': 'start', 'group': 1, 'xoff': 3, 'yoff': 10, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 1, 'xoff': 3, 'yoff': -3, 'alignment-baseline': 'ideographic'},

                {'anchor': 'start', 'group': 2, 'xoff': 5, 'yoff': 10, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 2, 'xoff': 5, 'yoff': -3, 'alignment-baseline': 'ideographic'},

                {'anchor': 'start', 'group': 3, 'xoff': 10, 'yoff': 15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 3, 'xoff': -10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 3, 'xoff': 10, 'yoff': -15, 'alignment-baseline': 'ideographic'},
                {'anchor': 'start', 'group': 3, 'xoff': -10, 'yoff': 15, 'alignment-baseline': 'ideographic'},
            ];
            if (centerLabelsOverPoints) {
                configs = [{'anchor': 'middle', 'xoff': 0, 'yoff': 0, 'alignment-baseline': 'middle'}];
            }

            function labelPointsIfPossible(datum, myX, myY) {
                if (suppressTextColumn !== undefined
                    && datum.etc !== undefined
                    && datum.etc[suppressTextColumn] === true) {
                    return false;
                }

                var term = datum.term;
                if (datum.x > datum.y) {
                    configs.sort((a, b) => a.anchor == 'end' && b.anchor == 'end'
                        ? a.group - b.group : (a.anchor == 'end') - (b.anchor == 'end'));
                } else {
                    configs.sort((a, b) => a.anchor == 'start' && b.anchor == 'start'
                        ? a.group - b.group : (a.anchor == 'start') - (b.anchor == 'start'));
                }
                var matchedElement = null;

                var termColor = 'rgb(0,0,0)';
                if (textColorColumn !== undefined && datum.etc !== undefined && datum.etc[textColorColumn] !== undefined) {
                    termColor = datum.etc[textColorColumn];
                }
                term = formatTermForDisplay(term);

                for (var configI in configs) {
                    var config = configs[configI];
                    var curLabel = svg.append("text")
                        //.attr("x", x(data[i].x) + config['xoff'])
                        //.attr("y", y(data[i].y) + config['yoff'])
                        .attr("x", x(myX) + config['xoff'])
                        .attr("y", y(myY) + config['yoff'])
                        .attr('class', 'label')
                        .attr('class', 'pointlabel')
                        .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                        .attr('font-size', '10px')
                        .attr("text-anchor", config['anchor'])
                        .attr("alignment-baseline", config['alignment'])
                        .attr("fill", termColor)
                        .text(term);
                    var bbox = curLabel.node().getBBox();
                    var borderToRemove = doCensorPoints ? 0.5 : .25;

                    var x1 = bbox.x + borderToRemove,
                        y1 = bbox.y + borderToRemove,
                        x2 = bbox.x + bbox.width - borderToRemove,
                        y2 = bbox.y + bbox.height - borderToRemove;
                    //matchedElement = searchRangeTree(rangeTree, x1, y1, x2, y2);
                    var matchedElement = false;
                    rectHolder.findMatchingRectangles(x1, y1, x2, y2, function (elem) {
                        matchedElement = true;
                        return false;
                    });
                    if (matchedElement) {
                        curLabel.remove();
                    } else {
                        curLabel = makeWordInteractive(data, svg, curLabel, term, datum);
                        break;
                    }
                }

                if (!matchedElement) {
                    coords[term] = [x1, y1, x2, y2];
                    //rangeTree = insertRangeTree(rangeTree, x1, y1, x2, y2, term);
                    var labelRect = new Rectangle(x1, y1, x2, y2)
                    rectHolder.add(labelRect);
                    pointStore.push([x1, y1]);
                    pointStore.push([x2, y1]);
                    pointStore.push([x1, y2]);
                    pointStore.push([x2, y2]);
                    return {label: curLabel, rect: labelRect};
                } else {
                    //curLabel.remove();
                    return false;
                }

            }

            var radius = 2;

            function euclideanDistanceSort(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                return (Math.min(aCatDist, aNotCatDist) > Math.min(bCatDist, bNotCatDist)) * 2 - 1;
            }

            function euclideanDistanceSortForCategory(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                return (aCatDist > bCatDist) * 2 - 1;
            }

            function euclideanDistanceSortForNotCategory(a, b) {
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                return (aNotCatDist > bNotCatDist) * 2 - 1;
            }

            function scoreSort(a, b) {
                return a.s - b.s;
            }

            function scoreSortReverse(a, b) {
                return b.s - a.s;
            }

            function backgroundScoreSort(a, b) {
                if (b.bg === a.bg)
                    return (b.cat + b.ncat) - (a.cat + a.ncat);
                return b.bg - a.bg;
            }

            function arePointsPredictiveOfDifferentCategories(a, b) {
                var aCatDist = a.x * a.x + (1 - a.y) * (1 - a.y);
                var bCatDist = b.x * b.x + (1 - b.y) * (1 - b.y);
                var aNotCatDist = a.y * a.y + (1 - a.x) * (1 - a.x);
                var bNotCatDist = b.y * b.y + (1 - b.x) * (1 - b.x);
                var aGood = aCatDist < aNotCatDist;
                var bGood = bCatDist < bNotCatDist;
                return {aGood: aGood, bGood: bGood};
            }

            function scoreSortForCategory(a, b) {
                var __ret = arePointsPredictiveOfDifferentCategories(a, b);
                if (sortByDist) {
                    var aGood = __ret.aGood;
                    var bGood = __ret.bGood;
                    if (aGood && !bGood) return -1;
                    if (!aGood && bGood) return 1;
                }
                return b.s - a.s;
            }

            function scoreSortForNotCategory(a, b) {
                var __ret = arePointsPredictiveOfDifferentCategories(a, b);
                if (sortByDist) {
                    var aGood = __ret.aGood;
                    var bGood = __ret.bGood;
                    if (aGood && !bGood) return 1;
                    if (!aGood && bGood) return -1;
                }
                if (reverseSortScoresForNotCategory)
                    return a.s - b.s;
                else
                    return b.s - a.s;
            }

            var sortedData = data.map(x => x).sort(sortByDist ? euclideanDistanceSort : scoreSort);
            if (doCensorPoints) {
                for (var i in data) {
                    var d = sortedData[i];

                    if (!(censorPointColumn !== undefined
                        && d.etc !== undefined
                        && d.etc[censorPointColumn] === false)) {

                        censorPoints(
                            d,
                            function (d) {
                                return d.x
                            },
                            function (d) {
                                return d.y
                            }
                        );
                    }

                }
            }


            function registerFigureBBox(curLabel, axis = false) {
                var bbox = curLabel.node().getBBox();
                var borderToRemove = 1.5;
                var x1 = bbox.x + borderToRemove,
                    y1 = bbox.y + borderToRemove,
                    x2 = bbox.x + bbox.width - borderToRemove,
                    y2 = bbox.y + bbox.height - borderToRemove;
                var rect = new Rectangle(x1, y1, x2, y2)
                if (axis) {
                    axisRectHolder.add(rect)
                } else {
                    rectHolder.add(rect);
                }
                //return insertRangeTree(rangeTree, x1, y1, x2, y2, '~~_other_');
            }

            function drawXLabel(svg, labelText) {
                return svg.append("text")
                    .attr("class", "x label")
                    .attr("text-anchor", "end")
                    .attr("x", width)
                    .attr("y", height - 6)
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '10px')
                    .text(labelText);
            }

            function drawYLabel(svg, labelText) {
                return svg.append("text")
                    .attr("class", "y label")
                    .attr("text-anchor", "end")
                    .attr("y", 6)
                    .attr("dy", ".75em")
                    .attr("transform", "rotate(-90)")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '10px')
                    .text(labelText);
            }

            d3.selection.prototype.moveToBack = function () {
                return this.each(function () {
                    var firstChild = this.parentNode.firstChild;
                    if (firstChild) {
                        this.parentNode.insertBefore(this, firstChild);
                    }
                });
            };

            if (verticalLines) {
                if (typeof (verticalLines) === "number") {
                    verticalLines = [verticalLines]; // r likes to make single element vectors doubles; this is a hackish workaround
                }
                for (i in verticalLines) {
                    svg.append("g")
                        .attr("transform", "translate(" + x(verticalLines[i]) + ", 1)")
                        .append("line")
                        .attr("y2", height)
                        .style("stroke", "#dddddd")
                        .style("stroke-width", "1px")
                        .moveToBack();
                }
            }

            if (fullData['line'] !== undefined) {
                var valueline = d3.line()
                    .x(function (d) {
                        return x(d.x);
                    })
                    .y(function (d) {
                        return y(d.y);
                    });
                fullData.line = fullData.line.sort((a, b) => b.x - a.x);
                svg.append("path")
                    .attr("class", "line")
                    .style("stroke-width", "1px")
                    .attr("d", valueline(fullData['line'])).moveToBack();
            }
            if (showAxes || showAxesAndCrossHairs) {

                var myXAxis = svg.append("g")
                    .attr("class", "x axis")
                    .attr("transform", "translate(0," + height + ")")
                    .call(xAxis);

                //rangeTree = registerFigureBBox(myXAxis);


                var xLabel = drawXLabel(svg, getLabelText('x'));

                //console.log('xLabel');
                //console.log(xLabel);

                //rangeTree = registerFigureBBox(xLabel);
                // Add the Y Axis

                if (!yAxisValues) {
                    var myYAxis = svg.append("g")
                        .attr("class", "y axis")
                        .call(yAxis)
                        .selectAll("text")
                        .style("text-anchor", "end")
                        .attr("dx", "30px")
                        .attr("dy", "-13px")
                        .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                        .attr('font-size', '10px')
                        .attr("transform", "rotate(-90)");
                } else {
                    var myYAxis = svg.append("g")
                        .attr("class", "y axis")
                        .call(yAxis)
                        .selectAll("text")
                        .style("text-anchor", "end")
                        .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                        .attr('font-size', '10px');
                }
                registerFigureBBox(myYAxis, true);
                registerFigureBBox(myXAxis, true);

                function getLabelText(axis) {
                    if (axis == 'y') {
                        if (yLabelText == null)
                            return modelInfo['category_name'] + " Frequency";
                        else
                            return yLabelText;
                    } else {
                        if (xLabelText == null)
                            return modelInfo['not_category_name'] + " Frequency";
                        else
                            return xLabelText;
                    }
                }

                var yLabel = drawYLabel(svg, getLabelText('y'))

            }

            if (!showAxes || showAxesAndCrossHairs) {
                horizontal_line_y_position_translated = 0.5;
                if (horizontal_line_y_position !== null) {
                    var loOy = null, hiOy = null, loY = null, hiY = null;
                    for (i in fullData.data) {
                        var curOy = fullData.data[i].oy;
                        if (curOy < horizontal_line_y_position && (curOy > loOy || loOy === null)) {
                            loOy = curOy;
                            loY = fullData.data[i].y
                        }
                        if (curOy > horizontal_line_y_position && (curOy < hiOy || hiOy === null)) {
                            hiOy = curOy;
                            hiY = fullData.data[i].y
                        }
                    }
                    horizontal_line_y_position_translated = loY + (hiY - loY) / 2.
                    if (loY === null) {
                        horizontal_line_y_position_translated = 0;
                    }
                }
                if (vertical_line_x_position === null) {
                    vertical_line_x_position_translated = 0.5;
                } else {
                    if (vertical_line_x_position !== null) {
                        var loOx = null, hiOx = null, loX = null, hiX = null;
                        for (i in fullData.data) {
                            var curOx = fullData.data[i].ox;
                            if (curOx < vertical_line_x_position && (curOx > loOx || loOx === null)) {
                                loOx = curOx;
                                loX = fullData.data[i].x;
                            }
                            if (curOx > vertical_line_x_position && (curOx < hiOx || hiOx === null)) {
                                hiOx = curOx;
                                hiX = fullData.data[i].x
                            }
                        }
                        vertical_line_x_position_translated = loX + (hiX - loX) / 2.
                        if (loX === null) {
                            vertical_line_x_position_translated = 0;
                        }
                    }
                }
                if (showCrossAxes) {
                    var x_line = svg.append("g")
                        .attr("transform", "translate(0, " + y(horizontal_line_y_position_translated) + ")")
                        .append("line")
                        .attr("x2", width)
                        .style("stroke", "#cccccc")
                        .style("stroke-width", "1px")
                        .moveToBack();
                    var y_line = svg.append("g")
                        .attr("transform", "translate(" + x(vertical_line_x_position_translated) + ", 0)")
                        .append("line")
                        .attr("y2", height)
                        .style("stroke", "#cccccc")
                        .style("stroke-width", "1px")
                        .moveToBack();
                }
            }

            if (showDiagonal) {
                var diagonal = svg.append("g")
                    .append("line")
                    .attr("x1", 0)
                    .attr("y1", height)
                    .attr("x2", width)
                    .attr("y2", 0)
                    .style("stroke-dasharray", "5,5")
                    .style("stroke", "#cccccc")
                    .style("stroke-width", "1px")
                    .moveToBack();
            }

            function showWordList(word, termDataList, xOffset=null) {
                var maxWidth = word.node().getBBox().width;
                var wordObjList = [];
                for (var i in termDataList) {
                    var datum = termDataList[i];
                    var curTerm = datum.term;
                    word = (function (word, curTerm) {
                        var termColor = 'rgb(0,0,0)';
                        if (textColorColumn !== undefined && datum.etc !== undefined && datum.etc[textColorColumn] !== undefined) {
                            termColor = datum.etc[textColorColumn];
                        }
                        console.log("Show WORD "); console.log(word.node().getBBox().x)
                        var curWordPrinted = svg.append("text")
                            .attr("text-anchor", "start")
                            .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                            .attr('font-size', '12px')
                            .attr("fill", termColor)
                            .attr("x", xOffset == null ? word.node().getBBox().x : xOffset)
                            .attr("y", word.node().getBBox().y
                                + 2 * word.node().getBBox().height)
                            .text(formatTermForDisplay(curTerm));
                        wordObjList.push(curWordPrinted)
                        return makeWordInteractive(
                            termDataList, //data,
                            svg,
                            curWordPrinted,
                            curTerm,
                            termDataList[i]);
                    })(word, curTerm);
                    if (word.node().getBBox().width > maxWidth)
                        maxWidth = word.node().getBBox().width;
                    registerFigureBBox(word);
                }
                return {
                    'word': word,
                    'maxWidth': maxWidth,
                    'wordObjList': wordObjList
                };
            }

            function pickEuclideanDistanceSortAlgo(category) {
                if (category == true) return euclideanDistanceSortForCategory;
                return euclideanDistanceSortForNotCategory;
            }

            function pickScoreSortAlgo(isTopPane) {
                console.log("PICK SCORE ALGO")
                console.log(isTopPane)
                if (isTopPane === true) {
                    if (headerSortingAlgos !== null && headerSortingAlgos['upper'] !== undefined)
                        return headerSortingAlgos['upper'];
                    return scoreSortForCategory;
                } else {
                    if (headerSortingAlgos !== null && headerSortingAlgos['lower'] !== undefined)
                        return headerSortingAlgos['lower'];
                    return scoreSortForNotCategory;
                }

            }

            function pickTermSortingAlgorithm(isUpperPane) {
                if (sortByDist) return pickEuclideanDistanceSortAlgo(isUpperPane);
                return pickScoreSortAlgo(isUpperPane);
            }

            function showAssociatedWordList(data, word, header, isUpperPane, xOffset, length = topTermsLength) {
                var sortedData = null;
                var sortingAlgo = pickTermSortingAlgorithm(isUpperPane);
                console.log("showAssociatedWordList");
                console.log(header);
                console.log("WORD");
                console.log(word)
                sortedData = data.filter(term => (term.display === undefined || term.display === true)).sort(sortingAlgo);
                if (wordVecMaxPValue) {
                    function signifTest(x) {
                        if (isUpperPane)
                            return x.p >= 1 - minPVal;
                        return x.p <= minPVal;
                    }

                    sortedData = sortedData.filter(signifTest)
                }
                return showWordList(word, sortedData.slice(0, length), xOffset);

            }

            var characteristicXOffset = width;

            function showCatHeader(startingOffset, catName, registerFigureBBox) {
                var catHeader = svg.append("text")
                    .attr("text-anchor", "start")
                    .attr("x", startingOffset //width
                    )
                    .attr("dy", "6px")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '12px')
                    .attr('font-weight', 'bolder')
                    .attr('font-decoration', 'underline')
                    .text(catName
                        //"Top " + fullData['info']['category_name']
                    );
                registerFigureBBox(catHeader);
                return catHeader;
            }

            function showNotCatHeader(startingOffset, word, notCatName) {
                console.log("showNotCatHeader")
                return svg.append("text")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '12px')
                    .attr('font-weight', 'bolder')
                    .attr('font-decoration', 'underline')
                    .attr("text-anchor", "start")
                    .attr("x", startingOffset)
                    .attr("y", word.node().getBBox().y + 3 * word.node().getBBox().height)
                    .text(notCatName);
            }

            function showTopTermsPane(data,
                                      registerFigureBBox,
                                      showAssociatedWordList,
                                      upperHeaderName,
                                      lowerHeaderName,
                                      startingOffset) {
                data = data.filter(term => (term.display === undefined || term.display === true));
                //var catHeader = showCatHeader(startingOffset, catName, registerFigureBBox);
                var catHeader = svg.append("text")
                    .attr("text-anchor", "start")
                    .attr("x", startingOffset)
                    .attr("dy", "6px")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr('font-size', '12px')
                    .attr('font-weight', 'bolder')
                    .attr('font-decoration', 'underline')
                    .text(upperHeaderName
                        //"Top " + fullData['info']['category_name']
                    );
                registerFigureBBox(catHeader);
                var word = catHeader;
                var wordListData = showAssociatedWordList(data, word, catHeader, true, startingOffset);
                word = wordListData.word;
                var maxWidth = wordListData.maxWidth;

                var notCatHeader = showNotCatHeader(startingOffset, word, lowerHeaderName);
                word = notCatHeader;
                characteristicXOffset = Math.max(
                    catHeader.node().getBBox().x + maxWidth + 10,
                    notCatHeader.node().getBBox().x + maxWidth + 10
                )
                console.log("characteristicXOffset", characteristicXOffset)
                console.log(catHeader.node().getBBox().x + maxWidth + 10)
                console.log(notCatHeader.node().getBBox().x + maxWidth + 10)

                var notWordListData = showAssociatedWordList(data, word, notCatHeader, false, startingOffset);
                word = wordListData.word;
                if (wordListData.maxWidth > maxWidth) {
                    maxWidth = wordListData.maxWidth;
                }
                return {
                    wordListData, notWordListData,
                    word, maxWidth, characteristicXOffset, startingOffset,
                    catHeader, notCatHeader, registerFigureBBox
                };
            }

            var payload = Object();
            if (showTopTerms) {
                var upperHeaderName = "Top " + fullData['info']['category_name'];
                var lowerHeaderName = "Top " + fullData['info']['not_category_name'];
                if (headerNames !== null) {
                    if (headerNames.upper !== undefined)
                        upperHeaderName = headerNames.upper;
                    if (headerNames.lower !== undefined)
                        lowerHeaderName = headerNames.lower;
                }
                payload.topTermsPane = showTopTermsPane(
                    data,
                    registerFigureBBox,
                    showAssociatedWordList,
                    upperHeaderName,
                    lowerHeaderName,
                    width + topTermsLeftBuffer
                );
                payload.showTopTermsPane = showTopTermsPane;
                payload.showAssociatedWordList = showAssociatedWordList;
                payload.showWordList = showWordList;

                /*var wordListData = topTermsPane.wordListData;
                var word = topTermsPane.word;
                var maxWidth = topTermsPane.maxWidth;
                var catHeader = topTermsPane.catHeader;
                var notCatHeader = topTermsPane.notCatHeader;
                var startingOffset = topTermsPane.startingOffset;*/
                characteristicXOffset = payload.topTermsPane.characteristicXOffset;
            }


            if ((!nonTextFeaturesMode && !asianMode && showCharacteristic)
                || (headerNames !== null && headerNames.right !== undefined)) {
                var sortMethod = backgroundScoreSort;
                var title = 'Characteristic';
                if (headerNames !== null && headerNames.right !== undefined) {
                    title = headerNames.right;
                }
                if (wordVecMaxPValue) {
                    title = 'Most similar';
                    sortMethod = scoreSortReverse;
                } else if (data.reduce(function (a, b) {
                    return a + b.bg
                }, 0) === 0) {
                    title = 'Most frequent';
                }
                word = svg.append("text")
                    .attr('font-family', 'Helvetica, Arial, Sans-Serif')
                    .attr("text-anchor", "start")
                    .attr('font-size', '12px')
                    .attr('font-weight', 'bolder')
                    .attr('font-decoration', 'underline')
                    .attr("x", characteristicXOffset)
                    .attr("dy", "6px")
                    .text(title);

                var rightSortMethod = sortMethod;
                if (rightOrderColumn !== undefined && rightOrderColumn !== null) {
                    rightSortMethod = ((a, b) => b.etc[rightOrderColumn] - a.etc[rightOrderColumn]);
                }

                var wordListData = showWordList(
                    word,
                    data.filter(term => (term.display === undefined || term.display === true))
                        .sort(rightSortMethod).slice(0, topTermsLength * 2 + 2),
                    characteristicXOffset
                );

                word = wordListData.word;
                maxWidth = wordListData.maxWidth;
                console.log(maxWidth);
                console.log(word.node().getBBox().x + maxWidth);

                svg.attr('width', word.node().getBBox().x + 3 * maxWidth + 10);
            }

            function performPartialLabeling(
                data,
                existingLabels,
                getX,
                getY,
                labelPriorityFunction = ((a, b) => Math.min(a.x, 1 - a.x, a.y, 1 - a.y) - Math.min(b.x, 1 - b.x, b.y, 1 - b.y))
            ) {
                for (i in existingLabels) {
                    rectHolder.remove(existingLabels[i].rect);
                    existingLabels[i].label.remove();
                }

                var labeledPoints = [];

                //var filteredData = data.filter(d=>d.display === undefined || d.display === true);
                //for (var i = 0; i < filteredData.length; i++) {
                data.sort(labelPriorityFunction).forEach(function (datum, i) {
                    //console.log(datum.i, datum.ci, i)
                    //var label = labelPointsIfPossible(i, getX(filteredData[i]), getY(filteredData[i]));
                    if (datum.display === undefined || datum.display === true) {
                        var label = labelPointsIfPossible(datum, getX(datum), getY(datum));
                        if (label !== false) {
                            //console.log("labeled")
                            labeledPoints.push(label)
                        }
                    }
                    //if (labelPointsIfPossible(i), true) numPointsLabeled++;
                })
                return labeledPoints;
            }

            //var labeledPoints = performPartialLabeling();
            var labeledPoints = [];
            var labelPriorityFunction = ((a, b) => Math.min(a.x, 1 - a.x, a.y, 1 - a.y) - Math.min(b.x, 1 - b.x, b.y, 1 - b.y))
            if (labelPriorityColumn !== undefined && labelPriorityColumn !== null) {
                labelPriorityFunction = (a, b) => b.etc[labelPriorityColumn] - a.etc[labelPriorityColumn];
            }

            labeledPoints = performPartialLabeling(
                data,
                labeledPoints,
                function (d) {
                    return d.x
                },
                function (d) {
                    return d.y
                },
                labelPriorityFunction
            );

            if (backgroundLabels !== null) {
                backgroundLabels.map(
                    function (label) {
                        svg.append("text")
                            .attr("x", x(label.X))
                            .attr("y", y(label.Y))
                            .attr("text-anchor", "middle")
                            .style("font-size", "30")
                            .style("fill", "rgb(200,200,200)")
                            .text(label.Text)
                            .lower()
                            .on('mouseover', function (d) {
                                d3.select(this).style('stroke', 'black').style('stroke-width', '1px').raise()
                            })
                            .on('mouseout', function (d) {
                                d3.select(this).style('stroke-width', '0px').style('fill', 'rgb(200,200,200)').lower()
                            })
                    }
                )
            }


            /*
            // pointset has to be sorted by X
            function convex(pointset) {
                function _cross(o, a, b) {
                    return (a[0] - o[0]) * (b[1] - o[1]) - (a[1] - o[1]) * (b[0] - o[0]);
                }

                function _upperTangent(pointset) {
                    var lower = [];
                    for (var l = 0; l < pointset.length; l++) {
                        while (lower.length >= 2 && (_cross(lower[lower.length - 2], lower[lower.length - 1], pointset[l]) <= 0)) {
                            lower.pop();
                        }
                        lower.push(pointset[l]);
                    }
                    lower.pop();
                    return lower;
                }

                function _lowerTangent(pointset) {
                    var reversed = pointset.reverse(),
                        upper = [];
                    for (var u = 0; u < reversed.length; u++) {
                        while (upper.length >= 2 && (_cross(upper[upper.length - 2], upper[upper.length - 1], reversed[u]) <= 0)) {
                            upper.pop();
                        }
                        upper.push(reversed[u]);
                    }
                    upper.pop();
                    return upper;
                }

                var convex,
                    upper = _upperTangent(pointset),
                    lower = _lowerTangent(pointset);
                convex = lower.concat(upper);
                convex.push(pointset[0]);
                return convex;
            }

            console.log("POINTSTORE")
            console.log(pointStore);
            pointStore.sort();
            var convexHull = convex(pointStore);
            var minX = convexHull.sort(function (a,b) {
                return a[0] < b[0] ? -1 : 1;
            })[0][0];
            var minY = convexHull.sort(function (a,b) {
                return a[1] < b[1] ? -1 : 1;
            })[0][0];
            //svg.append("text").text("BLAH BLAH").attr("text-anchor", "middle").attr("cx", x(0)).attr("y", minY);
            console.log("POINTSTORE")
            console.log(pointStore);
            console.log(convexHull);
            for (i in convexHull) {
                var i = parseInt(i);
                if (i + 1 == convexHull.length) {
                    var nextI = 0;
                } else {
                    var nextI = i + 1;
                }
                console.log(i, ',', nextI);
                svg.append("line")
                    .attr("x2", width)
                    .style("stroke", "#cc0000")
                    .style("stroke-width", "1px")
                    .attr("x1", convexHull[i][0])     // x position of the first end of the line
                    .attr("y1", convexHull[i][1])      // y position of the first end of the line
                    .attr("x2", convexHull[nextI][0])     // x position of the second end of the line
                    .attr("y2", convexHull[nextI][1]);    // y position of the second end of the line
            }*/

            function populateCorpusStats() {
                var wordCounts = {};
                var docCounts = {}
                fullData.docs.labels.forEach(function (x, i) {
                    var cnt = (
                        fullData.docs.texts[i]
                            .trim()
                            .replace(/['";:,.?¿\-!¡]+/g, '')
                            .match(/\S+/g) || []
                    ).length;
                    var name = null;
                    if (unifiedContexts) {
                        var name = fullData.docs.categories[x];
                        wordCounts[name] = wordCounts[name] ? wordCounts[name] + cnt : cnt;
                    } else {
                        if (fullData.docs.categories[x] == fullData.info.category_internal_name) {
                            name = fullData.info.category_name;
                        } else if (fullData.info.not_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.not_category_name;
                        } else if (fullData.info.neutral_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.neutral_category_name;
                        } else if (fullData.info.extra_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.extra_category_name;
                        }
                        if (name) {
                            wordCounts[name] = wordCounts[name] ? wordCounts[name] + cnt : cnt
                        }
                    }
                    //!!!

                });
                fullData.docs.labels.forEach(function (x) {

                    if (unifiedContexts) {
                        var name = fullData.docs.categories[x];
                        docCounts[name] = docCounts[name] ? docCounts[name] + 1 : 1
                    } else {
                        var name = null;
                        if (fullData.docs.categories[x] == fullData.info.category_internal_name) {
                            name = fullData.info.category_name;
                        } else if (fullData.info.not_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.not_category_name;
                        } else if (fullData.info.neutral_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.neutral_category_name;
                        } else if (fullData.info.extra_category_internal_names.indexOf(fullData.docs.categories[x]) > -1) {
                            name = fullData.info.extra_category_name;
                        }
                        if (name) {
                            docCounts[name] = docCounts[name] ? docCounts[name] + 1 : 1
                        }
                    }
                });
                console.log("docCounts");
                console.log(docCounts)
                var messages = [];
                if (ignoreCategories) {
                    var wordCount = getCorpusWordCounts();
                    console.log("wordCount")
                    console.log(wordCount)
                    messages.push(
                        '<b>Document count: </b>' + fullData.docs.texts.length.toLocaleString('en') +
                        '; <b>word count: </b>'
                        + wordCount['sums'].reduce((a, b) => a + b, 0).toLocaleString('en')
                    )
                } else if (unifiedContexts) {
                    fullData.docs.categories.forEach(function (x, i) {
                        if (docCounts[x] > 0) {
                            var message = '<b>' + x + '</b>: ';
                            message += 'document count: '
                                + Number(docCounts[x]).toLocaleString('en')
                                + '; word count: '
                                + Number(wordCounts[x]).toLocaleString('en')
                            messages.push(message);
                        }
                    });
                } else {
                    [fullData.info.category_name,
                        fullData.info.not_category_name,
                        fullData.info.neutral_category_name,
                        fullData.info.extra_category_name].forEach(function (x, i) {
                        if (docCounts[x] > 0) {
                            messages.push('<b>' + x + '</b> document count: '
                                + Number(docCounts[x]).toLocaleString('en')
                                + '; word count: '
                                + Number(wordCounts[x]).toLocaleString('en'));
                        }
                    });
                }

                if (showCorpusStats) {
                    d3.select('#' + divName + '-' + 'corpus-stats')
                        .style('width', width + margin.left + margin.right + 200)
                        .append('div')
                        .html(messages.join('<br />'));
                }
            }


            if (fullData.docs) {
                populateCorpusStats();
            }

            if (saveSvgButton) {
                // from https://stackoverflow.com/questions/23218174/how-do-i-save-export-an-svg-file-after-creating-an-svg-with-d3-js-ie-safari-an
                var svgElement = document.getElementById(divName);

                var serializer = new XMLSerializer();
                var source = serializer.serializeToString(svgElement);

                if (!source.match(/^<svg[^>]+xmlns="http\:\/\/www\.w3\.org\/2000\/svg"/)) {
                    source = source.replace(/^<svg/, '<svg xmlns="https://www.w3.org/2000/svg"');
                }
                if (!source.match(/^<svg[^>]+"http\:\/\/www\.w3\.org\/1999\/xlink"/)) {
                    source = source.replace(/^<svg/, '<svg xmlns:xlink="https://www.w3.org/1999/xlink"');
                }

                source = '<?xml version="1.0" standalone="no"?>\r\n' + source;

                var url = "data:image/svg+xml;charset=utf-8," + encodeURIComponent(source);

                var downloadLink = document.createElement("a");
                downloadLink.href = url;
                downloadLink.download = fullData['info']['category_name'] + ".svg";
                downloadLink.innerText = 'Download SVG';
                document.body.appendChild(downloadLink);

            }

            function rerender(xCoords, yCoords, color) {
                labeledPoints.forEach(function (p) {
                    p.label.remove();
                    rectHolder.remove(p.rect);
                });
                pointRects.forEach(function (rect) {
                    rectHolder.remove(rect);
                });
                pointRects = []
                /*
                var circles = d3.select('#' + divName).selectAll('circle')
                    .attr("cy", function (d) {return y(yCoords[d.i])})
                    .transition(0)
                    .attr("cx", function (d) {return x(xCoords[d.i])})
                    .transition(0);
                */
                d3.select('#' + divName).selectAll("dot").remove();
                d3.select('#' + divName).selectAll("circle").remove();
                console.log(this.fullData)
                console.log(this)
                console.log("X/Y coords")
                console.log(this.fullData.data.filter(d => d.display === undefined || d.display === true).map(d => [d.x, d.y]))
                var circles = this.svg//.select('#' + divName)
                    .selectAll("dot")
                    .data(this.fullData.data.filter(d => d.display === undefined || d.display === true))
                    //.filter(function (d) {return d.display === undefined || d.display === true})
                    .enter()
                    .append("circle")
                    .attr("cy", d => d.y)
                    .attr("cx", d => d.x)
                    .attr("r", d => 2)
                    .on("mouseover", function (d) {
                        /*var mySVGMatrix = circle.getScreenCTM()n
                            .translate(circle.cx.baseVal.value, circle.cy.baseVal.value);
                        var pageX = mySVGMatrix.e;
                        var pageY = mySVGMatrix.f;*/

                        /*showTooltip(
                            d,
                            d3.event.pageX,
                            d3.event.pageY
                        );*/
                        console.log("point MOUSOEVER")
                        console.log(d)
                        showToolTipForTerm(data, this, d.term, d, true);
                        d3.select(this).style("stroke", "black");
                    })
                    .on("click", function (d) {
                        var runDisplayTermContexts = true;
                        if (alternativeTermFunc != null) {
                            runDisplayTermContexts = alternativeTermFunc(d);
                        }
                        if (runDisplayTermContexts) {
                            displayTermContexts(data, gatherTermContexts(d), alwaysJump, includeAllContexts);
                        }
                    })
                    .on("mouseout", function (d) {
                        tooltip.transition()
                            .duration(0)
                            .style("opacity", 0);
                        d3.select(this).style("stroke", null);
                        d3.select('#' + divName + '-' + 'overlapped-terms')
                            .selectAll('div')
                            .remove();
                    });

                if (color !== null) {
                    console.log("COLOR")
                    console.log(color)
                    circles.style("fill", d => color(d));
                }
                xCoords.forEach((xCoord, i) => censorCircle(xCoord, yCoords[i]))
                labeledPoints = [];
                labeledPoints = performPartialLabeling(
                    this.fullData.data,
                    labeledPoints,
                    (d => d.ox), //function (d) {return xCoords[d.ci]},
                    (d => d.oy) //function (d) {return yCoords[d.ci]}

                );
            }

            //return [performPartialLabeling, labeledPoints];
            return {
                ...payload,
                ...{
                    'rerender': rerender,
                    'performPartialLabeling': performPartialLabeling,
                    'showToolTipForTerm': showToolTipForTerm,
                    'svg': svg,
                    'data': data,
                    'xLabel': xLabel,
                    'yLabel': yLabel,
                    'drawXLabel': drawXLabel,
                    'drawYLabel': drawYLabel,
                    'populateCorpusStats': populateCorpusStats
                }
            };
        }


        //fullData = getDataAndInfo();
        if (fullData.docs) {
            var corpusWordCounts = getCorpusWordCounts();
        }
        var payload = processData(fullData);

        // The tool tip is down here in order to make sure it has the highest z-index
        var tooltip = d3.select('#' + divName)
            .append("div")
            //.attr("class", getTooltipContent == null && sortByDist ? "tooltip" : "tooltipscore")
            .attr("class", "tooltipscore")
            .style("opacity", 0);

        plotInterface = {}
        if (payload.topTermsPane) {
            plotInterface.topTermsPane = payload.topTermsPane;
            plotInterface.showTopTermsPane = payload.showTopTermsPane;
            plotInterface.showAssociatedWordList = payload.showAssociatedWordList;
        }
        plotInterface.includeAllContexts = includeAllContexts;
        plotInterface.divName = divName;
        plotInterface.displayTermContexts = displayTermContexts;
        plotInterface.gatherTermContexts = gatherTermContexts;
        plotInterface.xLabel = payload.xLabel;
        plotInterface.yLabel = payload.yLabel;
        plotInterface.drawXLabel = payload.drawXLabel;
        plotInterface.drawYLabel = payload.drawYLabel;
        plotInterface.svg = payload.svg;
        plotInterface.termDict = termDict;
        plotInterface.showToolTipForTerm = payload.showToolTipForTerm;
        plotInterface.fullData = fullData;
        plotInterface.data = payload.data;
        plotInterface.rerender = payload.rerender;
        plotInterface.populateCorpusStats = payload.populateCorpusStats;
        plotInterface.handleSearch = handleSearch;
        plotInterface.handleSearchTerm = handleSearchTerm;
        plotInterface.highlightTerm = highlightTerm;
        plotInterface.y = y;
        plotInterface.x = x;
        plotInterface.tooltip = tooltip;
        plotInterface.alternativeTermFunc = alternativeTermFunc;

        plotInterface.showTooltipSimple = function (term) {
            plotInterface.showToolTipForTerm(
                plotInterface.data,
                plotInterface.svg,
                term.replace("'", "\\'"),
                plotInterface.termDict[term.replace("'", "\\'")]
            )
        };

        plotInterface.drawCategoryAssociation = function (category, otherCategory = null) {
            console.log("+++++++ Entering drawCategoryAssociation")
            console.log("Category: " + category)
            console.log("Other Category: " + otherCategory)
            var categoryNum = this.fullData.info.categories.indexOf(category);

            var otherCategoryNum = null;
            if(otherCategory !== null)
                otherCategoryNum = this.fullData.info.categories.indexOf(otherCategory);

            console.log("cat/other: " + category + "/" + otherCategory + " ::: " + categoryNum + "/" + otherCategoryNum)

            console.log("Full Data")
            console.log(this.fullData)
            /*
            var rawLogTermCounts = getTermCounts(this.fullData).map(Math.log);
            var maxRawLogTermCounts = Math.max(...rawLogTermCounts);
            var minRawLogTermCounts = Math.min(...rawLogTermCounts);
            var logTermCounts = rawLogTermCounts.map(
                x => (x - minRawLogTermCounts) / maxRawLogTermCounts
            )
            */

            //var rawScores = getCategoryDenseRankScores(this.fullData, categoryNum);
            //console.log("RAW SCORES")
            //console.log(rawScores);
            /*
            function logOddsRatioUninformativeDirichletPrior(fgFreqs, bgFreqs, alpha) {
                var fgVocabSize = fgFreqs.reduce((x,y) => x+y);
                var fgL = fgFreqs.map(x => (x + alpha)/((1+alpha)*fgVocabSize - x - alpha))
                var bgVocabSize = bgFreqs.reduce((x,y) => x+y);
                var bgL = bgFreqs.map(x => (x + alpha)/((1+alpha)*bgVocabSize - x - alpha))
                var pooledVar = fgFreqs.map(function(x, i) {
                    return (
                        1/(x + alpha)
                        + 1/((1+alpha)*fgVocabSize - x - alpha)
                        + 1/(bgFreqs[i] + alpha)
                        + 1/((1+alpha)*bgVocabSize - bgFreqs[i] - alpha))
                })
                return pooledVar.map(function(x, i) {
                    return (Math.log(fgL[i]) - Math.log(bgL[i]))/x;
                })
            }
            var rawScores = logOddsRatioUninformativeDirichletPrior(
                denseRanks.fgFreqs, denseRanks.bgFreqs, 0.01);
            */


            var denseRanks = getDenseRanks(this.fullData, categoryNum)
            if (otherCategoryNum !== null) {
                var otherDenseRanks = getDenseRanks(this.fullData, otherCategoryNum);
                denseRanks.bg = otherDenseRanks.fg;
                denseRanks.bgFreqs = otherDenseRanks.fgFreqs;
            }

            var rawScores = denseRanks.fg.map((x, i) => x - denseRanks.bg[i]);
            var minRawScores = Math.min(...rawScores);
            var maxRawScores = Math.max(...rawScores);

            var scores = rawScores.map(
                function (rawScore) {
                    if (rawScore == 0) {
                        return 0.5;
                    } else if (rawScore > 0) {
                        return rawScore / (2. * maxRawScores) + 0.5;
                    } else if (rawScore < 0) {
                        return 0.5 - rawScore / (2. * minRawScores);
                    }
                }
            )
            var fgFreqSum = denseRanks.fgFreqs.reduce((a, b) => a + b, 0)
            var bgFreqSum = denseRanks.bgFreqs.reduce((a, b) => a + b, 0)

            //!!! OLD and good
            var ox = denseRanks.bg;
            var oy = denseRanks.fg;

            var oxmax = Math.max(...ox)
            var oxmin = Math.min(...ox)
            var ox = ox.map(x => (x - oxmin) / (oxmax - oxmin))
            var oymax = Math.max(...oy)
            var oymin = Math.min(...oy)
            var oy = oy.map(x => (x - oymin) / (oymax - oymin))
            //var ox = logTermCounts
            //var oy = scores;
            var xf = this.x;
            var yf = this.y;

            this.fullData.data = this.fullData.data.map(function (term, i) {
                //term.ci = i;
                term.s = scores[term.i];
                term.os = rawScores[term.i];
                term.cat = denseRanks.fgFreqs[term.i];
                term.ncat = denseRanks.bgFreqs[term.i];
                term.cat25k = parseInt(denseRanks.fgFreqs[term.i] * 25000 / fgFreqSum);
                term.ncat25k = parseInt(denseRanks.bgFreqs[term.i] * 25000 / bgFreqSum);
                term.x = xf(ox[term.i]) // logTermCounts[term.i];
                term.y = yf(oy[term.i]) // scores[term.i];
                term.ox = ox[term.i];
                term.oy = oy[term.i];
                term.display = true;
                return term;
            })

            // Feature selection
            var targetTermsToShow = 1500;

            var sortedBg = denseRanks.bg.map((x, i) => [x, i]).sort((a, b) => b[0] - a[0]).map(x => x[1]).slice(0, parseInt(targetTermsToShow / 2));
            var sortedFg = denseRanks.fg.map((x, i) => [x, i]).sort((a, b) => b[0] - a[0]).map(x => x[1]).slice(0, parseInt(targetTermsToShow / 2));
            var sortedScores = denseRanks.fg.map((x, i) => [x, i]).sort((a, b) => b[0] - a[0]).map(x => x[1]);
            var myFullData = this.fullData

            sortedBg.concat(sortedFg)//.concat(sortedScores.slice(0, parseInt(targetTermsToShow/2))).concat(sortedScores.slice(-parseInt(targetTermsToShow/4)))
                .forEach(function (i) {
                    myFullData.data[i].display = true;
                })

            console.log('newly filtered')
            console.log(myFullData)

            // begin rescaling to ignore hidden terms
            /*
            function scaleDenseRanks(ranks) {
                var max = Math.max(...ranks);
                return ranks.map(x=>x/max)
            }
            var filteredData = myFullData.data.filter(d=>d.display);
            var catRanks = scaleDenseRanks(denseRank(filteredData.map(d=>d.cat)))
            var ncatRanks = scaleDenseRanks(denseRank(filteredData.map(d=>d.ncat)))
            var rawScores = catRanks.map((x,i) => x - ncatRanks[i]);
            function stretch_0_1(scores) {
                var max = 1.*Math.max(...rawScores);
                var min = -1.*Math.min(...rawScores);
                return scores.map(function(x, i) {
                    if(x == 0) return 0.5;
                    if(x > 0) return (x/max + 1)/2;
                    return (x/min + 1)/2;
                })
            }
            var scores = stretch_0_1(rawScores);
            console.log(scores)
            filteredData.forEach(function(d, i) {
                d.x = xf(catRanks[i]);
                d.y = yf(ncatRanks[i]);
                d.ox = catRanks[i];
                d.oy = ncatRanks[i];
                d.s = scores[i];
                d.os = rawScores[i];
            });
            console.log("rescaled");
            */
            // end rescaling


            this.rerender(//denseRanks.bg,
                fullData.data.map(x => x.ox), //ox
                //denseRanks.fg,
                fullData.data.map(x => x.oy), //oy,
                d => d3.interpolateRdYlBu(d.s));
            if (this.yLabel !== undefined) {
                this.yLabel.remove()
            }
            if (this.xLabel !== undefined) {
                this.xLabel.remove()
            }
            var leftName = this.fullData.info.categories[categoryNum];
            var bottomName = "Not " + this.fullData.info.categories[categoryNum];
            if (otherCategoryNum !== null) {
                bottomName = this.fullData.info.categories[otherCategoryNum];
            }


            this.yLabel = this.drawYLabel(this.svg, leftName + ' Frequncy Rank')
            this.xLabel = this.drawXLabel(this.svg, bottomName + ' Frequency Rank')
            if (this.topTermsPane !== undefined) {
                this.topTermsPane.catHeader.remove()
                this.topTermsPane.notCatHeader.remove()
                this.topTermsPane.wordListData.wordObjList.map(x => x.remove())
                this.topTermsPane.notWordListData.wordObjList.map(x => x.remove())
            }
            this.showWordList = payload.showWordList;


            this.showAssociatedWordList = function (
                data,
                word,
                header,
                isUpperPane,
                xOffset=this.topTermsPane.startingOffset,
                length = 14
            ) {
                var sortedData = null;
                if (!isUpperPane) {
                    sortedData = data.map(x => x).sort((a, b) => scores[a.i] - scores[b.i])
                } else {
                    sortedData = data.map(x => x).sort((a, b) => scores[b.i] - scores[a.i])
                }
                console.log('sortedData');
                console.log(isUpperPane);
                console.log(sortedData.slice(0, length))
                console.log(payload)
                console.log(word)
                return payload.showWordList(word, sortedData.slice(0, length), xOffset);
            }
            if (this.topTermsPane !== undefined)
                this.topTermsPane = payload.showTopTermsPane(
                    this.data,
                    this.topTermsPane.registerFigureBBox,
                    this.showAssociatedWordList,
                    "Top " + leftName,
                    "Top " + bottomName,
                    this.topTermsPane.startingOffset
                )

            fullData.info.category_name = leftName;
            fullData.info.not_category_name = bottomName;
            fullData.info.category_internal_name = this.fullData.info.categories[categoryNum];
            if (otherCategoryNum === null) {
                fullData.info.not_category_internal_names = this.fullData.info.categories
                    .filter(x => x !== this.fullData.info.categories[categoryNum]);
            } else {
                fullData.info.not_category_internal_names = this.fullData.info.categories
                    .filter(x => x === this.fullData.info.categories[otherCategoryNum]);

                fullData.info.neutral_category_internal_names = this.fullData.info.categories
                    .filter(x => (x !== this.fullData.info.categories[categoryNum]
                        && x !== this.fullData.info.categories[otherCategoryNum]));
                fullData.info.neutral_category_name = "All Others";

            }
            console.log("fullData.info.not_category_internal_names");
            console.log(fullData.info.not_category_internal_names);
            ['snippets', 'snippetsalt', 'termstats',
                'overlapped-terms-clicked', 'categoryinfo',
                'cathead', 'cat', 'corpus-stats', 'notcathead',
                'notcat', 'neuthead', 'neut'
            ].forEach(function (divSubName) {
                var mydiv = '#' + divName + '-' + divSubName;
                console.log("Clearing");
                console.log(mydiv);
                d3.select(mydiv).selectAll("*").remove();
                d3.select(mydiv).html("");

            });
            this.populateCorpusStats();

            console.log(fullData)
        };

        plotInterface.yAxisLogCounts = function (categoryName) {
            var categoryNum = this.fullData.docs.categories.indexOf(categoryName);
            var denseRanks = getDenseRanks(this.fullData, categoryNum)
            console.log("denseRanks")
            console.log(denseRanks);

            var rawScores = denseRanks.fg.map((x, i) => x - denseRanks.bg[i]);
            var minRawScores = Math.min(...rawScores);
            var maxRawScores = Math.max(...rawScores);

            var scores = rawScores.map(
                function (rawScore) {
                    if (rawScore == 0) {
                        return 0.5;
                    } else if (rawScore > 0) {
                        return rawScore / (2. * maxRawScores) + 0.5;
                    } else if (rawScore < 0) {
                        return 0.5 - rawScore / (2. * minRawScores);
                    }
                }
            )
            var fgFreqSum = denseRanks.fgFreqs.reduce((a, b) => a + b, 0)
            var bgFreqSum = denseRanks.bgFreqs.reduce((a, b) => a + b, 0)

            var oy = denseRanks.fgFreqs.map(count => Math.log(count + 1) / Math.log(2))

            var oymax = Math.max(...oy)
            var oymin = Math.min(...oy)
            oy = oy.map(y => (y - oymin) / (oymax - oymin))
            var xf = this.x;
            var yf = this.y;
            var ox = this.fullData.data.map(term => term.ox);
            var oxmax = Math.max(...ox)
            var oxmin = Math.min(...ox)
            ox = ox.map(y => (y - oxmin) / (oxmax - oxmin))


            this.fullData.data = this.fullData.data.map(function (term, i) {
                term.s = 1;//scores[i];
                term.os = rawScores[i];
                term.cat = denseRanks.fgFreqs[i];
                term.ncat = denseRanks.bgFreqs[i];
                term.cat25k = parseInt(denseRanks.fgFreqs[i] * 25000 / fgFreqSum);
                term.ncat25k = parseInt(denseRanks.bgFreqs[i] * 25000 / bgFreqSum);
                //term.x = xf(term.ox) // scores[term.i];
                //term.ox = term.ox;
                term.y = yf(oy[i]) // scores[term.i];
                term.oy = oy[i];
                term.x = xf(ox[i]) // scores[term.i];
                term.ox = ox[i];
                term.display = true;
                return term;
            })


            this.rerender(//denseRanks.bg,
                this.fullData.data.map(point => point.ox), //ox
                this.fullData.data.map(point => point.oy), //oy,
                d => d3.interpolateRdYlBu(d.s)
            );

            if (this.yLabel !== undefined) {
                this.yLabel.remove()
                this.yLabel = this.drawYLabel(this.svg, this.fullData.info.categories[categoryNum] + ' log freq.')
            }

            if (this.topTermsPane !== undefined) {
                this.topTermsPane.catHeader.remove()
                this.topTermsPane.notCatHeader.remove()
                this.topTermsPane.wordListData.wordObjList.map(x => x.remove())
                this.topTermsPane.notWordListData.wordObjList.map(x => x.remove())
            }
            this.showWordList = payload.showWordList;


            this.showAssociatedWordList = function (data, word, header, isUpperPane, xOffset=this.topTermsPane.startingOffset, length = 14) {
                var sortedData = null;
                if (!isUpperPane) {
                    sortedData = data.map(x => x).sort((a, b) => scores[a.i] - scores[b.i])
                } else {
                    sortedData = data.map(x => x).sort((a, b) => scores[b.i] - scores[a.i])
                }
                console.log('sortedData');
                console.log(isUpperPane);
                console.log(sortedData.slice(0, length))
                console.log(payload)
                console.log(word)
                return payload.showWordList(word, sortedData.slice(0, length), xOffset);
            }
            var leftName = this.fullData.info.categories[categoryNum];
            var bottomName = "Not " + this.fullData.info.categories[categoryNum];

            if (this.topTermsPane !== undefined)
                this.topTermsPane = payload.showTopTermsPane(
                    this.data,
                    this.topTermsPane.registerFigureBBox,
                    this.showAssociatedWordList,
                    "Top " + leftName,
                    "Top " + bottomName,
                    this.topTermsPane.startingOffset
                )

            fullData.info.category_name = leftName;
            fullData.info.not_category_name = bottomName;
            fullData.info.category_internal_name = this.fullData.info.categories[categoryNum];
            fullData.info.not_category_internal_names = this.fullData.info.categories
                .filter(x => x !== this.fullData.info.categories[categoryNum]);

            console.log("fullData.info.not_category_internal_names");
            console.log(fullData.info.not_category_internal_names);
            ['snippets', 'snippetsalt', 'termstats',
                'overlapped-terms-clicked', 'categoryinfo',
                'cathead', 'cat', 'corpus-stats', 'notcathead',
                'notcat', 'neuthead', 'neut'
            ].forEach(function (divSubName) {
                var mydiv = '#' + divName + '-' + divSubName;
                console.log("Clearing");
                console.log(mydiv);
                d3.select(mydiv).selectAll("*").remove();
                d3.select(mydiv).html("");

            });
            this.populateCorpusStats();
        };

        return plotInterface
    };
}(d3);

; 
 
 // Adapted from https://www.w3schools.com/howto/howto_js_autocomplete.asp
function autocomplete(inputField, autocompleteValues, myPlotInterface) {
    var currentFocus; // current position in autocomplete list.

    inputField.addEventListener("input", function (e) {
        var matchedCandidateListDiv, matchedCandidateDiv, i, userInput = this.value;

        closeAllLists();
        if (!userInput) {
            return false;
        }
        currentFocus = -1;

        matchedCandidateListDiv = document.createElement("div");
        matchedCandidateListDiv.setAttribute("id", this.id + "autocomplete-list");
        matchedCandidateListDiv.setAttribute("class", "autocomplete-items");

        this.parentNode.appendChild(matchedCandidateListDiv);
        autocompleteValues.map(function (candidate) {
            var candidatePrefix = candidate.substr(0, userInput.length);
            if (candidatePrefix.toLowerCase() === userInput.toLowerCase()) {
                matchedCandidateDiv = document.createElement("div");
                matchedCandidateDiv.innerHTML = "<strong>" + candidatePrefix + "</strong>";
                matchedCandidateDiv.innerHTML += candidate.substr(userInput.length);
                matchedCandidateDiv.innerHTML += '<input type=hidden value="' + encodeURIComponent(candidate) + '">';
                matchedCandidateDiv.addEventListener("click", function (e) {
                    console.log("CLICK")
                    console.log(this.getElementsByTagName("input")[0].value)
                    inputField.value = decodeURIComponent(this.getElementsByTagName("input")[0].value);
                    console.log(inputField.value)
                    closeAllLists();
                    myPlotInterface.handleSearchTerm(inputField.value);
                });
                matchedCandidateListDiv.appendChild(matchedCandidateDiv);
            }
        });
    });

    inputField.addEventListener("keydown", function (keyboardEvent) {

        var candidateDivList = document.getElementById(this.id + "autocomplete-list");

        if (!candidateDivList)
            return true;

        var selectedCandidate = Array.prototype.find.call(
            candidateDivList.children,
            x => x.className !== ""
        );

        if (keyboardEvent.keyCode === 40 || keyboardEvent.keyCode === 9) { // down or tab
            keyboardEvent.preventDefault();
            currentFocus++;
            addActive(candidateDivList.getElementsByTagName("div"));
        } else if (keyboardEvent.keyCode === 38) { //up
            currentFocus--;
            addActive(candidateDivList.getElementsByTagName("div"));
        } else if (keyboardEvent.keyCode === 13) { // enter
            keyboardEvent.preventDefault();
            var selectedTerm = inputField.value;
            console.log("selected term");console.log(selectedTerm);
            console.log(myPlotInterface);
            //if (selectedCandidate)
            //    selectedTerm = selectedCandidate.children[1].value;
            myPlotInterface.handleSearchTerm(selectedTerm);
            closeAllLists(null);
        } else if (keyboardEvent.keyCode === 27) { // esc
            closeAllLists(null);
        }
    });

    function addActive(candidateDivList) {
        if (!candidateDivList) return false;

        removeActive(candidateDivList);

        if (currentFocus >= candidateDivList.length)
            currentFocus = 0;
        if (currentFocus < 0)
            currentFocus = (candidateDivList.length - 1);

        candidateDivList[currentFocus].classList.add("autocomplete-active");

        var selectedCandidate = Array.prototype.find.call(
            candidateDivList,
            x => x.className !== ""
        );

        if (selectedCandidate) {
            var candidateValue = decodeURIComponent(selectedCandidate.children[1].value);

            myPlotInterface.highlightTerm(candidateValue);
            inputField.value = candidateValue;
        }

    }

    function removeActive(candidateDivList) {
        Array.prototype.find.call(
            candidateDivList,
            x => x.classList.remove("autocomplete-active")
        );
    }

    function closeAllLists(elmnt) {
        /*close all autocomplete lists in the document,
        except the one passed as an argument:*/
        var x = document.getElementsByClassName("autocomplete-items");
        for (var i = 0; i < x.length; i++) {
            if (elmnt != x[i] && elmnt != inputField) {
                x[i].parentNode.removeChild(x[i]);
            }
        }
    }

    /*execute a function when someone clicks in the document:*/
    document.addEventListener("click", function (e) {
        closeAllLists(e.target);
    });
}

function getDataAndInfo() { return{"info": {"category_name": "Data scientist", "not_category_name": "Data Engineer", "category_terms": ["statistical", "quantitative", "statistics", "mining", "r", "research", "math", "mathematics", "problems", "machine"], "not_category_terms": ["apache", "kafka", "self", "implement", "streaming", "dimensional", "excited", "distributed", "storm", "design"], "category_internal_name": "data scientist", "not_category_internal_names": ["data engineer"], "categories": ["data scientist", "data engineer"], "neutral_category_internal_names": [], "extra_category_internal_names": [], "neutral_category_name": "Neutral", "extra_category_name": "Extra"}, "data": [{"x": 0.9844961240310077, "y": 0.9328358208955224, "ox": 0.9844961240310077, "oy": 0.9328358208955224, "term": "years", "cat25k": 263, "ncat25k": 286, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 308, "ncat": 277, "s": 0.3333333333333333, "os": -0.05128205128205132, "bg": 3.462596264208442e-06}, {"x": 0.20930232558139533, "y": 0.32835820895522383, "ox": 0.20930232558139533, "oy": 0.32835820895522383, "term": "relevant", "cat25k": 38, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 27, "s": 0.7286324786324787, "os": 0.11794871794871792, "bg": 3.134862744560412e-06}, {"x": 0.9922480620155038, "y": 1.0, "ox": 0.9922480620155038, "oy": 1.0, "term": "experience", "cat25k": 1135, "ncat25k": 1167, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1330, "ncat": 1131, "s": 0.6292735042735043, "os": 0.007692307692307665, "bg": 3.5876734828186594e-05}, {"x": 0.3720930232558139, "y": 0.4402985074626865, "ox": 0.3720930232558139, "oy": 0.4402985074626865, "term": "preferred", "cat25k": 50, "ncat25k": 52, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 50, "s": 0.6602564102564104, "os": 0.0675213675213675, "bg": 1.106868397663289e-05}, {"x": 0.03875968992248062, "y": 0.26119402985074625, "ox": 0.03875968992248062, "oy": 0.26119402985074625, "term": "healthcare", "cat25k": 30, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 5, "s": 0.842948717948718, "os": 0.2205128205128205, "bg": 2.7987159631097047e-06}, {"x": 0.6589147286821705, "y": 0.29104477611940294, "ox": 0.6589147286821705, "oy": 0.29104477611940294, "term": "industry", "cat25k": 33, "ncat25k": 97, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 94, "s": 0.05876068376068376, "os": -0.36524216524216524, "bg": 1.653533662044636e-06}, {"x": 0.9147286821705426, "y": 0.8582089552238805, "ox": 0.9147286821705426, "oy": 0.8582089552238805, "term": "knowledge", "cat25k": 192, "ncat25k": 177, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 225, "ncat": 171, "s": 0.3087606837606838, "os": -0.056125356125356074, "bg": 8.81914892005958e-06}, {"x": 0.0, "y": 0.29850746268656714, "ox": 0.0, "oy": 0.29850746268656714, "term": "pursuing", "cat25k": 34, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 0, "s": 0.9198717948717949, "os": 0.29601139601139603, "bg": 1.9253818272836232e-05}, {"x": 0.05426356589147286, "y": 0.5597014925373134, "ox": 0.05426356589147286, "oy": 0.5597014925373134, "term": "phd", "cat25k": 67, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 78, "ncat": 7, "s": 0.983974358974359, "os": 0.5014245014245015, "bg": 1.450483356514048e-05}, {"x": 0.22480620155038755, "y": 0.32089552238805963, "ox": 0.22480620155038755, "oy": 0.32089552238805963, "term": "ms", "cat25k": 37, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 29, "s": 0.6901709401709403, "os": 0.09515669515669517, "bg": 2.104820776264918e-06}, {"x": 0.05426356589147286, "y": 0.7014925373134329, "ox": 0.05426356589147286, "oy": 0.7014925373134329, "term": "math", "cat25k": 96, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 112, "ncat": 7, "s": 0.9935897435897436, "os": 0.6421652421652422, "bg": 8.296477625846498e-06}, {"x": 0.17829457364341084, "y": 0.8955223880597015, "ox": 0.17829457364341084, "oy": 0.8955223880597015, "term": "statistics", "cat25k": 206, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 242, "ncat": 23, "s": 0.997863247863248, "os": 0.7116809116809116, "bg": 7.653478231486238e-06}, {"x": 0.031007751937984492, "y": 0.32089552238805963, "ox": 0.031007751937984492, "oy": 0.32089552238805963, "term": "physics", "cat25k": 37, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 4, "s": 0.9113247863247864, "os": 0.2874643874643875, "bg": 3.15651276394701e-06}, {"x": 0.007751937984496123, "y": 0.5074626865671641, "ox": 0.007751937984496123, "oy": 0.5074626865671641, "term": "economics", "cat25k": 60, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 70, "ncat": 1, "s": 0.9818376068376069, "os": 0.4957264957264957, "bg": 4.591826852169142e-06}, {"x": 0.09302325581395349, "y": 0.8432835820895522, "ox": 0.09302325581395349, "oy": 0.8432835820895522, "term": "quantitative", "cat25k": 181, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 212, "ncat": 12, "s": 0.9989316239316239, "os": 0.7444444444444445, "bg": 6.482062916523184e-05}, {"x": 0.4031007751937984, "y": 0.917910447761194, "ox": 0.4031007751937984, "oy": 0.917910447761194, "term": "field", "cat25k": 224, "ncat25k": 57, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 262, "ncat": 55, "s": 0.9861111111111112, "os": 0.5108262108262108, "bg": 4.6843492625393495e-06}, {"x": 0.0, "y": 0.26865671641791045, "ox": 0.0, "oy": 0.26865671641791045, "term": "graduation", "cat25k": 31, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 0, "s": 0.8920940170940171, "os": 0.26638176638176636, "bg": 7.659487280304364e-06}, {"x": 0.0697674418604651, "y": 0.2761194029850746, "ox": 0.0697674418604651, "oy": 0.2761194029850746, "term": "candidates", "cat25k": 32, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 9, "s": 0.8354700854700855, "os": 0.20455840455840457, "bg": 4.572845755974161e-06}, {"x": 0.01550387596899225, "y": 0.13432835820895522, "ox": 0.01550387596899225, "oy": 0.13432835820895522, "term": "currently", "cat25k": 15, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 2, "s": 0.7275641025641025, "os": 0.11766381766381767, "bg": 4.4877835183883957e-07}, {"x": 0.7286821705426356, "y": 0.8134328358208954, "ox": 0.7286821705426356, "oy": 0.8134328358208954, "term": "business", "cat25k": 159, "ncat25k": 109, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 186, "ncat": 106, "s": 0.6730769230769231, "os": 0.08404558404558404, "bg": 9.165250946658211e-07}, {"x": 0.0, "y": 0.2761194029850746, "ox": 0.0, "oy": 0.2761194029850746, "term": "school", "cat25k": 32, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 0, "s": 0.9006410256410257, "os": 0.2737891737891738, "bg": 2.1567288941825507e-07}, {"x": 0.8449612403100775, "y": 0.9776119402985075, "ox": 0.8449612403100775, "oy": 0.9776119402985075, "term": "ability", "cat25k": 409, "ncat25k": 141, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 480, "ncat": 137, "s": 0.7457264957264959, "os": 0.1316239316239316, "bg": 2.370353068315439e-05}, {"x": 0.046511627906976744, "y": 0.04477611940298507, "ox": 0.046511627906976744, "oy": 0.04477611940298507, "term": "execute", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 6, "s": 0.6121794871794872, "os": -0.0019943019943019988, "bg": 2.7417955196319413e-06}, {"x": 0.30232558139534876, "y": 0.19402985074626863, "ox": 0.30232558139534876, "oy": 0.19402985074626863, "term": "multiple", "cat25k": 22, "ncat25k": 40, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 39, "s": 0.17735042735042736, "os": -0.1076923076923077, "bg": 1.962967204314324e-06}, {"x": 0.05426356589147286, "y": 0.04477611940298507, "ox": 0.05426356589147286, "oy": 0.04477611940298507, "term": "strategy", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 7, "s": 0.5630341880341881, "os": -0.009686609686609692, "bg": 4.543467413910947e-07}, {"x": 0.15503875968992245, "y": 0.08955223880597014, "ox": 0.15503875968992245, "oy": 0.08955223880597014, "term": "vision", "cat25k": 10, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 20, "s": 0.25747863247863245, "os": -0.06524216524216525, "bg": 1.4719258546789223e-06}, {"x": 0.9767441860465117, "y": 0.9253731343283583, "ox": 0.9767441860465117, "oy": 0.9253731343283583, "term": "skills", "cat25k": 258, "ncat25k": 274, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 302, "ncat": 265, "s": 0.3354700854700855, "os": -0.050997150997150964, "bg": 1.5873679331576437e-05}, {"x": 0.23255813953488366, "y": 0.6641791044776119, "ox": 0.23255813953488366, "oy": 0.6641791044776119, "term": "demonstrated", "cat25k": 86, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 101, "ncat": 30, "s": 0.9636752136752137, "os": 0.4282051282051281, "bg": 2.2248622411156243e-05}, {"x": 0.10077519379844961, "y": 0.23880597014925373, "ox": 0.10077519379844961, "oy": 0.23880597014925373, "term": "communicate", "cat25k": 27, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 13, "s": 0.7553418803418804, "os": 0.13675213675213674, "bg": 6.897757906784926e-06}, {"x": 0.5348837209302324, "y": 0.8805970149253731, "ox": 0.5348837209302324, "oy": 0.8805970149253731, "term": "technical", "cat25k": 200, "ncat25k": 77, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 235, "ncat": 75, "s": 0.9433760683760684, "os": 0.3430199430199431, "bg": 6.028312455601964e-06}, {"x": 0.0697674418604651, "y": 0.06716417910447761, "ox": 0.0697674418604651, "oy": 0.06716417910447761, "term": "general", "cat25k": 8, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 9, "s": 0.607905982905983, "os": -0.0028490028490028574, "bg": 1.1545393955460307e-07}, {"x": 0.031007751937984492, "y": 0.12686567164179102, "ox": 0.031007751937984492, "oy": 0.12686567164179102, "term": "audiences", "cat25k": 15, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 4, "s": 0.688034188034188, "os": 0.09487179487179487, "bg": 8.519447769395588e-06}, {"x": 0.6046511627906975, "y": 0.6343283582089552, "ox": 0.6046511627906975, "oy": 0.6343283582089552, "term": "including", "cat25k": 80, "ncat25k": 90, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 94, "ncat": 87, "s": 0.639957264957265, "os": 0.029344729344729426, "bg": 1.6896110952597274e-06}, {"x": 0.0, "y": 0.19402985074626863, "ox": 0.0, "oy": 0.19402985074626863, "term": "tell", "cat25k": 22, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 0, "s": 0.8215811965811967, "os": 0.19230769230769232, "bg": 4.7441530708287e-07}, {"x": 1.0, "y": 0.9925373134328359, "ox": 1.0, "oy": 0.9925373134328359, "term": "data", "cat25k": 931, "ncat25k": 1313, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1091, "ncat": 1272, "s": 0.5950854700854702, "os": -0.007407407407407418, "bg": 1.1612776845921648e-05}, {"x": 0.09302325581395349, "y": 0.05223880597014925, "ox": 0.09302325581395349, "oy": 0.05223880597014925, "term": "write", "cat25k": 6, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 12, "s": 0.3899572649572649, "os": -0.040740740740740744, "bg": 2.9992102842541803e-07}, {"x": 0.19379844961240308, "y": 0.29104477611940294, "ox": 0.19379844961240308, "oy": 0.29104477611940294, "term": "code", "cat25k": 33, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 25, "s": 0.6933760683760685, "os": 0.09629629629629627, "bg": 5.113862669311503e-07}, {"x": 0.821705426356589, "y": 0.7388059701492536, "ox": 0.821705426356589, "oy": 0.7388059701492536, "term": "development", "cat25k": 107, "ncat25k": 135, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 126, "ncat": 131, "s": 0.21581196581196582, "os": -0.08233618233618234, "bg": 1.7950279451102947e-06}, {"x": 0.9689922480620154, "y": 0.7462686567164178, "ox": 0.9689922480620154, "oy": 0.7462686567164178, "term": "sql", "cat25k": 108, "ncat25k": 254, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 127, "ncat": 246, "s": 0.08974358974358974, "os": -0.2210826210826211, "bg": 2.8579729937642555e-05}, {"x": 0.9457364341085271, "y": 0.9104477611940299, "ox": 0.9457364341085271, "oy": 0.9104477611940299, "term": "python", "cat25k": 218, "ncat25k": 211, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 256, "ncat": 204, "s": 0.40705128205128205, "os": -0.03504273504273503, "bg": 5.207776999818917e-05}, {"x": 0.20930232558139533, "y": 0.8656716417910447, "ox": 0.20930232558139533, "oy": 0.8656716417910447, "term": "r", "cat25k": 197, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 231, "ncat": 27, "s": 0.9957264957264957, "os": 0.6512820512820513, "bg": 1.5946136275594082e-06}, {"x": 0.9302325581395349, "y": 0.3582089552238806, "ox": 0.9302325581395349, "oy": 0.3582089552238806, "term": "design", "cat25k": 41, "ncat25k": 181, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 175, "s": 0.009615384615384616, "os": -0.5678062678062679, "bg": 1.6861786330611818e-06}, {"x": 0.8837209302325579, "y": 0.753731343283582, "ox": 0.8837209302325579, "oy": 0.753731343283582, "term": "tools", "cat25k": 109, "ncat25k": 155, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 128, "ncat": 150, "s": 0.14316239316239318, "os": -0.12905982905982905, "bg": 2.995512103293921e-06}, {"x": 0.9069767441860465, "y": 0.5970149253731343, "ox": 0.9069767441860465, "oy": 0.5970149253731343, "term": "e", "cat25k": 73, "ncat25k": 172, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 86, "ncat": 167, "s": 0.06837606837606838, "os": -0.3076923076923077, "bg": 8.530851314625768e-07}, {"x": 0.15503875968992245, "y": 0.29104477611940294, "ox": 0.15503875968992245, "oy": 0.29104477611940294, "term": "tableau", "cat25k": 33, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 20, "s": 0.7521367521367522, "os": 0.13475783475783473, "bg": 0.0002109044763581444}, {"x": 0.5503875968992247, "y": 0.8283582089552238, "ox": 0.5503875968992247, "oy": 0.8283582089552238, "term": "plus", "cat25k": 171, "ncat25k": 81, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 200, "ncat": 78, "s": 0.9049145299145299, "os": 0.27578347578347573, "bg": 5.8658836598193444e-06}, {"x": 0.8294573643410852, "y": 0.33582089552238803, "ox": 0.8294573643410852, "oy": 0.33582089552238803, "term": "building", "cat25k": 38, "ncat25k": 136, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 132, "s": 0.019230769230769232, "os": -0.4900284900284901, "bg": 2.7590207879824162e-06}, {"x": 0.5271317829457364, "y": 0.05223880597014925, "ox": 0.5271317829457364, "oy": 0.05223880597014925, "term": "pipelines", "cat25k": 6, "ncat25k": 76, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 74, "s": 0.024572649572649572, "os": -0.47150997150997154, "bg": 7.823407345020474e-05}, {"x": 0.0697674418604651, "y": 0.04477611940298507, "ox": 0.0697674418604651, "oy": 0.04477611940298507, "term": "stock", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 9, "s": 0.45833333333333337, "os": -0.025071225071225077, "bg": 1.7011580911561368e-07}, {"x": 0.1705426356589147, "y": 0.31343283582089554, "ox": 0.1705426356589147, "oy": 0.31343283582089554, "term": "employee", "cat25k": 36, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 22, "s": 0.7606837606837608, "os": 0.1415954415954416, "bg": 2.7963023707291803e-06}, {"x": 0.10077519379844961, "y": 0.05223880597014925, "ox": 0.10077519379844961, "oy": 0.05223880597014925, "term": "travel", "cat25k": 6, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 13, "s": 0.33974358974358976, "os": -0.04843304843304844, "bg": 1.389978849491085e-07}, {"x": 0.18604651162790695, "y": 0.08955223880597014, "ox": 0.18604651162790695, "oy": 0.08955223880597014, "term": "competitive", "cat25k": 10, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 24, "s": 0.19337606837606838, "os": -0.09601139601139602, "bg": 2.8338407023359302e-06}, {"x": 0.12403100775193797, "y": 0.16417910447761191, "ox": 0.12403100775193797, "oy": 0.16417910447761191, "term": "salary", "cat25k": 19, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 16, "s": 0.6410256410256411, "os": 0.03960113960113959, "bg": 3.5830215318850255e-06}, {"x": 0.1472868217054263, "y": 0.06716417910447761, "ox": 0.1472868217054263, "oy": 0.06716417910447761, "term": "paid", "cat25k": 8, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 19, "s": 0.21794871794871795, "os": -0.07977207977207978, "bg": 9.279234622216262e-07}, {"x": 0.5503875968992247, "y": 0.7761194029850746, "ox": 0.5503875968992247, "oy": 0.7761194029850746, "term": "time", "cat25k": 124, "ncat25k": 81, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 145, "ncat": 78, "s": 0.8461538461538463, "os": 0.2239316239316239, "bg": 4.907782967123702e-07}, {"x": 0.1395348837209302, "y": 0.34328358208955223, "ox": 0.1395348837209302, "oy": 0.34328358208955223, "term": "medical", "cat25k": 39, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 18, "s": 0.8322649572649573, "os": 0.201994301994302, "bg": 8.241612662104634e-07}, {"x": 0.1317829457364341, "y": 0.05223880597014925, "ox": 0.1317829457364341, "oy": 0.05223880597014925, "term": "dental", "cat25k": 6, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 17, "s": 0.2200854700854701, "os": -0.07920227920227921, "bg": 2.0604767084571405e-06}, {"x": 0.23255813953488366, "y": 0.08955223880597014, "ox": 0.23255813953488366, "oy": 0.08955223880597014, "term": "insurance", "cat25k": 10, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 30, "s": 0.12393162393162394, "os": -0.14216524216524218, "bg": 4.34498826087624e-07}, {"x": 0.16279069767441856, "y": 0.14179104477611937, "ox": 0.16279069767441856, "oy": 0.14179104477611937, "term": "life", "cat25k": 16, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 21, "s": 0.4978632478632478, "os": -0.0210826210826211, "bg": 2.6091429443553236e-07}, {"x": 0.07751937984496123, "y": 0.33582089552238803, "ox": 0.07751937984496123, "oy": 0.33582089552238803, "term": "disability", "cat25k": 38, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 10, "s": 0.8782051282051282, "os": 0.25612535612535614, "bg": 5.03264355615744e-06}, {"x": 0.062015503875968984, "y": 0.029850746268656716, "ox": 0.062015503875968984, "oy": 0.029850746268656716, "term": "coverage", "cat25k": 3, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.40918803418803423, "os": -0.0321937321937322, "bg": 5.494955985288079e-07}, {"x": 0.11627906976744183, "y": 0.04477611940298507, "ox": 0.11627906976744183, "oy": 0.04477611940298507, "term": "401k", "cat25k": 5, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 15, "s": 0.23504273504273504, "os": -0.07122507122507124, "bg": 0.0}, {"x": 0.12403100775193797, "y": 0.45522388059701485, "ox": 0.12403100775193797, "oy": 0.45522388059701485, "term": "flexible", "cat25k": 52, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 61, "ncat": 16, "s": 0.9337606837606838, "os": 0.3284900284900285, "bg": 7.783927867048087e-06}, {"x": 0.031007751937984492, "y": 0.022388059701492536, "ox": 0.031007751937984492, "oy": 0.022388059701492536, "term": "spending", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 4, "s": 0.5801282051282052, "os": -0.008831908831908833, "bg": 7.151334393024874e-07}, {"x": 0.031007751937984492, "y": 0.022388059701492536, "ox": 0.031007751937984492, "oy": 0.022388059701492536, "term": "accounts", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 4, "s": 0.5801282051282052, "os": -0.008831908831908833, "bg": 3.3030444160382623e-07}, {"x": 0.15503875968992245, "y": 0.0373134328358209, "ox": 0.15503875968992245, "oy": 0.0373134328358209, "term": "apple", "cat25k": 4, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 20, "s": 0.15170940170940173, "os": -0.1170940170940171, "bg": 9.880242567859236e-07}, {"x": 0.01550387596899225, "y": 0.11194029850746266, "ox": 0.01550387596899225, "oy": 0.11194029850746266, "term": "equipment", "cat25k": 13, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 2, "s": 0.6912393162393162, "os": 0.09544159544159544, "bg": 2.3553355665978612e-07}, {"x": 0.03875968992248062, "y": 0.022388059701492536, "ox": 0.03875968992248062, "oy": 0.022388059701492536, "term": "daily", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 5, "s": 0.5181623931623932, "os": -0.016524216524216526, "bg": 1.5109646835236308e-07}, {"x": 0.03875968992248062, "y": 0.022388059701492536, "ox": 0.03875968992248062, "oy": 0.022388059701492536, "term": "lunch", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 5, "s": 0.5181623931623932, "os": -0.016524216524216526, "bg": 7.669345377273314e-07}, {"x": 0.062015503875968984, "y": 0.47761194029850745, "ox": 0.062015503875968984, "oy": 0.47761194029850745, "term": "computational", "cat25k": 55, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 65, "ncat": 8, "s": 0.9615384615384617, "os": 0.4122507122507122, "bg": 2.2357332653833762e-05}, {"x": 0.6821705426356588, "y": 0.9402985074626867, "ox": 0.6821705426356588, "oy": 0.9402985074626867, "term": "computer", "cat25k": 265, "ncat25k": 100, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 311, "ncat": 97, "s": 0.8782051282051282, "os": 0.25612535612535614, "bg": 3.639083153692253e-06}, {"x": 0.7519379844961239, "y": 0.9850746268656716, "ox": 0.7519379844961239, "oy": 0.9850746268656716, "term": "science", "cat25k": 444, "ncat25k": 114, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 521, "ncat": 110, "s": 0.8536324786324787, "os": 0.23133903133903133, "bg": 7.2408766374429206e-06}, {"x": 0.6124031007751937, "y": 0.8880597014925372, "ox": 0.6124031007751937, "oy": 0.8880597014925372, "term": "related", "cat25k": 205, "ncat25k": 91, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 240, "ncat": 88, "s": 0.8995726495726497, "os": 0.2735042735042734, "bg": 2.8375831959603506e-06}, {"x": 0.17829457364341084, "y": 0.30597014925373134, "ox": 0.17829457364341084, "oy": 0.30597014925373134, "term": "project", "cat25k": 35, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 23, "s": 0.7414529914529915, "os": 0.1264957264957265, "bg": 5.439459291908952e-07}, {"x": 0.27131782945736427, "y": 0.25373134328358204, "ox": 0.27131782945736427, "oy": 0.25373134328358204, "term": "based", "cat25k": 29, "ncat25k": 36, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 35, "s": 0.5032051282051283, "os": -0.017663817663817694, "bg": 5.466209827784208e-07}, {"x": 0.9069767441860465, "y": 0.9701492537313432, "ox": 0.9069767441860465, "oy": 0.9701492537313432, "term": "work", "cat25k": 386, "ncat25k": 172, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 453, "ncat": 167, "s": 0.655982905982906, "os": 0.06267806267806264, "bg": 2.9556221650563056e-06}, {"x": 0.4961240310077519, "y": 0.9477611940298508, "ox": 0.4961240310077519, "oy": 0.9477611940298508, "term": "analysis", "cat25k": 282, "ncat25k": 71, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 330, "ncat": 69, "s": 0.9679487179487181, "os": 0.4481481481481482, "bg": 6.3837554766302235e-06}, {"x": 0.062015503875968984, "y": 0.2014925373134328, "ox": 0.062015503875968984, "oy": 0.2014925373134328, "term": "fields", "cat25k": 23, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 8, "s": 0.7574786324786326, "os": 0.13817663817663817, "bg": 1.7176726703334397e-06}, {"x": 0.6744186046511627, "y": 0.5671641791044776, "ox": 0.6744186046511627, "oy": 0.5671641791044776, "term": "proficiency", "cat25k": 67, "ncat25k": 99, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 79, "ncat": 96, "s": 0.1794871794871795, "os": -0.10655270655270654, "bg": 9.91880747594689e-05}, {"x": 0.0697674418604651, "y": 0.007462686567164179, "ox": 0.0697674418604651, "oy": 0.007462686567164179, "term": "analysing", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.27350427350427353, "os": -0.06210826210826211, "bg": 1.2678802816722834e-05}, {"x": 0.0, "y": 0.18656716417910446, "ox": 0.0, "oy": 0.18656716417910446, "term": "matlab", "cat25k": 21, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 0, "s": 0.8151709401709403, "os": 0.1849002849002849, "bg": 2.1715715228796776e-05}, {"x": 0.7906976744186046, "y": 0.835820895522388, "ox": 0.7906976744186046, "oy": 0.835820895522388, "term": "strong", "cat25k": 180, "ncat25k": 122, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 211, "ncat": 118, "s": 0.6463675213675214, "os": 0.044729344729344755, "bg": 1.0782574867407513e-05}, {"x": 0.062015503875968984, "y": 0.22388059701492533, "ox": 0.062015503875968984, "oy": 0.22388059701492533, "term": "help", "cat25k": 26, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 8, "s": 0.7852564102564104, "os": 0.1603988603988604, "bg": 1.243640820618948e-07}, {"x": 0.03875968992248062, "y": 0.01492537313432836, "ox": 0.03875968992248062, "oy": 0.01492537313432836, "term": "biological", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47970085470085466, "os": -0.023931623931623933, "bg": 6.96574386482158e-07}, {"x": 0.0697674418604651, "y": 0.17164179104477612, "ox": 0.0697674418604651, "oy": 0.17164179104477612, "term": "questions", "cat25k": 20, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 9, "s": 0.6965811965811967, "os": 0.10085470085470086, "bg": 4.0827107959561735e-07}, {"x": 0.5813953488372092, "y": 0.4029850746268656, "ox": 0.5813953488372092, "oy": 0.4029850746268656, "term": "analytical", "cat25k": 46, "ncat25k": 87, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 84, "s": 0.09722222222222222, "os": -0.17720797720797726, "bg": 3.31689507374984e-05}, {"x": 0.007751937984496123, "y": 0.34328358208955223, "ox": 0.007751937984496123, "oy": 0.34328358208955223, "term": "linear", "cat25k": 39, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 1, "s": 0.9380341880341881, "os": 0.33276353276353277, "bg": 4.087008408976322e-06}, {"x": 0.10077519379844961, "y": 0.5746268656716417, "ox": 0.10077519379844961, "oy": 0.5746268656716417, "term": "non", "cat25k": 68, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 80, "ncat": 13, "s": 0.9754273504273505, "os": 0.47008547008547, "bg": 9.481838478930813e-07}, {"x": 0.023255813953488372, "y": 0.4850746268656716, "ox": 0.023255813953488372, "oy": 0.4850746268656716, "term": "regression", "cat25k": 56, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 66, "ncat": 3, "s": 0.9722222222222223, "os": 0.4581196581196581, "bg": 2.166671010475697e-05}, {"x": 0.4108527131782945, "y": 0.6716417910447761, "ox": 0.4108527131782945, "oy": 0.6716417910447761, "term": "models", "cat25k": 88, "ncat25k": 59, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 103, "ncat": 57, "s": 0.8803418803418804, "os": 0.25868945868945864, "bg": 3.693947449788144e-06}, {"x": 0.01550387596899225, "y": 0.38805970149253727, "ox": 0.01550387596899225, "oy": 0.38805970149253727, "term": "clustering", "cat25k": 44, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 52, "ncat": 2, "s": 0.951923076923077, "os": 0.3695156695156695, "bg": 3.9550530196829805e-05}, {"x": 0.03875968992248062, "y": 0.14925373134328357, "ox": 0.03875968992248062, "oy": 0.14925373134328357, "term": "ai", "cat25k": 17, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 5, "s": 0.716880341880342, "os": 0.1094017094017094, "bg": 4.0638129163579525e-06}, {"x": 0.38759689922480617, "y": 0.9552238805970149, "ox": 0.38759689922480617, "oy": 0.9552238805970149, "term": "machine", "cat25k": 286, "ncat25k": 54, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 335, "ncat": 52, "s": 0.9903846153846154, "os": 0.5632478632478632, "bg": 1.1101358935419263e-05}, {"x": 0.5116279069767441, "y": 0.9626865671641791, "ox": 0.5116279069767441, "oy": 0.9626865671641791, "term": "learning", "cat25k": 357, "ncat25k": 73, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 419, "ncat": 71, "s": 0.9668803418803419, "os": 0.4475783475783476, "bg": 8.324259890053177e-06}, {"x": 0.062015503875968984, "y": 0.44776119402985065, "ox": 0.062015503875968984, "oy": 0.44776119402985065, "term": "methods", "cat25k": 51, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 60, "ncat": 8, "s": 0.9561965811965812, "os": 0.38262108262108263, "bg": 1.5462873441895213e-06}, {"x": 0.22480620155038755, "y": 0.20895522388059698, "ox": 0.22480620155038755, "oy": 0.20895522388059698, "term": "develop", "cat25k": 24, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 29, "s": 0.5566239316239318, "os": -0.015954415954415962, "bg": 2.047930851507013e-06}, {"x": 0.03875968992248062, "y": 0.26865671641791045, "ox": 0.03875968992248062, "oy": 0.26865671641791045, "term": "apply", "cat25k": 31, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 5, "s": 0.8482905982905983, "os": 0.22792022792022792, "bg": 1.073349455565741e-06}, {"x": 0.023255813953488372, "y": 0.42537313432835816, "ox": 0.023255813953488372, "oy": 0.42537313432835816, "term": "predictive", "cat25k": 49, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 57, "ncat": 3, "s": 0.9594017094017094, "os": 0.39886039886039887, "bg": 4.63351591960232e-05}, {"x": 0.0697674418604651, "y": 0.6194029850746269, "ox": 0.0697674418604651, "oy": 0.6194029850746269, "term": "algorithms", "cat25k": 78, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 92, "ncat": 9, "s": 0.9893162393162394, "os": 0.5452991452991454, "bg": 1.9880443735440898e-05}, {"x": 0.17829457364341084, "y": 0.28358208955223874, "ox": 0.17829457364341084, "oy": 0.28358208955223874, "term": "identify", "cat25k": 32, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 23, "s": 0.7072649572649574, "os": 0.10427350427350424, "bg": 3.397474211847945e-06}, {"x": 0.0, "y": 0.11194029850746266, "ox": 0.0, "oy": 0.11194029850746266, "term": "novel", "cat25k": 13, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.7190170940170941, "os": 0.11082621082621083, "bg": 1.3713625579532101e-06}, {"x": 0.007751937984496123, "y": 0.11940298507462686, "ox": 0.007751937984496123, "oy": 0.11940298507462686, "term": "generate", "cat25k": 14, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 1, "s": 0.717948717948718, "os": 0.11054131054131053, "bg": 1.868916607731664e-06}, {"x": 0.03875968992248062, "y": 0.029850746268656716, "ox": 0.03875968992248062, "oy": 0.029850746268656716, "term": "hypotheses", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.5726495726495727, "os": -0.009116809116809121, "bg": 9.125887048895995e-06}, {"x": 0.0, "y": 0.14925373134328357, "ox": 0.0, "oy": 0.14925373134328357, "term": "causal", "cat25k": 17, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 0, "s": 0.7670940170940171, "os": 0.14786324786324787, "bg": 1.9322411340323215e-05}, {"x": 0.007751937984496123, "y": 0.14925373134328357, "ox": 0.007751937984496123, "oy": 0.14925373134328357, "term": "inference", "cat25k": 17, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 1, "s": 0.7596153846153847, "os": 0.14017094017094017, "bg": 1.500897858540377e-05}, {"x": 0.9147286821705426, "y": 0.4402985074626865, "ox": 0.9147286821705426, "oy": 0.4402985074626865, "term": "excellent", "cat25k": 50, "ncat25k": 177, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 171, "s": 0.02564102564102564, "os": -0.47094017094017093, "bg": 7.619501192335987e-06}, {"x": 0.6589147286821705, "y": 0.34328358208955223, "ox": 0.6589147286821705, "oy": 0.34328358208955223, "term": "written", "cat25k": 39, "ncat25k": 97, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 94, "s": 0.0673076923076923, "os": -0.31339031339031337, "bg": 2.8538859617135504e-06}, {"x": 0.03875968992248062, "y": 0.12686567164179102, "ox": 0.03875968992248062, "oy": 0.12686567164179102, "term": "oral", "cat25k": 15, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 5, "s": 0.6773504273504274, "os": 0.08717948717948718, "bg": 1.0475700612476121e-06}, {"x": 0.7364341085271318, "y": 0.7313432835820894, "ox": 0.7364341085271318, "oy": 0.7313432835820894, "term": "communication", "cat25k": 106, "ncat25k": 110, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 124, "ncat": 107, "s": 0.5982905982905984, "os": -0.005128205128205221, "bg": 7.170168300611448e-06}, {"x": 0.6356589147286821, "y": 0.7686567164179104, "ox": 0.6356589147286821, "oy": 0.7686567164179104, "term": "team", "cat25k": 116, "ncat25k": 94, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 136, "ncat": 91, "s": 0.7478632478632479, "os": 0.13190883190883196, "bg": 2.6434862655545807e-06}, {"x": 0.33333333333333326, "y": 0.7164179104477612, "ox": 0.33333333333333326, "oy": 0.7164179104477612, "term": "bachelor", "cat25k": 99, "ncat25k": 45, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 116, "ncat": 44, "s": 0.9540598290598291, "os": 0.3800569800569801, "bg": 2.5826521319309102e-05}, {"x": 0.7674418604651162, "y": 0.8731343283582089, "ox": 0.7674418604651162, "oy": 0.8731343283582089, "term": "degree", "cat25k": 199, "ncat25k": 117, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 233, "ncat": 113, "s": 0.7083333333333334, "os": 0.10484330484330484, "bg": 1.0315289968241874e-05}, {"x": 0.1705426356589147, "y": 0.7985074626865671, "ox": 0.1705426356589147, "oy": 0.7985074626865671, "term": "mathematics", "cat25k": 137, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 161, "ncat": 22, "s": 0.9925213675213675, "os": 0.6230769230769231, "bg": 1.4719838638394014e-05}, {"x": 0.8062015503875969, "y": 0.8507462686567163, "ox": 0.8062015503875969, "oy": 0.8507462686567163, "term": "engineering", "cat25k": 185, "ncat25k": 131, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 217, "ncat": 127, "s": 0.6452991452991454, "os": 0.04415954415954415, "bg": 7.99815754070885e-06}, {"x": 0.08527131782945736, "y": 0.5298507462686567, "ox": 0.08527131782945736, "oy": 0.5298507462686567, "term": "operations", "cat25k": 62, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 73, "ncat": 11, "s": 0.9647435897435899, "os": 0.441025641025641, "bg": 2.7010204310490976e-06}, {"x": 0.11627906976744183, "y": 0.7686567164179104, "ox": 0.11627906976744183, "oy": 0.7686567164179104, "term": "research", "cat25k": 116, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 136, "ncat": 15, "s": 0.9946581196581198, "os": 0.6472934472934473, "bg": 9.692118557254853e-07}, {"x": 0.18604651162790695, "y": 0.417910447761194, "ox": 0.18604651162790695, "oy": 0.417910447761194, "term": "minimum", "cat25k": 48, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 56, "ncat": 24, "s": 0.8514957264957265, "os": 0.2299145299145299, "bg": 3.196382909172322e-06}, {"x": 0.15503875968992245, "y": 0.6865671641791045, "ox": 0.15503875968992245, "oy": 0.6865671641791045, "term": "master", "cat25k": 93, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 109, "ncat": 20, "s": 0.9871794871794872, "os": 0.5273504273504274, "bg": 4.177510179645082e-06}, {"x": 0.1705426356589147, "y": 0.30597014925373134, "ox": 0.1705426356589147, "oy": 0.30597014925373134, "term": "proven", "cat25k": 35, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 22, "s": 0.7510683760683761, "os": 0.13418803418803418, "bg": 9.710312104844241e-06}, {"x": 0.09302325581395349, "y": 0.29850746268656714, "ox": 0.09302325581395349, "oy": 0.29850746268656714, "term": "track", "cat25k": 34, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 12, "s": 0.8333333333333334, "os": 0.2037037037037037, "bg": 9.249365600243272e-07}, {"x": 0.01550387596899225, "y": 0.14179104477611937, "ox": 0.01550387596899225, "oy": 0.14179104477611937, "term": "driving", "cat25k": 16, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 2, "s": 0.7361111111111112, "os": 0.12507122507122506, "bg": 1.0476760188630577e-06}, {"x": 0.2015503875968992, "y": 0.05223880597014925, "ox": 0.2015503875968992, "oy": 0.05223880597014925, "term": "value", "cat25k": 6, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 26, "s": 0.11645299145299146, "os": -0.14843304843304844, "bg": 3.624061216456012e-07}, {"x": 0.03875968992248062, "y": 0.0373134328358209, "ox": 0.03875968992248062, "oy": 0.0373134328358209, "term": "commercial", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 5, "s": 0.6164529914529915, "os": -0.0017094017094017103, "bg": 2.2072669828804706e-07}, {"x": 0.10077519379844961, "y": 0.23134328358208953, "ox": 0.10077519379844961, "oy": 0.23134328358208953, "term": "setting", "cat25k": 26, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 13, "s": 0.7435897435897436, "os": 0.12934472934472935, "bg": 1.8964618874304188e-06}, {"x": 0.8914728682170541, "y": 0.5447761194029851, "ox": 0.8914728682170541, "oy": 0.5447761194029851, "term": "using", "cat25k": 64, "ncat25k": 166, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 75, "ncat": 161, "s": 0.0641025641025641, "os": -0.3441595441595442, "bg": 1.7513656988311843e-06}, {"x": 0.18604651162790695, "y": 0.25373134328358204, "ox": 0.18604651162790695, "oy": 0.25373134328358204, "term": "various", "cat25k": 29, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 24, "s": 0.6591880341880342, "os": 0.06695156695156693, "bg": 1.2717757890858963e-06}, {"x": 0.31782945736434104, "y": 0.8059701492537312, "ox": 0.31782945736434104, "oy": 0.8059701492537312, "term": "modeling", "cat25k": 143, "ncat25k": 42, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 168, "ncat": 41, "s": 0.9764957264957266, "os": 0.48433048433048437, "bg": 2.691638893847171e-05}, {"x": 0.8992248062015502, "y": 0.46268656716417905, "ox": 0.8992248062015502, "oy": 0.46268656716417905, "term": "g", "cat25k": 53, "ncat25k": 167, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 62, "ncat": 162, "s": 0.04594017094017094, "os": -0.43333333333333335, "bg": 1.9664067929873812e-06}, {"x": 0.0, "y": 0.35074626865671643, "ox": 0.0, "oy": 0.35074626865671643, "term": "trees", "cat25k": 40, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 0, "s": 0.9465811965811965, "os": 0.34786324786324785, "bg": 3.426528592631388e-06}, {"x": 0.0, "y": 0.29104477611940294, "ox": 0.0, "oy": 0.29104477611940294, "term": "neural", "cat25k": 33, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 0, "s": 0.9145299145299146, "os": 0.2886039886039886, "bg": 1.4586088873039501e-05}, {"x": 0.007751937984496123, "y": 0.18656716417910446, "ox": 0.007751937984496123, "oy": 0.18656716417910446, "term": "networks", "cat25k": 21, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 1, "s": 0.8119658119658121, "os": 0.1772079772079772, "bg": 8.506714488798071e-07}, {"x": 0.031007751937984492, "y": 0.13432835820895522, "ox": 0.031007751937984492, "oy": 0.13432835820895522, "term": "series", "cat25k": 15, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 4, "s": 0.6997863247863249, "os": 0.10227920227920229, "bg": 2.7232200581892714e-07}, {"x": 0.05426356589147286, "y": 0.23134328358208953, "ox": 0.05426356589147286, "oy": 0.23134328358208953, "term": "pandas", "cat25k": 26, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 7, "s": 0.8087606837606839, "os": 0.1754985754985755, "bg": 0.00016646296856040197}, {"x": 0.023255813953488372, "y": 0.14179104477611937, "ox": 0.023255813953488372, "oy": 0.14179104477611937, "term": "scikit", "cat25k": 16, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 3, "s": 0.7254273504273505, "os": 0.11737891737891737, "bg": 0.0008018807749084215}, {"x": 0.12403100775193797, "y": 0.23134328358208953, "ox": 0.12403100775193797, "oy": 0.23134328358208953, "term": "learn", "cat25k": 26, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 16, "s": 0.7094017094017094, "os": 0.10626780626780627, "bg": 5.237259499890078e-07}, {"x": 0.9612403100775194, "y": 0.42537313432835816, "ox": 0.9612403100775194, "oy": 0.42537313432835816, "term": "spark", "cat25k": 49, "ncat25k": 253, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 57, "ncat": 245, "s": 0.01388888888888889, "os": -0.5319088319088319, "bg": 0.00012332636525757448}, {"x": 0.1472868217054263, "y": 0.9029850746268656, "ox": 0.1472868217054263, "oy": 0.9029850746268656, "term": "statistical", "cat25k": 207, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 243, "ncat": 19, "s": 1.0, "os": 0.7498575498575498, "bg": 2.8537031645498618e-05}, {"x": 0.4961240310077519, "y": 0.38805970149253727, "ox": 0.4961240310077519, "oy": 0.38805970149253727, "term": "databases", "cat25k": 44, "ncat25k": 71, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 52, "ncat": 69, "s": 0.17841880341880345, "os": -0.1074074074074074, "bg": 1.116608382323757e-05}, {"x": 0.8604651162790696, "y": 0.5820895522388059, "ox": 0.8604651162790696, "oy": 0.5820895522388059, "term": "hadoop", "cat25k": 69, "ncat25k": 148, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 81, "ncat": 143, "s": 0.07158119658119659, "os": -0.27635327635327644, "bg": 0.008134657636228277}, {"x": 0.8062015503875969, "y": 0.2014925373134328, "ox": 0.8062015503875969, "oy": 0.2014925373134328, "term": "distributed", "cat25k": 23, "ncat25k": 131, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 127, "s": 0.007478632478632479, "os": -0.6002849002849003, "bg": 1.0941421193987563e-05}, {"x": 0.24806201550387594, "y": 0.2761194029850746, "ox": 0.24806201550387594, "oy": 0.2761194029850746, "term": "computing", "cat25k": 32, "ncat25k": 33, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 32, "s": 0.6356837606837606, "os": 0.027635327635327667, "bg": 3.7874630526804e-06}, {"x": 0.2015503875968992, "y": 0.3582089552238806, "ox": 0.2015503875968992, "oy": 0.3582089552238806, "term": "frameworks", "cat25k": 41, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 26, "s": 0.7735042735042735, "os": 0.15527065527065528, "bg": 4.6938455122238206e-05}, {"x": 0.5038759689922481, "y": 0.6791044776119403, "ox": 0.5038759689922481, "oy": 0.6791044776119403, "term": "software", "cat25k": 91, "ncat25k": 72, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 107, "ncat": 70, "s": 0.8066239316239316, "os": 0.1737891737891738, "bg": 9.55279780615087e-07}, {"x": 0.565891472868217, "y": 0.7985074626865671, "ox": 0.565891472868217, "oy": 0.7985074626865671, "term": "environment", "cat25k": 137, "ncat25k": 84, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 161, "ncat": 81, "s": 0.8525641025641026, "os": 0.23076923076923084, "bg": 4.744428935193796e-06}, {"x": 0.3565891472868217, "y": 0.09701492537313433, "ox": 0.3565891472868217, "oy": 0.09701492537313433, "term": "agile", "cat25k": 11, "ncat25k": 50, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 48, "s": 0.07478632478632478, "os": -0.25783475783475784, "bg": 6.545538437601435e-05}, {"x": 0.8449612403100775, "y": 0.4029850746268656, "ox": 0.8449612403100775, "oy": 0.4029850746268656, "term": "management", "cat25k": 46, "ncat25k": 141, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 137, "s": 0.041666666666666664, "os": -0.43874643874643876, "bg": 1.255520522917593e-06}, {"x": 0.15503875968992245, "y": 0.05970149253731343, "ox": 0.15503875968992245, "oy": 0.05970149253731343, "term": "git", "cat25k": 7, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 20, "s": 0.19444444444444445, "os": -0.09487179487179488, "bg": 3.602514040155166e-05}, {"x": 0.19379844961240308, "y": 0.17164179104477612, "ox": 0.19379844961240308, "oy": 0.17164179104477612, "term": "understand", "cat25k": 20, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 25, "s": 0.4967948717948718, "os": -0.022222222222222227, "bg": 1.5540300807775124e-06}, {"x": 0.6899224806201549, "y": 0.5149253731343283, "ox": 0.6899224806201549, "oy": 0.5149253731343283, "term": "complex", "cat25k": 61, "ncat25k": 101, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 71, "ncat": 98, "s": 0.09829059829059829, "os": -0.1737891737891738, "bg": 7.203597996862684e-06}, {"x": 0.01550387596899225, "y": 0.23134328358208953, "ox": 0.01550387596899225, "oy": 0.23134328358208953, "term": "needs", "cat25k": 26, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 2, "s": 0.8408119658119659, "os": 0.21396011396011397, "bg": 5.337618179112319e-07}, {"x": 0.08527131782945736, "y": 0.32089552238805963, "ox": 0.08527131782945736, "oy": 0.32089552238805963, "term": "applying", "cat25k": 37, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 11, "s": 0.8557692307692308, "os": 0.23361823361823364, "bg": 7.4506384852244875e-06}, {"x": 0.023255813953488372, "y": 0.2014925373134328, "ox": 0.023255813953488372, "oy": 0.2014925373134328, "term": "right", "cat25k": 23, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 3, "s": 0.8098290598290598, "os": 0.17663817663817663, "bg": 2.1923795757962565e-07}, {"x": 0.10077519379844961, "y": 0.20895522388059698, "ox": 0.10077519379844961, "oy": 0.20895522388059698, "term": "approaches", "cat25k": 24, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 13, "s": 0.7104700854700856, "os": 0.10712250712250712, "bg": 4.788590727092608e-06}, {"x": 0.27131782945736427, "y": 0.6044776119402985, "ox": 0.27131782945736427, "oy": 0.6044776119402985, "term": "must", "cat25k": 74, "ncat25k": 36, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 87, "ncat": 35, "s": 0.935897435897436, "os": 0.33048433048433046, "bg": 8.332488502096442e-07}, {"x": 0.4806201550387596, "y": 0.0373134328358209, "ox": 0.4806201550387596, "oy": 0.0373134328358209, "term": "curious", "cat25k": 4, "ncat25k": 68, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 66, "s": 0.038461538461538464, "os": -0.44017094017094016, "bg": 1.667643764985306e-05}, {"x": 0.8527131782945736, "y": 0.14925373134328357, "ox": 0.8527131782945736, "oy": 0.14925373134328357, "term": "self", "cat25k": 17, "ncat25k": 143, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 139, "s": 0.0021367521367521374, "os": -0.6982905982905983, "bg": 2.815225988636544e-06}, {"x": 0.16279069767441856, "y": 0.33582089552238803, "ox": 0.16279069767441856, "oy": 0.33582089552238803, "term": "driven", "cat25k": 38, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 21, "s": 0.8044871794871795, "os": 0.1715099715099715, "bg": 7.424647839672155e-06}, {"x": 0.07751937984496123, "y": 0.19402985074626863, "ox": 0.07751937984496123, "oy": 0.19402985074626863, "term": "passion", "cat25k": 22, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 10, "s": 0.7232905982905984, "os": 0.11538461538461539, "bg": 5.362947130130571e-06}, {"x": 0.6434108527131782, "y": 0.6194029850746269, "ox": 0.6434108527131782, "oy": 0.6194029850746269, "term": "problem", "cat25k": 78, "ncat25k": 95, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 92, "ncat": 92, "s": 0.4786324786324786, "os": -0.023931623931623958, "bg": 2.6086884338984965e-06}, {"x": 0.5736434108527131, "y": 0.5373134328358209, "ox": 0.5736434108527131, "oy": 0.5373134328358209, "term": "solving", "cat25k": 63, "ncat25k": 86, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 74, "ncat": 83, "s": 0.405982905982906, "os": -0.03618233618233613, "bg": 3.1429270079224786e-05}, {"x": 0.565891472868217, "y": 0.20895522388059698, "ox": 0.565891472868217, "oy": 0.20895522388059698, "term": "verbal", "cat25k": 24, "ncat25k": 84, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 81, "s": 0.061965811965811975, "os": -0.3544159544159544, "bg": 4.102001343875853e-05}, {"x": 0.2790697674418604, "y": 0.29850746268656714, "ox": 0.2790697674418604, "oy": 0.29850746268656714, "term": "developing", "cat25k": 34, "ncat25k": 37, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 36, "s": 0.6314102564102565, "os": 0.019088319088319095, "bg": 3.549395299797339e-06}, {"x": 0.19379844961240308, "y": 0.1791044776119403, "ox": 0.19379844961240308, "oy": 0.1791044776119403, "term": "testing", "cat25k": 20, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 25, "s": 0.5576923076923077, "os": -0.014814814814814836, "bg": 1.7308706952517252e-06}, {"x": 0.21705426356589144, "y": 0.3731343283582089, "ox": 0.21705426356589144, "oy": 0.3731343283582089, "term": "projects", "cat25k": 43, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 50, "ncat": 28, "s": 0.7702991452991453, "os": 0.15470085470085468, "bg": 1.6836033748606696e-06}, {"x": 0.5193798449612402, "y": 0.8582089552238805, "ox": 0.5193798449612402, "oy": 0.8582089552238805, "term": "analytics", "cat25k": 192, "ncat25k": 75, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 225, "ncat": 73, "s": 0.9391025641025642, "os": 0.33618233618233617, "bg": 0.00015238940709526102}, {"x": 0.10852713178294572, "y": 0.6119402985074627, "ox": 0.10852713178294572, "oy": 0.6119402985074627, "term": "deep", "cat25k": 77, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 90, "ncat": 14, "s": 0.982905982905983, "os": 0.4994301994301994, "bg": 3.887678755422447e-06}, {"x": 0.01550387596899225, "y": 0.29104477611940294, "ox": 0.01550387596899225, "oy": 0.29104477611940294, "term": "geospatial", "cat25k": 33, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 2, "s": 0.8985042735042735, "os": 0.2732193732193732, "bg": 8.379009151512922e-05}, {"x": 0.6124031007751937, "y": 0.2462686567164179, "ox": 0.6124031007751937, "oy": 0.2462686567164179, "term": "applications", "cat25k": 28, "ncat25k": 91, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 88, "s": 0.05982905982905983, "os": -0.3635327635327636, "bg": 2.435118383783351e-06}, {"x": 0.062015503875968984, "y": 0.007462686567164179, "ox": 0.062015503875968984, "oy": 0.007462686567164179, "term": "images", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3162393162393162, "os": -0.05441595441595442, "bg": 1.5811258269419835e-07}, {"x": 0.09302325581395349, "y": 0.0373134328358209, "ox": 0.09302325581395349, "oy": 0.0373134328358209, "term": "video", "cat25k": 4, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 12, "s": 0.3098290598290598, "os": -0.05555555555555556, "bg": 9.303219428609231e-08}, {"x": 0.1317829457364341, "y": 0.0373134328358209, "ox": 0.1317829457364341, "oy": 0.0373134328358209, "term": "strategies", "cat25k": 4, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 17, "s": 0.19764957264957267, "os": -0.09401709401709402, "bg": 1.191903636974758e-06}, {"x": 0.9534883720930232, "y": 0.8208955223880596, "ox": 0.9534883720930232, "oy": 0.8208955223880596, "term": "working", "cat25k": 165, "ncat25k": 225, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 193, "ncat": 218, "s": 0.13782051282051283, "os": -0.1316239316239317, "bg": 5.564159620603918e-06}, {"x": 0.7131782945736433, "y": 0.6492537313432835, "ox": 0.7131782945736433, "oy": 0.6492537313432835, "term": "cloud", "cat25k": 83, "ncat25k": 106, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 97, "ncat": 103, "s": 0.26068376068376065, "os": -0.06353276353276349, "bg": 3.511153045982324e-05}, {"x": 0.16279069767441856, "y": 0.05223880597014925, "ox": 0.16279069767441856, "oy": 0.05223880597014925, "term": "amazon", "cat25k": 6, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 21, "s": 0.1634615384615385, "os": -0.10997150997150998, "bg": 9.538000868741558e-07}, {"x": 0.2403100775193798, "y": 0.1791044776119403, "ox": 0.2403100775193798, "oy": 0.1791044776119403, "term": "web", "cat25k": 20, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 31, "s": 0.3012820512820513, "os": -0.06096866096866099, "bg": 1.7752630613450592e-07}, {"x": 0.5426356589147285, "y": 0.3582089552238806, "ox": 0.5426356589147285, "oy": 0.3582089552238806, "term": "services", "cat25k": 41, "ncat25k": 78, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 76, "s": 0.09508547008547008, "os": -0.1831908831908831, "bg": 4.410756877752335e-07}, {"x": 0.5968992248062014, "y": 0.5223880597014925, "ox": 0.5968992248062014, "oy": 0.5223880597014925, "term": "big", "cat25k": 61, "ncat25k": 89, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 72, "ncat": 86, "s": 0.233974358974359, "os": -0.07407407407407407, "bg": 1.4579307193444472e-06}, {"x": 0.0697674418604651, "y": 0.28358208955223874, "ox": 0.0697674418604651, "oy": 0.28358208955223874, "term": "variety", "cat25k": 32, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 9, "s": 0.8376068376068377, "os": 0.21196581196581193, "bg": 1.883786901456522e-06}, {"x": 0.33333333333333326, "y": 0.7910447761194029, "ox": 0.33333333333333326, "oy": 0.7910447761194029, "term": "techniques", "cat25k": 134, "ncat25k": 45, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 157, "ncat": 44, "s": 0.9700854700854701, "os": 0.45413105413105415, "bg": 9.966950138794742e-06}, {"x": 0.48837209302325574, "y": 0.5820895522388059, "ox": 0.48837209302325574, "oy": 0.5820895522388059, "term": "understanding", "cat25k": 69, "ncat25k": 70, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 81, "ncat": 68, "s": 0.6837606837606839, "os": 0.0928774928774928, "bg": 6.0267323194886485e-06}, {"x": 0.007751937984496123, "y": 0.14179104477611937, "ox": 0.007751937984496123, "oy": 0.14179104477611937, "term": "applicable", "cat25k": 16, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 1, "s": 0.7500000000000001, "os": 0.13276353276353275, "bg": 1.1686277896533085e-06}, {"x": 0.07751937984496123, "y": 0.06716417910447761, "ox": 0.07751937984496123, "oy": 0.06716417910447761, "term": "coding", "cat25k": 8, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 10, "s": 0.5608974358974359, "os": -0.01054131054131055, "bg": 3.4193383922144907e-06}, {"x": 0.8604651162790696, "y": 0.3805970149253731, "ox": 0.8604651162790696, "oy": 0.3805970149253731, "term": "java", "cat25k": 44, "ncat25k": 148, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 51, "ncat": 143, "s": 0.023504273504273504, "os": -0.4763532763532764, "bg": 7.001690078056573e-06}, {"x": 0.8139534883720929, "y": 0.26119402985074625, "ox": 0.8139534883720929, "oy": 0.26119402985074625, "term": "scala", "cat25k": 30, "ncat25k": 133, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 129, "s": 0.011752136752136754, "os": -0.5487179487179488, "bg": 0.000285649840323481}, {"x": 0.062015503875968984, "y": 0.753731343283582, "ox": 0.062015503875968984, "oy": 0.753731343283582, "term": "mining", "cat25k": 109, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 128, "ncat": 8, "s": 0.9967948717948718, "os": 0.6863247863247863, "bg": 1.5952286710449045e-05}, {"x": 0.03875968992248062, "y": 0.11940298507462686, "ox": 0.03875968992248062, "oy": 0.11940298507462686, "term": "libraries", "cat25k": 14, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 5, "s": 0.6698717948717949, "os": 0.07977207977207976, "bg": 1.4915468711856797e-06}, {"x": 0.7209302325581395, "y": 0.29104477611940294, "ox": 0.7209302325581395, "oy": 0.29104477611940294, "term": "high", "cat25k": 33, "ncat25k": 107, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 104, "s": 0.050213675213675216, "os": -0.4267806267806268, "bg": 8.278621367204536e-07}, {"x": 0.3410852713178294, "y": 0.28358208955223874, "ox": 0.3410852713178294, "oy": 0.28358208955223874, "term": "performance", "cat25k": 32, "ncat25k": 46, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 45, "s": 0.3044871794871795, "os": -0.057264957264957284, "bg": 1.1877034174634703e-06}, {"x": 0.8682170542635658, "y": 0.7835820895522387, "ox": 0.8682170542635658, "oy": 0.7835820895522387, "term": "systems", "cat25k": 129, "ncat25k": 150, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 151, "ncat": 145, "s": 0.21474358974358976, "os": -0.08404558404558404, "bg": 2.6474540212814157e-06}, {"x": 0.01550387596899225, "y": 0.17164179104477612, "ox": 0.01550387596899225, "oy": 0.17164179104477612, "term": "mathematical", "cat25k": 20, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 2, "s": 0.7713675213675214, "os": 0.1547008547008547, "bg": 4.53447205413942e-06}, {"x": 0.10852713178294572, "y": 0.5671641791044776, "ox": 0.10852713178294572, "oy": 0.5671641791044776, "term": "background", "cat25k": 67, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 79, "ncat": 14, "s": 0.9711538461538463, "os": 0.454985754985755, "bg": 3.55850943299629e-06}, {"x": 0.007751937984496123, "y": 0.2761194029850746, "ox": 0.007751937984496123, "oy": 0.2761194029850746, "term": "thorough", "cat25k": 32, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 1, "s": 0.8910256410256411, "os": 0.2660968660968661, "bg": 1.4486695399883307e-05}, {"x": 0.046511627906976744, "y": 0.04477611940298507, "ox": 0.046511627906976744, "oy": 0.04477611940298507, "term": "another", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 6, "s": 0.6121794871794872, "os": -0.0019943019943019988, "bg": 1.246166668010623e-07}, {"x": 0.11627906976744183, "y": 0.11194029850746266, "ox": 0.11627906976744183, "oy": 0.11194029850746266, "term": "scalable", "cat25k": 13, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 15, "s": 0.6004273504273504, "os": -0.004558404558404561, "bg": 1.9114720819945067e-05}, {"x": 0.6744186046511627, "y": 0.7313432835820894, "ox": 0.6744186046511627, "oy": 0.7313432835820894, "term": "solutions", "cat25k": 106, "ncat25k": 99, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 124, "ncat": 96, "s": 0.6527777777777778, "os": 0.05641025641025632, "bg": 4.14473454477707e-06}, {"x": 0.17829457364341084, "y": 0.32835820895522383, "ox": 0.17829457364341084, "oy": 0.32835820895522383, "term": "production", "cat25k": 38, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 23, "s": 0.7681623931623932, "os": 0.1487179487179487, "bg": 1.4779250660006594e-06}, {"x": 0.1395348837209302, "y": 0.22388059701492533, "ox": 0.1395348837209302, "oy": 0.22388059701492533, "term": "environments", "cat25k": 26, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 18, "s": 0.6720085470085471, "os": 0.08347578347578347, "bg": 7.171423312384808e-06}, {"x": 0.10077519379844961, "y": 0.26865671641791045, "ox": 0.10077519379844961, "oy": 0.26865671641791045, "term": "comfortable", "cat25k": 31, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 13, "s": 0.795940170940171, "os": 0.16638176638176638, "bg": 5.006287232664901e-06}, {"x": 0.07751937984496123, "y": 0.022388059701492536, "ox": 0.07751937984496123, "oy": 0.022388059701492536, "term": "ad", "cat25k": 3, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 10, "s": 0.313034188034188, "os": -0.05498575498575499, "bg": 3.298277255241156e-07}, {"x": 0.09302325581395349, "y": 0.029850746268656716, "ox": 0.09302325581395349, "oy": 0.029850746268656716, "term": "tech", "cat25k": 3, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 12, "s": 0.26175213675213677, "os": -0.06296296296296297, "bg": 3.424051655928092e-07}, {"x": 0.10852713178294572, "y": 0.10447761194029849, "ox": 0.10852713178294572, "oy": 0.10447761194029849, "term": "search", "cat25k": 12, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 14, "s": 0.6014957264957266, "os": -0.004273504273504286, "bg": 5.46795973564348e-08}, {"x": 0.0697674418604651, "y": 0.45522388059701485, "ox": 0.0697674418604651, "oy": 0.45522388059701485, "term": "prior", "cat25k": 52, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 61, "ncat": 9, "s": 0.9551282051282052, "os": 0.38233618233618233, "bg": 2.222081913621391e-06}, {"x": 0.007751937984496123, "y": 0.23880597014925373, "ox": 0.007751937984496123, "oy": 0.23880597014925373, "term": "natural", "cat25k": 27, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 1, "s": 0.8504273504273505, "os": 0.22905982905982905, "bg": 6.276350389356725e-07}, {"x": 0.5891472868217054, "y": 0.5298507462686567, "ox": 0.5891472868217054, "oy": 0.5298507462686567, "term": "language", "cat25k": 62, "ncat25k": 88, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 73, "ncat": 85, "s": 0.30235042735042733, "os": -0.05897435897435899, "bg": 2.280386527537013e-06}, {"x": 0.8372093023255813, "y": 0.417910447761194, "ox": 0.8372093023255813, "oy": 0.417910447761194, "term": "processing", "cat25k": 48, "ncat25k": 140, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 56, "ncat": 136, "s": 0.053418803418803416, "os": -0.41623931623931626, "bg": 6.913698267014913e-06}, {"x": 0.2403100775193798, "y": 0.4402985074626865, "ox": 0.2403100775193798, "oy": 0.4402985074626865, "term": "expertise", "cat25k": 50, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 31, "s": 0.826923076923077, "os": 0.19829059829059825, "bg": 1.009195282819409e-05}, {"x": 0.2015503875968992, "y": 0.2462686567164179, "ox": 0.2015503875968992, "oy": 0.2462686567164179, "term": "discipline", "cat25k": 28, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 26, "s": 0.6431623931623932, "os": 0.04415954415954412, "bg": 1.0008587877309981e-05}, {"x": 0.23255813953488366, "y": 0.5373134328358209, "ox": 0.23255813953488366, "oy": 0.5373134328358209, "term": "similar", "cat25k": 63, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 74, "ncat": 30, "s": 0.9220085470085471, "os": 0.30227920227920224, "bg": 1.7448819801256265e-06}, {"x": 0.7441860465116278, "y": 0.6268656716417911, "ox": 0.7441860465116278, "oy": 0.6268656716417911, "term": "least", "cat25k": 79, "ncat25k": 113, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 93, "ncat": 109, "s": 0.15491452991452992, "os": -0.11652421652421652, "bg": 3.6303234773675256e-06}, {"x": 0.07751937984496123, "y": 0.13432835820895522, "ox": 0.07751937984496123, "oy": 0.13432835820895522, "term": "job", "cat25k": 15, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 10, "s": 0.6506410256410257, "os": 0.05612535612535613, "bg": 3.150283025364852e-07}, {"x": 0.8372093023255813, "y": 0.6716417910447761, "ox": 0.8372093023255813, "oy": 0.6716417910447761, "term": "programming", "cat25k": 88, "ncat25k": 140, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 103, "ncat": 136, "s": 0.10363247863247863, "os": -0.16438746438746443, "bg": 9.430708569185722e-06}, {"x": 0.37984496124031003, "y": 0.7238805970149252, "ox": 0.37984496124031003, "oy": 0.7238805970149252, "term": "languages", "cat25k": 102, "ncat25k": 53, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 119, "ncat": 51, "s": 0.9423076923076924, "os": 0.34131054131054134, "bg": 9.08337086457501e-06}, {"x": 0.9379844961240309, "y": 0.6940298507462687, "ox": 0.9379844961240309, "oy": 0.6940298507462687, "term": "etc", "cat25k": 95, "ncat25k": 204, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 111, "ncat": 198, "s": 0.07799145299145299, "os": -0.24216524216524216, "bg": 1.1981162123483012e-05}, {"x": 0.24806201550387594, "y": 0.34328358208955223, "ox": 0.24806201550387594, "oy": 0.34328358208955223, "term": "concepts", "cat25k": 39, "ncat25k": 33, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 32, "s": 0.6858974358974359, "os": 0.09430199430199432, "bg": 6.8136329439152835e-06}, {"x": 0.2635658914728682, "y": 0.6119402985074627, "ox": 0.2635658914728682, "oy": 0.6119402985074627, "term": "advanced", "cat25k": 77, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 90, "ncat": 34, "s": 0.9444444444444445, "os": 0.3455840455840456, "bg": 1.4295832210370734e-06}, {"x": 0.6434108527131782, "y": 0.19402985074626863, "ox": 0.6434108527131782, "oy": 0.19402985074626863, "term": "aws", "cat25k": 22, "ncat25k": 95, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 92, "s": 0.03739316239316239, "os": -0.4461538461538462, "bg": 0.0002905648901578531}, {"x": 0.1317829457364341, "y": 0.022388059701492536, "ox": 0.1317829457364341, "oy": 0.022388059701492536, "term": "emr", "cat25k": 3, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 17, "s": 0.16666666666666669, "os": -0.10883190883190884, "bg": 5.6409771864780135e-05}, {"x": 0.05426356589147286, "y": 0.04477611940298507, "ox": 0.05426356589147286, "oy": 0.04477611940298507, "term": "bigquery", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 7, "s": 0.5630341880341881, "os": -0.009686609686609692, "bg": 0.00047391637198789693}, {"x": 0.10852713178294572, "y": 0.08955223880597014, "ox": 0.10852713178294572, "oy": 0.08955223880597014, "term": "modern", "cat25k": 10, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 14, "s": 0.49893162393162394, "os": -0.019088319088319095, "bg": 9.109241598879312e-07}, {"x": 0.11627906976744183, "y": 0.5074626865671641, "ox": 0.11627906976744183, "oy": 0.5074626865671641, "term": "visualization", "cat25k": 60, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 70, "ncat": 15, "s": 0.9583333333333334, "os": 0.388034188034188, "bg": 4.077315495477777e-05}, {"x": 0.05426356589147286, "y": 0.13432835820895522, "ox": 0.05426356589147286, "oy": 0.13432835820895522, "term": "deliver", "cat25k": 15, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 7, "s": 0.6666666666666667, "os": 0.07920227920227921, "bg": 2.1870409947712225e-06}, {"x": 0.023255813953488372, "y": 0.35074626865671643, "ox": 0.023255813953488372, "oy": 0.35074626865671643, "term": "actionable", "cat25k": 40, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 3, "s": 0.9316239316239316, "os": 0.3247863247863248, "bg": 0.0001492777940324709}, {"x": 0.05426356589147286, "y": 0.4104477611940298, "ox": 0.05426356589147286, "oy": 0.4104477611940298, "term": "insights", "cat25k": 47, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 7, "s": 0.9487179487179488, "os": 0.35327635327635326, "bg": 1.6241844203776985e-05}, {"x": 0.18604651162790695, "y": 0.2462686567164179, "ox": 0.18604651162790695, "oy": 0.2462686567164179, "term": "year", "cat25k": 28, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 24, "s": 0.6538461538461539, "os": 0.05954415954415951, "bg": 2.526889826045336e-07}, {"x": 0.11627906976744183, "y": 0.20895522388059698, "ox": 0.11627906976744183, "oy": 0.20895522388059698, "term": "open", "cat25k": 24, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 15, "s": 0.6826923076923077, "os": 0.09173789173789174, "bg": 3.5874406090342857e-07}, {"x": 0.34883720930232553, "y": 0.18656716417910446, "ox": 0.34883720930232553, "oy": 0.18656716417910446, "term": "source", "cat25k": 21, "ncat25k": 49, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 47, "s": 0.10576923076923077, "os": -0.16125356125356127, "bg": 7.999164220658344e-07}, {"x": 0.7751937984496123, "y": 0.7686567164179104, "ox": 0.7751937984496123, "oy": 0.7686567164179104, "term": "large", "cat25k": 116, "ncat25k": 118, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 136, "ncat": 114, "s": 0.5961538461538463, "os": -0.0065527065527065664, "bg": 3.0135211510792545e-06}, {"x": 0.7054263565891472, "y": 0.7611940298507462, "ox": 0.7054263565891472, "oy": 0.7611940298507462, "term": "scale", "cat25k": 111, "ncat25k": 103, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 130, "ncat": 100, "s": 0.6495726495726496, "os": 0.05527065527065522, "bg": 9.921789338627542e-06}, {"x": 0.046511627906976744, "y": 0.2462686567164179, "ox": 0.046511627906976744, "oy": 0.2462686567164179, "term": "analyzing", "cat25k": 28, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 6, "s": 0.8258547008547009, "os": 0.19800569800569798, "bg": 1.7709768231354964e-05}, {"x": 0.4341085271317829, "y": 0.46268656716417905, "ox": 0.4341085271317829, "oy": 0.46268656716417905, "term": "required", "cat25k": 53, "ncat25k": 62, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 62, "ncat": 60, "s": 0.6378205128205129, "os": 0.028205128205128216, "bg": 1.5294550827970195e-06}, {"x": 0.6976744186046511, "y": 0.15671641791044774, "ox": 0.6976744186046511, "oy": 0.15671641791044774, "term": "good", "cat25k": 18, "ncat25k": 102, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 99, "s": 0.012820512820512822, "os": -0.537037037037037, "bg": 6.560041124897813e-07}, {"x": 0.10077519379844961, "y": 0.07462686567164178, "ox": 0.10077519379844961, "oy": 0.07462686567164178, "term": "english", "cat25k": 9, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 13, "s": 0.4551282051282051, "os": -0.02621082621082621, "bg": 2.667804651605069e-07}, {"x": 0.062015503875968984, "y": 0.30597014925373134, "ox": 0.062015503875968984, "oy": 0.30597014925373134, "term": "applied", "cat25k": 35, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 8, "s": 0.858974358974359, "os": 0.24188034188034188, "bg": 2.1770469673002654e-06}, {"x": 0.08527131782945736, "y": 0.7089552238805971, "ox": 0.08527131782945736, "oy": 0.7089552238805971, "term": "problems", "cat25k": 98, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 115, "ncat": 11, "s": 0.9914529914529916, "os": 0.6188034188034188, "bg": 2.080193180606706e-06}, {"x": 0.18604651162790695, "y": 0.12686567164179102, "ox": 0.18604651162790695, "oy": 0.12686567164179102, "term": "proficient", "cat25k": 15, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 24, "s": 0.30235042735042733, "os": -0.05897435897435899, "bg": 3.637550881351963e-05}, {"x": 0.7596899224806201, "y": 0.3656716417910447, "ox": 0.7596899224806201, "oy": 0.3656716417910447, "term": "one", "cat25k": 42, "ncat25k": 116, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 112, "s": 0.056623931623931624, "os": -0.39116809116809115, "bg": 3.2407680048105395e-07}, {"x": 0.4031007751937984, "y": 0.5223880597014925, "ox": 0.4031007751937984, "oy": 0.5223880597014925, "term": "c", "cat25k": 61, "ncat25k": 57, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 72, "ncat": 55, "s": 0.7307692307692308, "os": 0.11823361823361822, "bg": 4.256900848977702e-07}, {"x": 0.0697674418604651, "y": 0.05223880597014925, "ox": 0.0697674418604651, "oy": 0.05223880597014925, "term": "familiar", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 9, "s": 0.5042735042735044, "os": -0.017663817663817666, "bg": 1.9458105149046345e-06}, {"x": 0.41860465116279066, "y": 0.28358208955223874, "ox": 0.41860465116279066, "oy": 0.28358208955223874, "term": "relational", "cat25k": 32, "ncat25k": 60, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 58, "s": 0.13461538461538464, "os": -0.1341880341880342, "bg": 6.168553805049926e-05}, {"x": 0.5348837209302324, "y": 0.04477611940298507, "ox": 0.5348837209302324, "oy": 0.04477611940298507, "term": "mapreduce", "cat25k": 5, "ncat25k": 77, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 75, "s": 0.021367521367521368, "os": -0.48660968660968656, "bg": 0.002949208083014746}, {"x": 0.34883720930232553, "y": 0.18656716417910446, "ox": 0.34883720930232553, "oy": 0.18656716417910446, "term": "hive", "cat25k": 21, "ncat25k": 49, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 47, "s": 0.10576923076923077, "os": -0.16125356125356127, "bg": 0.00013974775530168048}, {"x": 0.11627906976744183, "y": 0.05970149253731343, "ox": 0.11627906976744183, "oy": 0.05970149253731343, "term": "pig", "cat25k": 7, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 15, "s": 0.3066239316239316, "os": -0.05641025641025642, "bg": 6.493769581009281e-06}, {"x": 0.44186046511627897, "y": 0.6417910447761193, "ox": 0.44186046511627897, "oy": 0.6417910447761193, "term": "information", "cat25k": 82, "ncat25k": 63, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 96, "ncat": 61, "s": 0.827991452991453, "os": 0.1982905982905983, "bg": 3.3667528479268516e-07}, {"x": 0.01550387596899225, "y": 0.39552238805970147, "ox": 0.01550387596899225, "oy": 0.39552238805970147, "term": "retrieval", "cat25k": 45, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 53, "ncat": 2, "s": 0.952991452991453, "os": 0.3769230769230769, "bg": 1.875670978663731e-05}, {"x": 0.2403100775193798, "y": 0.47014925373134325, "ox": 0.2403100775193798, "oy": 0.47014925373134325, "term": "platforms", "cat25k": 54, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 31, "s": 0.8482905982905983, "os": 0.22792022792022792, "bg": 1.422853848236895e-05}, {"x": 0.30232558139534876, "y": 0.05223880597014925, "ox": 0.30232558139534876, "oy": 0.05223880597014925, "term": "azure", "cat25k": 6, "ncat25k": 40, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 39, "s": 0.07585470085470085, "os": -0.24843304843304845, "bg": 8.385346788776759e-05}, {"x": 0.01550387596899225, "y": 0.11940298507462686, "ox": 0.01550387596899225, "oy": 0.11940298507462686, "term": "ph", "cat25k": 14, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 2, "s": 0.7008547008547009, "os": 0.10284900284900284, "bg": 1.6490175977661307e-06}, {"x": 0.6356589147286821, "y": 0.2462686567164179, "ox": 0.6356589147286821, "oy": 0.2462686567164179, "term": "highly", "cat25k": 28, "ncat25k": 94, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 91, "s": 0.057692307692307696, "os": -0.3866096866096866, "bg": 5.779236071679865e-06}, {"x": 0.6279069767441859, "y": 0.47014925373134325, "ox": 0.6279069767441859, "oy": 0.47014925373134325, "term": "equivalent", "cat25k": 54, "ncat25k": 93, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 63, "ncat": 90, "s": 0.10790598290598291, "os": -0.15669515669515666, "bg": 1.2694108896371684e-05}, {"x": 0.6356589147286821, "y": 0.17164179104477612, "ox": 0.6356589147286821, "oy": 0.17164179104477612, "term": "hands", "cat25k": 20, "ncat25k": 94, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 91, "s": 0.03205128205128205, "os": -0.4606837606837606, "bg": 4.639550663588013e-06}, {"x": 0.0, "y": 0.26865671641791045, "ox": 0.0, "oy": 0.26865671641791045, "term": "scientist", "cat25k": 31, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 0, "s": 0.8920940170940171, "os": 0.26638176638176636, "bg": 8.318371817356283e-06}, {"x": 0.05426356589147286, "y": 0.16417910447761191, "ox": 0.05426356589147286, "oy": 0.16417910447761191, "term": "graph", "cat25k": 19, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 7, "s": 0.7126068376068376, "os": 0.10883190883190882, "bg": 3.7223334358472512e-06}, {"x": 0.046511627906976744, "y": 0.029850746268656716, "ox": 0.046511627906976744, "oy": 0.029850746268656716, "term": "desirable", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 6, "s": 0.513888888888889, "os": -0.016809116809116814, "bg": 3.3593853668532807e-06}, {"x": 0.1395348837209302, "y": 0.15671641791044774, "ox": 0.1395348837209302, "oy": 0.15671641791044774, "term": "analytic", "cat25k": 18, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 18, "s": 0.6303418803418804, "os": 0.016809116809116814, "bg": 2.9422393157105773e-05}, {"x": 0.3953488372093023, "y": 0.17164179104477612, "ox": 0.3953488372093023, "oy": 0.17164179104477612, "term": "processes", "cat25k": 20, "ncat25k": 55, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 53, "s": 0.08867521367521368, "os": -0.22222222222222224, "bg": 3.847578676972103e-06}, {"x": 0.1317829457364341, "y": 0.15671641791044774, "ox": 0.1317829457364341, "oy": 0.15671641791044774, "term": "use", "cat25k": 18, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 17, "s": 0.6346153846153847, "os": 0.024501424501424507, "bg": 1.0555040352308138e-07}, {"x": 0.05426356589147286, "y": 0.0373134328358209, "ox": 0.05426356589147286, "oy": 0.0373134328358209, "term": "demonstrable", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.5074786324786326, "os": -0.017094017094017096, "bg": 5.7706456871636795e-05}, {"x": 0.22480620155038755, "y": 0.42537313432835816, "ox": 0.22480620155038755, "oy": 0.42537313432835816, "term": "well", "cat25k": 49, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 57, "ncat": 29, "s": 0.8290598290598291, "os": 0.19886039886039886, "bg": 4.7495746714461005e-07}, {"x": 0.48837209302325574, "y": 0.05223880597014925, "ox": 0.48837209302325574, "oy": 0.05223880597014925, "term": "ambiguity", "cat25k": 6, "ncat25k": 70, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 68, "s": 0.04700854700854701, "os": -0.4330484330484331, "bg": 9.148466808447571e-05}, {"x": 0.031007751937984492, "y": 0.022388059701492536, "ox": 0.031007751937984492, "oy": 0.022388059701492536, "term": "prioritizing", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 4, "s": 0.5801282051282052, "os": -0.008831908831908833, "bg": 3.288175720110483e-05}, {"x": 0.1395348837209302, "y": 0.14179104477611937, "ox": 0.1395348837209302, "oy": 0.14179104477611937, "term": "delivering", "cat25k": 16, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 18, "s": 0.6228632478632479, "os": 0.001994301994301978, "bg": 9.138762977043429e-06}, {"x": 0.10077519379844961, "y": 0.4328358208955224, "ox": 0.10077519379844961, "oy": 0.4328358208955224, "term": "results", "cat25k": 49, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 58, "ncat": 13, "s": 0.93482905982906, "os": 0.32934472934472936, "bg": 5.293850395333003e-07}, {"x": 0.10077519379844961, "y": 0.4402985074626865, "ox": 0.10077519379844961, "oy": 0.4402985074626865, "term": "dynamic", "cat25k": 50, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 13, "s": 0.9401709401709402, "os": 0.3367521367521367, "bg": 4.91359259745437e-06}, {"x": 0.3255813953488372, "y": 0.46268656716417905, "ox": 0.3255813953488372, "oy": 0.46268656716417905, "term": "technology", "cat25k": 53, "ncat25k": 43, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 62, "ncat": 42, "s": 0.7542735042735043, "os": 0.13589743589743591, "bg": 8.712799159352276e-07}, {"x": 0.046511627906976744, "y": 0.14179104477611937, "ox": 0.046511627906976744, "oy": 0.14179104477611937, "term": "areas", "cat25k": 16, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 6, "s": 0.6848290598290598, "os": 0.09430199430199429, "bg": 4.096977052856109e-07}, {"x": 0.023255813953488372, "y": 0.4402985074626865, "ox": 0.023255813953488372, "oy": 0.4402985074626865, "term": "presentation", "cat25k": 50, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 3, "s": 0.9626068376068376, "os": 0.41367521367521365, "bg": 3.3117785514104345e-06}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "bsc", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 4.331366892764019e-06}, {"x": 0.046511627906976744, "y": 0.23880597014925373, "ox": 0.046511627906976744, "oy": 0.23880597014925373, "term": "ba", "cat25k": 27, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 6, "s": 0.8183760683760685, "os": 0.1905982905982906, "bg": 4.234462878163304e-06}, {"x": 0.4263565891472868, "y": 0.25373134328358204, "ox": 0.4263565891472868, "oy": 0.25373134328358204, "term": "database", "cat25k": 29, "ncat25k": 61, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 59, "s": 0.10042735042735043, "os": -0.17150997150997155, "bg": 1.7921391458456713e-06}, {"x": 0.062015503875968984, "y": 0.14925373134328357, "ox": 0.062015503875968984, "oy": 0.14925373134328357, "term": "solid", "cat25k": 17, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 8, "s": 0.6762820512820513, "os": 0.08632478632478632, "bg": 1.494077543318177e-06}, {"x": 0.22480620155038755, "y": 0.22388059701492533, "ox": 0.22480620155038755, "oy": 0.22388059701492533, "term": "linux", "cat25k": 26, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 29, "s": 0.6217948717948719, "os": -0.001139601139601154, "bg": 1.2979993133583634e-06}, {"x": 0.08527131782945736, "y": 0.08208955223880597, "ox": 0.08527131782945736, "oy": 0.08208955223880597, "term": "basic", "cat25k": 9, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 11, "s": 0.6047008547008548, "os": -0.0034188034188034205, "bg": 5.182867038834999e-07}, {"x": 0.1317829457364341, "y": 0.022388059701492536, "ox": 0.1317829457364341, "oy": 0.022388059701492536, "term": "workflow", "cat25k": 3, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 17, "s": 0.16666666666666669, "os": -0.10883190883190884, "bg": 7.536122992541688e-06}, {"x": 0.03875968992248062, "y": 0.01492537313432836, "ox": 0.03875968992248062, "oy": 0.01492537313432836, "term": "advantage", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47970085470085466, "os": -0.023931623931623933, "bg": 4.2400697188606636e-07}, {"x": 0.0697674418604651, "y": 0.33582089552238803, "ox": 0.0697674418604651, "oy": 0.33582089552238803, "term": "employment", "cat25k": 38, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 9, "s": 0.8878205128205129, "os": 0.2638176638176638, "bg": 1.3643244000108691e-06}, {"x": 0.07751937984496123, "y": 0.35074626865671643, "ox": 0.07751937984496123, "oy": 0.35074626865671643, "term": "social", "cat25k": 40, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 10, "s": 0.8963675213675215, "os": 0.270940170940171, "bg": 8.164944425628066e-07}, {"x": 0.22480620155038755, "y": 0.14925373134328357, "ox": 0.22480620155038755, "oy": 0.14925373134328357, "term": "benefits", "cat25k": 17, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 29, "s": 0.23290598290598294, "os": -0.07521367521367522, "bg": 1.2787748981550398e-06}, {"x": 0.023255813953488372, "y": 0.18656716417910446, "ox": 0.023255813953488372, "oy": 0.18656716417910446, "term": "toolkits", "cat25k": 21, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 3, "s": 0.7873931623931625, "os": 0.16182336182336182, "bg": 7.516052139927417e-05}, {"x": 0.03875968992248062, "y": 0.20895522388059698, "ox": 0.03875968992248062, "oy": 0.20895522388059698, "term": "numpy", "cat25k": 24, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 5, "s": 0.8002136752136753, "os": 0.16866096866096866, "bg": 0.0007491742057050752}, {"x": 0.10852713178294572, "y": 0.19402985074626863, "ox": 0.10852713178294572, "oy": 0.19402985074626863, "term": "product", "cat25k": 22, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 14, "s": 0.6741452991452992, "os": 0.08461538461538462, "bg": 2.004152383281397e-07}, {"x": 0.07751937984496123, "y": 0.029850746268656716, "ox": 0.07751937984496123, "oy": 0.029850746268656716, "term": "great", "cat25k": 3, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.3450854700854701, "os": -0.047578347578347585, "bg": 9.285596299421919e-08}, {"x": 0.18604651162790695, "y": 0.5074626865671641, "ox": 0.18604651162790695, "oy": 0.5074626865671641, "term": "able", "cat25k": 60, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 70, "ncat": 24, "s": 0.9294871794871796, "os": 0.3188034188034188, "bg": 1.7177737714054829e-06}, {"x": 0.0697674418604651, "y": 0.13432835820895522, "ox": 0.0697674418604651, "oy": 0.13432835820895522, "term": "independently", "cat25k": 15, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 9, "s": 0.6581196581196582, "os": 0.06381766381766382, "bg": 7.181849073441721e-06}, {"x": 0.18604651162790695, "y": 0.04477611940298507, "ox": 0.18604651162790695, "oy": 0.04477611940298507, "term": "procedures", "cat25k": 5, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 24, "s": 0.12500000000000003, "os": -0.14045584045584047, "bg": 1.2469329864753702e-06}, {"x": 0.062015503875968984, "y": 0.05970149253731343, "ox": 0.062015503875968984, "oy": 0.05970149253731343, "term": "used", "cat25k": 7, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 8, "s": 0.608974358974359, "os": -0.002564102564102569, "bg": 7.592059582559524e-08}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "amounts", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 4.048679841903101e-07}, {"x": 0.6201550387596898, "y": 0.10447761194029849, "ox": 0.6201550387596898, "oy": 0.10447761194029849, "term": "structured", "cat25k": 12, "ncat25k": 92, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 89, "s": 0.017094017094017096, "os": -0.5119658119658119, "bg": 2.3941239365905816e-05}, {"x": 0.3565891472868217, "y": 0.07462686567164178, "ox": 0.3565891472868217, "oy": 0.07462686567164178, "term": "unstructured", "cat25k": 9, "ncat25k": 50, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 48, "s": 0.07051282051282051, "os": -0.28005698005698004, "bg": 0.00014153970798894038}, {"x": 0.9224806201550386, "y": 0.6567164179104477, "ox": 0.9224806201550386, "oy": 0.6567164179104477, "term": "technologies", "cat25k": 85, "ncat25k": 178, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 100, "ncat": 172, "s": 0.07264957264957264, "os": -0.2638176638176639, "bg": 9.334642631357512e-06}, {"x": 0.05426356589147286, "y": 0.22388059701492533, "ox": 0.05426356589147286, "oy": 0.22388059701492533, "term": "performing", "cat25k": 26, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 7, "s": 0.7980769230769231, "os": 0.16809116809116809, "bg": 3.5301437784626787e-06}, {"x": 0.22480620155038755, "y": 0.05223880597014925, "ox": 0.22480620155038755, "oy": 0.05223880597014925, "term": "oracle", "cat25k": 6, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 29, "s": 0.10149572649572651, "os": -0.17150997150997152, "bg": 4.169310849398064e-06}, {"x": 0.23255813953488366, "y": 0.09701492537313433, "ox": 0.23255813953488366, "oy": 0.09701492537313433, "term": "mysql", "cat25k": 11, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 30, "s": 0.13354700854700857, "os": -0.13475783475783476, "bg": 3.7788213671608734e-06}, {"x": 0.08527131782945736, "y": 0.08208955223880597, "ox": 0.08527131782945736, "oy": 0.08208955223880597, "term": "attention", "cat25k": 9, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 11, "s": 0.6047008547008548, "os": -0.0034188034188034205, "bg": 9.81981246702412e-07}, {"x": 0.2015503875968992, "y": 0.2761194029850746, "ox": 0.2015503875968992, "oy": 0.2761194029850746, "term": "detail", "cat25k": 32, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 26, "s": 0.6645299145299146, "os": 0.0737891737891738, "bg": 2.7662367890787917e-06}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "resolve", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 1.1432266772707291e-06}, {"x": 0.41860465116279066, "y": 0.09701492537313433, "ox": 0.41860465116279066, "oy": 0.09701492537313433, "term": "quality", "cat25k": 11, "ncat25k": 60, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 58, "s": 0.06623931623931624, "os": -0.3193732193732194, "bg": 7.490855893747126e-07}, {"x": 0.19379844961240308, "y": 0.06716417910447761, "ox": 0.19379844961240308, "oy": 0.06716417910447761, "term": "issues", "cat25k": 8, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 25, "s": 0.1452991452991453, "os": -0.12592592592592594, "bg": 4.5284680378428356e-07}, {"x": 0.07751937984496123, "y": 0.13432835820895522, "ox": 0.07751937984496123, "oy": 0.13432835820895522, "term": "experienced", "cat25k": 15, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 10, "s": 0.6506410256410257, "os": 0.05612535612535613, "bg": 2.289537252222497e-06}, {"x": 0.12403100775193797, "y": 0.2462686567164179, "ox": 0.12403100775193797, "oy": 0.2462686567164179, "term": "multi", "cat25k": 28, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 16, "s": 0.732905982905983, "os": 0.12108262108262105, "bg": 1.7758010896931582e-06}, {"x": 0.33333333333333326, "y": 0.09701492537313433, "ox": 0.33333333333333326, "oy": 0.09701492537313433, "term": "designing", "cat25k": 11, "ncat25k": 45, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 44, "s": 0.08226495726495726, "os": -0.2347578347578348, "bg": 1.1963646884776542e-05}, {"x": 0.0, "y": 0.2014925373134328, "ox": 0.0, "oy": 0.2014925373134328, "term": "experiments", "cat25k": 23, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 0, "s": 0.8311965811965812, "os": 0.1997150997150997, "bg": 3.92858387428357e-06}, {"x": 0.0697674418604651, "y": 0.5149253731343283, "ox": 0.0697674418604651, "oy": 0.5149253731343283, "term": "solve", "cat25k": 61, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 71, "ncat": 9, "s": 0.9658119658119658, "os": 0.44159544159544156, "bg": 1.1845640349034756e-05}, {"x": 0.2790697674418604, "y": 0.44776119402985065, "ox": 0.2790697674418604, "oy": 0.44776119402985065, "term": "real", "cat25k": 51, "ncat25k": 37, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 60, "ncat": 36, "s": 0.7970085470085471, "os": 0.16723646723646723, "bg": 6.448808061767813e-07}, {"x": 0.062015503875968984, "y": 0.44776119402985065, "ox": 0.062015503875968984, "oy": 0.44776119402985065, "term": "world", "cat25k": 51, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 60, "ncat": 8, "s": 0.9561965811965812, "os": 0.38262108262108263, "bg": 3.1482271016028213e-07}, {"x": 0.046511627906976744, "y": 0.022388059701492536, "ox": 0.046511627906976744, "oy": 0.022388059701492536, "term": "experts", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.4700854700854701, "os": -0.02421652421652422, "bg": 5.628749626743541e-07}, {"x": 0.2945736434108526, "y": 0.6492537313432835, "ox": 0.2945736434108526, "oy": 0.6492537313432835, "term": "new", "cat25k": 83, "ncat25k": 39, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 97, "ncat": 38, "s": 0.9476495726495727, "os": 0.3518518518518519, "bg": 1.7404604414011248e-07}, {"x": 0.01550387596899225, "y": 0.11194029850746266, "ox": 0.01550387596899225, "oy": 0.11194029850746266, "term": "practice", "cat25k": 13, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 2, "s": 0.6912393162393162, "os": 0.09544159544159544, "bg": 4.0431582888383766e-07}, {"x": 0.48837209302325574, "y": 0.01492537313432836, "ox": 0.48837209302325574, "oy": 0.01492537313432836, "term": "go", "cat25k": 2, "ncat25k": 70, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 68, "s": 0.026709401709401708, "os": -0.47008547008547014, "bg": 3.324300125537208e-07}, {"x": 0.08527131782945736, "y": 0.23880597014925373, "ox": 0.08527131782945736, "oy": 0.23880597014925373, "term": "create", "cat25k": 27, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 32, "ncat": 11, "s": 0.7692307692307694, "os": 0.15213675213675212, "bg": 6.271603577713561e-07}, {"x": 0.046511627906976744, "y": 0.18656716417910446, "ox": 0.046511627906976744, "oy": 0.18656716417910446, "term": "masters", "cat25k": 21, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 6, "s": 0.7585470085470086, "os": 0.13874643874643874, "bg": 4.0028337480462785e-06}, {"x": 0.0, "y": 0.46268656716417905, "ox": 0.0, "oy": 0.46268656716417905, "term": "tensorflow", "cat25k": 53, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 62, "ncat": 0, "s": 0.9732905982905984, "os": 0.458974358974359, "bg": 0.002258199632131995}, {"x": 0.0, "y": 0.28358208955223874, "ox": 0.0, "oy": 0.28358208955223874, "term": "keras", "cat25k": 32, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 0, "s": 0.9081196581196581, "os": 0.2811965811965812, "bg": 0.0013846630349627416}, {"x": 0.062015503875968984, "y": 0.029850746268656716, "ox": 0.062015503875968984, "oy": 0.029850746268656716, "term": "consumer", "cat25k": 3, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.40918803418803423, "os": -0.0321937321937322, "bg": 3.543969662438367e-07}, {"x": 0.12403100775193797, "y": 0.11940298507462686, "ox": 0.12403100775193797, "oy": 0.11940298507462686, "term": "products", "cat25k": 14, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 16, "s": 0.5993589743589745, "os": -0.004843304843304863, "bg": 1.544280383233349e-07}, {"x": 0.0, "y": 0.17164179104477612, "ox": 0.0, "oy": 0.17164179104477612, "term": "ready", "cat25k": 20, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 0, "s": 0.8034188034188035, "os": 0.1700854700854701, "bg": 7.333904442541314e-07}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "molecular", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 6.499128873013685e-07}, {"x": 0.0697674418604651, "y": 0.14179104477611937, "ox": 0.0697674418604651, "oy": 0.14179104477611937, "term": "levels", "cat25k": 16, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 9, "s": 0.6623931623931624, "os": 0.07122507122507121, "bg": 8.30666794035575e-07}, {"x": 0.0, "y": 0.11194029850746266, "ox": 0.0, "oy": 0.11194029850746266, "term": "interdisciplinary", "cat25k": 13, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.7190170940170941, "os": 0.11082621082621083, "bg": 7.658021356179424e-06}, {"x": 0.22480620155038755, "y": 0.25373134328358204, "ox": 0.22480620155038755, "oy": 0.25373134328358204, "term": "teams", "cat25k": 29, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 29, "s": 0.638888888888889, "os": 0.028490028490028463, "bg": 3.658727808073593e-06}, {"x": 0.0697674418604651, "y": 0.5597014925373134, "ox": 0.0697674418604651, "oy": 0.5597014925373134, "term": "position", "cat25k": 67, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 78, "ncat": 9, "s": 0.9786324786324787, "os": 0.48603988603988607, "bg": 1.9955473375195254e-06}, {"x": 0.1395348837209302, "y": 0.13432835820895522, "ox": 0.1395348837209302, "oy": 0.13432835820895522, "term": "within", "cat25k": 15, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 18, "s": 0.5972222222222222, "os": -0.005413105413105412, "bg": 2.753916946753088e-07}, {"x": 0.16279069767441856, "y": 0.18656716417910446, "ox": 0.16279069767441856, "oy": 0.18656716417910446, "term": "professional", "cat25k": 21, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 21, "s": 0.6335470085470086, "os": 0.023361823361823353, "bg": 7.466471767947382e-07}, {"x": 0.05426356589147286, "y": 0.029850746268656716, "ox": 0.05426356589147286, "oy": 0.029850746268656716, "term": "culture", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.4658119658119658, "os": -0.024501424501424507, "bg": 3.204134428644356e-07}, {"x": 0.07751937984496123, "y": 0.5895522388059701, "ox": 0.07751937984496123, "oy": 0.5895522388059701, "term": "diverse", "cat25k": 70, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 82, "ncat": 10, "s": 0.9850427350427352, "os": 0.5079772079772079, "bg": 1.364249197317293e-05}, {"x": 0.046511627906976744, "y": 0.022388059701492536, "ox": 0.046511627906976744, "oy": 0.022388059701492536, "term": "international", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.4700854700854701, "os": -0.02421652421652422, "bg": 6.087372895104396e-08}, {"x": 0.046511627906976744, "y": 0.3656716417910447, "ox": 0.046511627906976744, "oy": 0.3656716417910447, "term": "creativity", "cat25k": 42, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 6, "s": 0.9262820512820514, "os": 0.3165242165242165, "bg": 1.4322125965962295e-05}, {"x": 0.03875968992248062, "y": 0.022388059701492536, "ox": 0.03875968992248062, "oy": 0.022388059701492536, "term": "annual", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 5, "s": 0.5181623931623932, "os": -0.016524216524216526, "bg": 1.8801731789911705e-07}, {"x": 0.1472868217054263, "y": 0.04477611940298507, "ox": 0.1472868217054263, "oy": 0.04477611940298507, "term": "bonus", "cat25k": 5, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 19, "s": 0.18055555555555558, "os": -0.10199430199430201, "bg": 1.7937630285493172e-06}, {"x": 0.08527131782945736, "y": 0.08955223880597014, "ox": 0.08527131782945736, "oy": 0.08955223880597014, "term": "training", "cat25k": 10, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 11, "s": 0.6271367521367521, "os": 0.003988603988603984, "bg": 2.6109063939769436e-07}, {"x": 0.007751937984496123, "y": 0.11194029850746266, "ox": 0.007751937984496123, "oy": 0.11194029850746266, "term": "prototyping", "cat25k": 13, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 1, "s": 0.701923076923077, "os": 0.10313390313390314, "bg": 2.5600020480016384e-05}, {"x": 0.0, "y": 0.12686567164179102, "ox": 0.0, "oy": 0.12686567164179102, "term": "explain", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 0, "s": 0.7382478632478633, "os": 0.12564102564102564, "bg": 1.3578935811611595e-06}, {"x": 0.1472868217054263, "y": 0.4104477611940298, "ox": 0.1472868217054263, "oy": 0.4104477611940298, "term": "following", "cat25k": 47, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 19, "s": 0.8856837606837608, "os": 0.26096866096866095, "bg": 6.703965841518393e-07}, {"x": 0.08527131782945736, "y": 0.18656716417910446, "ox": 0.08527131782945736, "oy": 0.18656716417910446, "term": "ideas", "cat25k": 21, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 11, "s": 0.6955128205128206, "os": 0.10028490028490028, "bg": 1.0688115418345753e-06}, {"x": 0.08527131782945736, "y": 0.16417910447761191, "ox": 0.08527131782945736, "oy": 0.16417910447761191, "term": "stakeholders", "cat25k": 19, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 11, "s": 0.6655982905982906, "os": 0.07806267806267805, "bg": 9.881050122075883e-06}, {"x": 0.007751937984496123, "y": 0.15671641791044774, "ox": 0.007751937984496123, "oy": 0.15671641791044774, "term": "disciplinary", "cat25k": 18, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 1, "s": 0.764957264957265, "os": 0.1475783475783476, "bg": 7.850919601408814e-06}, {"x": 0.1705426356589147, "y": 0.11940298507462686, "ox": 0.1705426356589147, "oy": 0.11940298507462686, "term": "end", "cat25k": 14, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 22, "s": 0.3344017094017094, "os": -0.05099715099715102, "bg": 3.440981496506849e-07}, {"x": 0.046511627906976744, "y": 0.12686567164179102, "ox": 0.046511627906976744, "oy": 0.12686567164179102, "term": "customer", "cat25k": 15, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 6, "s": 0.6688034188034189, "os": 0.07948717948717948, "bg": 2.4937417923367964e-07}, {"x": 0.05426356589147286, "y": 0.5074626865671641, "ox": 0.05426356589147286, "oy": 0.5074626865671641, "term": "ml", "cat25k": 60, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 70, "ncat": 7, "s": 0.969017094017094, "os": 0.44957264957264953, "bg": 7.750480819195625e-06}, {"x": 0.15503875968992245, "y": 0.06716417910447761, "ox": 0.15503875968992245, "oy": 0.06716417910447761, "term": "preferably", "cat25k": 8, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 20, "s": 0.20512820512820515, "os": -0.08746438746438748, "bg": 1.4640046686604055e-05}, {"x": 0.3100775193798449, "y": 0.15671641791044774, "ox": 0.3100775193798449, "oy": 0.15671641791044774, "term": "security", "cat25k": 18, "ncat25k": 41, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 40, "s": 0.11431623931623931, "os": -0.1524216524216524, "bg": 5.302758635434861e-07}, {"x": 0.5348837209302324, "y": 0.007462686567164179, "ox": 0.5348837209302324, "oy": 0.007462686567164179, "term": "flink", "cat25k": 1, "ncat25k": 77, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 75, "s": 0.016025641025641024, "os": -0.5236467236467236, "bg": 0.0013843729792252979}, {"x": 0.2015503875968992, "y": 0.01492537313432836, "ox": 0.2015503875968992, "oy": 0.01492537313432836, "term": "pipeline", "cat25k": 2, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 26, "s": 0.09294871794871794, "os": -0.18547008547008548, "bg": 3.5166590734042924e-06}, {"x": 0.16279069767441856, "y": 0.5298507462686567, "ox": 0.16279069767441856, "oy": 0.5298507462686567, "term": "process", "cat25k": 62, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 73, "ncat": 21, "s": 0.9497863247863249, "os": 0.3641025641025641, "bg": 1.0628427243779713e-06}, {"x": 0.0697674418604651, "y": 0.17164179104477612, "ox": 0.0697674418604651, "oy": 0.17164179104477612, "term": "desire", "cat25k": 20, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 9, "s": 0.6965811965811967, "os": 0.10085470085470086, "bg": 3.5878337007686147e-06}, {"x": 0.10077519379844961, "y": 0.4402985074626865, "ox": 0.10077519379844961, "oy": 0.4402985074626865, "term": "fast", "cat25k": 50, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 59, "ncat": 13, "s": 0.9401709401709402, "os": 0.3367521367521367, "bg": 1.5319736790325562e-06}, {"x": 0.07751937984496123, "y": 0.39552238805970147, "ox": 0.07751937984496123, "oy": 0.39552238805970147, "term": "paced", "cat25k": 45, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 53, "ncat": 10, "s": 0.9252136752136753, "os": 0.3153846153846154, "bg": 4.4832949228819896e-05}, {"x": 0.01550387596899225, "y": 0.14179104477611937, "ox": 0.01550387596899225, "oy": 0.14179104477611937, "term": "without", "cat25k": 16, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 2, "s": 0.7361111111111112, "os": 0.12507122507122506, "bg": 1.91566534193087e-07}, {"x": 0.07751937984496123, "y": 0.007462686567164179, "ox": 0.07751937984496123, "oy": 0.007462686567164179, "term": "curation", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 10, "s": 0.24145299145299146, "os": -0.06980056980056981, "bg": 0.00010647977852206067}, {"x": 0.30232558139534876, "y": 0.06716417910447761, "ox": 0.30232558139534876, "oy": 0.06716417910447761, "term": "system", "cat25k": 8, "ncat25k": 40, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 39, "s": 0.08333333333333333, "os": -0.23361823361823364, "bg": 2.417953820935634e-07}, {"x": 0.062015503875968984, "y": 0.0373134328358209, "ox": 0.062015503875968984, "oy": 0.0373134328358209, "term": "map", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 8, "s": 0.46260683760683763, "os": -0.02478632478632479, "bg": 8.394368924307112e-08}, {"x": 0.046511627906976744, "y": 0.0373134328358209, "ox": 0.046511627906976744, "oy": 0.0373134328358209, "term": "reduce", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.5662393162393163, "os": -0.009401709401709403, "bg": 5.515774010515271e-07}, {"x": 0.07751937984496123, "y": 0.08208955223880597, "ox": 0.07751937984496123, "oy": 0.08208955223880597, "term": "standard", "cat25k": 9, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 10, "s": 0.6282051282051283, "os": 0.004273504273504272, "bg": 2.8526391415153664e-07}, {"x": 0.07751937984496123, "y": 0.2014925373134328, "ox": 0.07751937984496123, "oy": 0.2014925373134328, "term": "free", "cat25k": 23, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 10, "s": 0.733974358974359, "os": 0.12279202279202278, "bg": 7.29666318208929e-08}, {"x": 0.08527131782945736, "y": 0.022388059701492536, "ox": 0.08527131782945736, "oy": 0.022388059701492536, "term": "elements", "cat25k": 3, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 11, "s": 0.26495726495726496, "os": -0.06267806267806268, "bg": 6.32982245209726e-07}, {"x": 0.05426356589147286, "y": 0.007462686567164179, "ox": 0.05426356589147286, "oy": 0.007462686567164179, "term": "ideally", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.36004273504273504, "os": -0.046723646723646726, "bg": 3.899505835123043e-06}, {"x": 0.23255813953488366, "y": 0.30597014925373134, "ox": 0.23255813953488366, "oy": 0.30597014925373134, "term": "google", "cat25k": 35, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 30, "s": 0.6634615384615385, "os": 0.07264957264957264, "bg": 1.6780189176307661e-06}, {"x": 0.05426356589147286, "y": 0.5895522388059701, "ox": 0.05426356589147286, "oy": 0.5895522388059701, "term": "sas", "cat25k": 70, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 82, "ncat": 7, "s": 0.9882478632478634, "os": 0.531054131054131, "bg": 2.916512853383458e-05}, {"x": 0.0, "y": 0.12686567164179102, "ox": 0.0, "oy": 0.12686567164179102, "term": "spss", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 0, "s": 0.7382478632478633, "os": 0.12564102564102564, "bg": 3.619439370132386e-05}, {"x": 0.046511627906976744, "y": 0.20895522388059698, "ox": 0.046511627906976744, "oy": 0.20895522388059698, "term": "sense", "cat25k": 24, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 6, "s": 0.7863247863247863, "os": 0.16096866096866097, "bg": 1.2907018069597527e-06}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "inconsistencies", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 1.0358293367584757e-05}, {"x": 0.16279069767441856, "y": 0.05223880597014925, "ox": 0.16279069767441856, "oy": 0.05223880597014925, "term": "managing", "cat25k": 6, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 21, "s": 0.1634615384615385, "os": -0.10997150997150998, "bg": 2.230249527485794e-06}, {"x": 0.046511627906976744, "y": 0.0373134328358209, "ox": 0.046511627906976744, "oy": 0.0373134328358209, "term": "collaboration", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.5662393162393163, "os": -0.009401709401709403, "bg": 1.2484229439027392e-06}, {"x": 0.10852713178294572, "y": 0.10447761194029849, "ox": 0.10852713178294572, "oy": 0.10447761194029849, "term": "show", "cat25k": 12, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 14, "s": 0.6014957264957266, "os": -0.004273504273504286, "bg": 2.2617410948377003e-07}, {"x": 0.09302325581395349, "y": 0.08955223880597014, "ox": 0.09302325581395349, "oy": 0.08955223880597014, "term": "priorities", "cat25k": 10, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 12, "ncat": 12, "s": 0.6036324786324787, "os": -0.003703703703703709, "bg": 4.23163027258928e-06}, {"x": 0.03875968992248062, "y": 0.01492537313432836, "ox": 0.03875968992248062, "oy": 0.01492537313432836, "term": "changing", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47970085470085466, "os": -0.023931623931623933, "bg": 4.357098588131998e-07}, {"x": 0.1472868217054263, "y": 0.05970149253731343, "ox": 0.1472868217054263, "oy": 0.05970149253731343, "term": "engineer", "cat25k": 7, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 19, "s": 0.2061965811965812, "os": -0.08717948717948719, "bg": 2.3827955804966594e-06}, {"x": 0.0, "y": 0.16417910447761191, "ox": 0.0, "oy": 0.16417910447761191, "term": "simulation", "cat25k": 19, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 0, "s": 0.7895299145299145, "os": 0.16267806267806267, "bg": 2.71275121309302e-06}, {"x": 0.34883720930232553, "y": 0.25373134328358204, "ox": 0.34883720930232553, "oy": 0.25373134328358204, "term": "familiarity", "cat25k": 29, "ncat25k": 49, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 47, "s": 0.19551282051282054, "os": -0.09458689458689462, "bg": 8.906178358866212e-05}, {"x": 0.046511627906976744, "y": 0.23134328358208953, "ox": 0.046511627906976744, "oy": 0.23134328358208953, "term": "optimization", "cat25k": 26, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 6, "s": 0.8141025641025642, "os": 0.1831908831908832, "bg": 4.589780320709039e-06}, {"x": 0.07751937984496123, "y": 0.30597014925373134, "ox": 0.07751937984496123, "oy": 0.30597014925373134, "term": "javascript", "cat25k": 35, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 10, "s": 0.8472222222222223, "os": 0.2264957264957265, "bg": 3.950253757330334e-06}, {"x": 0.21705426356589144, "y": 0.46268656716417905, "ox": 0.21705426356589144, "oy": 0.46268656716417905, "term": "decision", "cat25k": 53, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 62, "ncat": 28, "s": 0.8611111111111113, "os": 0.24358974358974358, "bg": 2.5330203833557476e-06}, {"x": 0.0, "y": 0.2761194029850746, "ox": 0.0, "oy": 0.2761194029850746, "term": "word", "cat25k": 32, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 0, "s": 0.9006410256410257, "os": 0.2737891737891738, "bg": 7.495475341116804e-07}, {"x": 0.0, "y": 0.21641791044776115, "ox": 0.0, "oy": 0.21641791044776115, "term": "embeddings", "cat25k": 25, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 0, "s": 0.8418803418803419, "os": 0.21452991452991452, "bg": 0.0003430470861698793}, {"x": 0.05426356589147286, "y": 0.22388059701492533, "ox": 0.05426356589147286, "oy": 0.22388059701492533, "term": "resources", "cat25k": 26, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 7, "s": 0.7980769230769231, "os": 0.16809116809116809, "bg": 3.4050617568228395e-07}, {"x": 0.3100775193798449, "y": 0.6119402985074627, "ox": 0.3100775193798449, "oy": 0.6119402985074627, "term": "like", "cat25k": 77, "ncat25k": 41, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 90, "ncat": 40, "s": 0.920940170940171, "os": 0.29943019943019944, "bg": 4.993851167093557e-07}, {"x": 0.1472868217054263, "y": 0.2462686567164179, "ox": 0.1472868217054263, "oy": 0.2462686567164179, "term": "platform", "cat25k": 28, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 19, "s": 0.6944444444444445, "os": 0.09800569800569797, "bg": 2.7378959069877796e-06}, {"x": 0.01550387596899225, "y": 0.21641791044776115, "ox": 0.01550387596899225, "oy": 0.21641791044776115, "term": "net", "cat25k": 25, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 2, "s": 0.8301282051282052, "os": 0.19914529914529913, "bg": 5.301933663095364e-07}, {"x": 0.0, "y": 0.20895522388059698, "ox": 0.0, "oy": 0.20895522388059698, "term": "ocr", "cat25k": 24, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 28, "ncat": 0, "s": 0.8365384615384616, "os": 0.20712250712250713, "bg": 2.4974479204063348e-05}, {"x": 0.31782945736434104, "y": 0.3582089552238806, "ox": 0.31782945736434104, "oy": 0.3582089552238806, "term": "implementing", "cat25k": 41, "ncat25k": 42, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 48, "ncat": 41, "s": 0.6420940170940173, "os": 0.03988603988603989, "bg": 1.304693024788581e-05}, {"x": 0.05426356589147286, "y": 0.2462686567164179, "ox": 0.05426356589147286, "oy": 0.2462686567164179, "term": "successful", "cat25k": 28, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 7, "s": 0.8173076923076924, "os": 0.19031339031339028, "bg": 1.856295574231694e-06}, {"x": 0.05426356589147286, "y": 0.04477611940298507, "ox": 0.05426356589147286, "oy": 0.04477611940298507, "term": "report", "cat25k": 5, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 7, "s": 0.5630341880341881, "os": -0.009686609686609692, "bg": 9.081629507281709e-08}, {"x": 0.2403100775193798, "y": 0.26119402985074625, "ox": 0.2403100775193798, "oy": 0.26119402985074625, "term": "best", "cat25k": 30, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 31, "s": 0.6324786324786325, "os": 0.020512820512820495, "bg": 3.549268088084542e-07}, {"x": 0.1705426356589147, "y": 0.31343283582089554, "ox": 0.1705426356589147, "oy": 0.31343283582089554, "term": "bs", "cat25k": 36, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 22, "s": 0.7606837606837608, "os": 0.1415954415954416, "bg": 9.711048510253083e-06}, {"x": 0.08527131782945736, "y": 0.22388059701492533, "ox": 0.08527131782945736, "oy": 0.22388059701492533, "term": "hbase", "cat25k": 26, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 30, "ncat": 11, "s": 0.7564102564102565, "os": 0.13732193732193732, "bg": 0.0014938968846784477}, {"x": 0.1395348837209302, "y": 0.23134328358208953, "ox": 0.1395348837209302, "oy": 0.23134328358208953, "term": "cassandra", "cat25k": 26, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 18, "s": 0.6805555555555557, "os": 0.09088319088319088, "bg": 8.192685269635484e-05}, {"x": 0.1705426356589147, "y": 0.31343283582089554, "ox": 0.1705426356589147, "oy": 0.31343283582089554, "term": "expert", "cat25k": 36, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 22, "s": 0.7606837606837608, "os": 0.1415954415954416, "bg": 3.404969185427892e-06}, {"x": 0.023255813953488372, "y": 0.2462686567164179, "ox": 0.023255813953488372, "oy": 0.2462686567164179, "term": "previous", "cat25k": 28, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 3, "s": 0.8440170940170941, "os": 0.22108262108262106, "bg": 3.568816343025665e-07}, {"x": 0.007751937984496123, "y": 0.23134328358208953, "ox": 0.007751937984496123, "oy": 0.23134328358208953, "term": "ecommerce", "cat25k": 26, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 1, "s": 0.8450854700854701, "os": 0.22165242165242166, "bg": 5.982272283870784e-06}, {"x": 0.031007751937984492, "y": 0.29850746268656714, "ox": 0.031007751937984492, "oy": 0.29850746268656714, "term": "key", "cat25k": 34, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 4, "s": 0.889957264957265, "os": 0.26524216524216526, "bg": 6.427217372464726e-07}, {"x": 0.05426356589147286, "y": 0.5522388059701492, "ox": 0.05426356589147286, "oy": 0.5522388059701492, "term": "quickly", "cat25k": 65, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 76, "ncat": 7, "s": 0.9807692307692308, "os": 0.49401709401709404, "bg": 4.127660071776528e-06}, {"x": 0.031007751937984492, "y": 0.32835820895522383, "ox": 0.031007751937984492, "oy": 0.32835820895522383, "term": "adapt", "cat25k": 38, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 4, "s": 0.9166666666666667, "os": 0.2948717948717948, "bg": 2.0293992301812253e-05}, {"x": 0.0, "y": 0.32089552238805963, "ox": 0.0, "oy": 0.32089552238805963, "term": "responsive", "cat25k": 37, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 0, "s": 0.9284188034188035, "os": 0.3182336182336183, "bg": 1.9608764114320008e-05}, {"x": 0.1317829457364341, "y": 0.3656716417910447, "ox": 0.1317829457364341, "oy": 0.3656716417910447, "term": "perform", "cat25k": 42, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 17, "s": 0.8547008547008547, "os": 0.23190883190883188, "bg": 4.337274481031143e-06}, {"x": 0.03875968992248062, "y": 0.38805970149253727, "ox": 0.03875968992248062, "oy": 0.38805970149253727, "term": "meet", "cat25k": 44, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 52, "ncat": 5, "s": 0.9455128205128206, "os": 0.34643874643874645, "bg": 1.2214620133135502e-06}, {"x": 0.0, "y": 0.32835820895522383, "ox": 0.0, "oy": 0.32835820895522383, "term": "aggressive", "cat25k": 38, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 0, "s": 0.9326923076923078, "os": 0.3256410256410256, "bg": 1.1890588747638603e-05}, {"x": 0.031007751937984492, "y": 0.4029850746268656, "ox": 0.031007751937984492, "oy": 0.4029850746268656, "term": "deadlines", "cat25k": 46, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 54, "ncat": 4, "s": 0.9508547008547009, "os": 0.3689458689458689, "bg": 2.8619701407694558e-05}, {"x": 0.0697674418604651, "y": 0.33582089552238803, "ox": 0.0697674418604651, "oy": 0.33582089552238803, "term": "members", "cat25k": 38, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 45, "ncat": 9, "s": 0.8878205128205129, "os": 0.2638176638176638, "bg": 4.763772967472746e-07}, {"x": 0.023255813953488372, "y": 0.30597014925373134, "ox": 0.023255813953488372, "oy": 0.30597014925373134, "term": "equal", "cat25k": 35, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 3, "s": 0.907051282051282, "os": 0.28034188034188035, "bg": 2.522411699610462e-06}, {"x": 0.07751937984496123, "y": 0.32089552238805963, "ox": 0.07751937984496123, "oy": 0.32089552238805963, "term": "opportunity", "cat25k": 37, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 10, "s": 0.857905982905983, "os": 0.24131054131054133, "bg": 1.7640748291252218e-06}, {"x": 0.046511627906976744, "y": 0.3731343283582089, "ox": 0.046511627906976744, "oy": 0.3731343283582089, "term": "employer", "cat25k": 43, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 50, "ncat": 6, "s": 0.9305555555555556, "os": 0.3239316239316239, "bg": 4.527827990401976e-06}, {"x": 0.007751937984496123, "y": 0.26119402985074625, "ox": 0.007751937984496123, "oy": 0.26119402985074625, "term": "discriminate", "cat25k": 30, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 1, "s": 0.8675213675213677, "os": 0.2512820512820513, "bg": 4.2007025675044155e-05}, {"x": 0.0, "y": 0.26119402985074625, "ox": 0.0, "oy": 0.26119402985074625, "term": "applicant", "cat25k": 30, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 0, "s": 0.8824786324786326, "os": 0.25897435897435894, "bg": 3.635759250410321e-06}, {"x": 0.007751937984496123, "y": 0.26119402985074625, "ox": 0.007751937984496123, "oy": 0.26119402985074625, "term": "race", "cat25k": 30, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 1, "s": 0.8675213675213677, "os": 0.2512820512820513, "bg": 1.3753030227116584e-06}, {"x": 0.007751937984496123, "y": 0.26119402985074625, "ox": 0.007751937984496123, "oy": 0.26119402985074625, "term": "color", "cat25k": 30, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 1, "s": 0.8675213675213677, "os": 0.2512820512820513, "bg": 6.370523277347906e-07}, {"x": 0.007751937984496123, "y": 0.25373134328358204, "ox": 0.007751937984496123, "oy": 0.25373134328358204, "term": "sex", "cat25k": 29, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 1, "s": 0.8632478632478633, "os": 0.24387464387464386, "bg": 2.1864007665171264e-07}, {"x": 0.0, "y": 0.2761194029850746, "ox": 0.0, "oy": 0.2761194029850746, "term": "age", "cat25k": 32, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 0, "s": 0.9006410256410257, "os": 0.2737891737891738, "bg": 5.573101075199562e-07}, {"x": 0.007751937984496123, "y": 0.32835820895522383, "ox": 0.007751937984496123, "oy": 0.32835820895522383, "term": "national", "cat25k": 38, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 1, "s": 0.9273504273504274, "os": 0.31794871794871793, "bg": 3.3779787776906907e-07}, {"x": 0.007751937984496123, "y": 0.26119402985074625, "ox": 0.007751937984496123, "oy": 0.26119402985074625, "term": "origin", "cat25k": 30, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 1, "s": 0.8675213675213677, "os": 0.2512820512820513, "bg": 2.274326192959899e-06}, {"x": 0.007751937984496123, "y": 0.26119402985074625, "ox": 0.007751937984496123, "oy": 0.26119402985074625, "term": "religion", "cat25k": 30, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 1, "s": 0.8675213675213677, "os": 0.2512820512820513, "bg": 1.79326418589654e-06}, {"x": 0.007751937984496123, "y": 0.26119402985074625, "ox": 0.007751937984496123, "oy": 0.26119402985074625, "term": "sexual", "cat25k": 30, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 1, "s": 0.8675213675213677, "os": 0.2512820512820513, "bg": 2.1396384516209176e-06}, {"x": 0.10852713178294572, "y": 0.28358208955223874, "ox": 0.10852713178294572, "oy": 0.28358208955223874, "term": "orientation", "cat25k": 32, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 14, "s": 0.8055555555555557, "os": 0.17350427350427347, "bg": 8.121380909632147e-06}, {"x": 0.007751937984496123, "y": 0.26865671641791045, "ox": 0.007751937984496123, "oy": 0.26865671641791045, "term": "gender", "cat25k": 31, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 1, "s": 0.8814102564102564, "os": 0.2586894586894587, "bg": 2.5252646758913222e-06}, {"x": 0.046511627906976744, "y": 0.26119402985074625, "ox": 0.046511627906976744, "oy": 0.26119402985074625, "term": "identity", "cat25k": 30, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 6, "s": 0.8397435897435898, "os": 0.2128205128205128, "bg": 3.079418734928217e-06}, {"x": 0.023255813953488372, "y": 0.32835820895522383, "ox": 0.023255813953488372, "oy": 0.32835820895522383, "term": "status", "cat25k": 38, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 44, "ncat": 3, "s": 0.9230769230769232, "os": 0.30256410256410254, "bg": 7.284027292816325e-07}, {"x": 0.007751937984496123, "y": 0.26119402985074625, "ox": 0.007751937984496123, "oy": 0.26119402985074625, "term": "veteran", "cat25k": 30, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 1, "s": 0.8675213675213677, "os": 0.2512820512820513, "bg": 8.052727469978425e-06}, {"x": 0.007751937984496123, "y": 0.5, "ox": 0.007751937984496123, "oy": 0.5, "term": "basis", "cat25k": 59, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 69, "ncat": 1, "s": 0.9797008547008548, "os": 0.48831908831908827, "bg": 2.3350323028368776e-06}, {"x": 0.0, "y": 0.26865671641791045, "ox": 0.0, "oy": 0.26865671641791045, "term": "federal", "cat25k": 31, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 0, "s": 0.8920940170940171, "os": 0.26638176638176636, "bg": 7.355008270298189e-07}, {"x": 0.007751937984496123, "y": 0.26119402985074625, "ox": 0.007751937984496123, "oy": 0.26119402985074625, "term": "state", "cat25k": 30, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 1, "s": 0.8675213675213677, "os": 0.2512820512820513, "bg": 1.5888462358703408e-07}, {"x": 0.0, "y": 0.4104477611940298, "ox": 0.0, "oy": 0.4104477611940298, "term": "local", "cat25k": 47, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 55, "ncat": 0, "s": 0.9604700854700855, "os": 0.4071225071225071, "bg": 4.062070820291886e-07}, {"x": 0.01550387596899225, "y": 0.26865671641791045, "ox": 0.01550387596899225, "oy": 0.26865671641791045, "term": "protected", "cat25k": 31, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 2, "s": 0.8653846153846154, "os": 0.25099715099715103, "bg": 2.5120931669247898e-06}, {"x": 0.007751937984496123, "y": 0.2462686567164179, "ox": 0.007751937984496123, "oy": 0.2462686567164179, "term": "class", "cat25k": 28, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 1, "s": 0.8568376068376069, "os": 0.23646723646723644, "bg": 3.557552360866559e-07}, {"x": 0.07751937984496123, "y": 0.029850746268656716, "ox": 0.07751937984496123, "oy": 0.029850746268656716, "term": "functions", "cat25k": 3, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.3450854700854701, "os": -0.047578347578347585, "bg": 5.397976310248381e-07}, {"x": 0.023255813953488372, "y": 0.14179104477611937, "ox": 0.023255813953488372, "oy": 0.14179104477611937, "term": "analyses", "cat25k": 16, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 3, "s": 0.7254273504273505, "os": 0.11737891737891737, "bg": 4.3864297024295935e-06}, {"x": 0.6434108527131782, "y": 0.35074626865671643, "ox": 0.6434108527131782, "oy": 0.35074626865671643, "term": "level", "cat25k": 40, "ncat25k": 95, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 92, "s": 0.06944444444444445, "os": -0.2905982905982906, "bg": 1.3720357303768168e-06}, {"x": 0.1395348837209302, "y": 0.07462686567164178, "ox": 0.1395348837209302, "oy": 0.07462686567164178, "term": "creating", "cat25k": 9, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 18, "s": 0.25854700854700857, "os": -0.06467236467236467, "bg": 1.4388976112064224e-06}, {"x": 0.3255813953488372, "y": 0.17164179104477612, "ox": 0.3255813953488372, "oy": 0.17164179104477612, "term": "support", "cat25k": 20, "ncat25k": 43, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 42, "s": 0.11324786324786325, "os": -0.15299145299145298, "bg": 3.4799602726664516e-07}, {"x": 0.046511627906976744, "y": 0.15671641791044774, "ox": 0.046511627906976744, "oy": 0.15671641791044774, "term": "network", "cat25k": 18, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 6, "s": 0.7147435897435898, "os": 0.10911680911680913, "bg": 2.397082303381422e-07}, {"x": 0.23255813953488366, "y": 0.10447761194029849, "ox": 0.23255813953488366, "oy": 0.10447761194029849, "term": "integration", "cat25k": 12, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 30, "s": 0.14423076923076925, "os": -0.12735042735042737, "bg": 2.706230859759396e-06}, {"x": 0.15503875968992245, "y": 0.10447761194029849, "ox": 0.15503875968992245, "oy": 0.10447761194029849, "term": "bi", "cat25k": 12, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 20, "s": 0.3376068376068376, "os": -0.05042735042735044, "bg": 5.180983170795225e-06}, {"x": 0.27131782945736427, "y": 0.1791044776119403, "ox": 0.27131782945736427, "oy": 0.1791044776119403, "term": "practices", "cat25k": 20, "ncat25k": 36, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 35, "s": 0.20405982905982908, "os": -0.09173789173789176, "bg": 3.099908473888788e-06}, {"x": 0.0, "y": 0.16417910447761191, "ox": 0.0, "oy": 0.16417910447761191, "term": "efficiencies", "cat25k": 19, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 0, "s": 0.7895299145299145, "os": 0.16267806267806267, "bg": 2.2211015755182106e-05}, {"x": 0.16279069767441856, "y": 0.11194029850746266, "ox": 0.16279069767441856, "oy": 0.11194029850746266, "term": "making", "cat25k": 13, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 21, "s": 0.3365384615384615, "os": -0.05071225071225072, "bg": 5.794601652523816e-07}, {"x": 0.22480620155038755, "y": 0.0373134328358209, "ox": 0.22480620155038755, "oy": 0.0373134328358209, "term": "architecture", "cat25k": 4, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 29, "s": 0.09188034188034187, "os": -0.18632478632478633, "bg": 1.6373977405982005e-06}, {"x": 0.05426356589147286, "y": 0.16417910447761191, "ox": 0.05426356589147286, "oy": 0.16417910447761191, "term": "analyze", "cat25k": 19, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 7, "s": 0.7126068376068376, "os": 0.10883190883190882, "bg": 6.591210938410019e-06}, {"x": 0.6434108527131782, "y": 0.23134328358208953, "ox": 0.6434108527131782, "oy": 0.23134328358208953, "term": "sets", "cat25k": 26, "ncat25k": 95, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 92, "s": 0.05448717948717949, "os": -0.40911680911680914, "bg": 4.8036374548750974e-06}, {"x": 0.2790697674418604, "y": 0.10447761194029849, "ox": 0.2790697674418604, "oy": 0.10447761194029849, "term": "monitoring", "cat25k": 12, "ncat25k": 37, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 36, "s": 0.09935897435897437, "os": -0.1735042735042735, "bg": 2.3979462837012165e-06}, {"x": 0.3410852713178294, "y": 0.2014925373134328, "ox": 0.3410852713178294, "oy": 0.2014925373134328, "term": "sources", "cat25k": 23, "ncat25k": 46, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 45, "s": 0.1324786324786325, "os": -0.13874643874643874, "bg": 2.29985209715048e-06}, {"x": 0.47286821705426346, "y": 0.029850746268656716, "ox": 0.47286821705426346, "oy": 0.029850746268656716, "term": "capable", "cat25k": 3, "ncat25k": 67, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 65, "s": 0.03952991452991453, "os": -0.4398860398860399, "bg": 8.396544201446966e-06}, {"x": 0.12403100775193797, "y": 0.06716417910447761, "ox": 0.12403100775193797, "oy": 0.06716417910447761, "term": "leading", "cat25k": 8, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 16, "s": 0.3055555555555555, "os": -0.056695156695156707, "bg": 8.887881595641382e-07}, {"x": 0.046511627906976744, "y": 0.04477611940298507, "ox": 0.046511627906976744, "oy": 0.04477611940298507, "term": "independent", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 6, "s": 0.6121794871794872, "os": -0.0019943019943019988, "bg": 3.736892557410153e-07}, {"x": 0.023255813953488372, "y": 0.21641791044776115, "ox": 0.023255813953488372, "oy": 0.21641791044776115, "term": "types", "cat25k": 25, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 3, "s": 0.8194444444444445, "os": 0.19145299145299144, "bg": 8.298150364325753e-07}, {"x": 0.0697674418604651, "y": 0.007462686567164179, "ox": 0.0697674418604651, "oy": 0.007462686567164179, "term": "pre", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.27350427350427353, "os": -0.06210826210826211, "bg": 2.821392230385184e-07}, {"x": 0.03875968992248062, "y": 0.022388059701492536, "ox": 0.03875968992248062, "oy": 0.022388059701492536, "term": "core", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 5, "s": 0.5181623931623932, "os": -0.016524216524216526, "bg": 3.118944982551258e-07}, {"x": 0.03875968992248062, "y": 0.01492537313432836, "ox": 0.03875968992248062, "oy": 0.01492537313432836, "term": "contributions", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47970085470085466, "os": -0.023931623931623933, "bg": 6.382807997840787e-07}, {"x": 0.1395348837209302, "y": 0.0373134328358209, "ox": 0.1395348837209302, "oy": 0.0373134328358209, "term": "activities", "cat25k": 4, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 18, "s": 0.18162393162393164, "os": -0.10170940170940171, "bg": 3.4654195053788663e-07}, {"x": 0.062015503875968984, "y": 0.007462686567164179, "ox": 0.062015503875968984, "oy": 0.007462686567164179, "term": "version", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3162393162393162, "os": -0.05441595441595442, "bg": 7.741908478762252e-08}, {"x": 0.17829457364341084, "y": 0.029850746268656716, "ox": 0.17829457364341084, "oy": 0.029850746268656716, "term": "control", "cat25k": 3, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 23, "s": 0.11752136752136752, "os": -0.1475783475783476, "bg": 2.5044601536656057e-07}, {"x": 0.23255813953488366, "y": 0.08208955223880597, "ox": 0.23255813953488366, "oy": 0.08208955223880597, "term": "query", "cat25k": 9, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 11, "ncat": 30, "s": 0.11538461538461539, "os": -0.1495726495726496, "bg": 2.616531122377299e-06}, {"x": 0.08527131782945736, "y": 0.022388059701492536, "ox": 0.08527131782945736, "oy": 0.022388059701492536, "term": "postgres", "cat25k": 3, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 11, "s": 0.26495726495726496, "os": -0.06267806267806268, "bg": 3.499715648103592e-05}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "looker", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 5.012112605463202e-05}, {"x": 0.08527131782945736, "y": 0.5746268656716417, "ox": 0.08527131782945736, "oy": 0.5746268656716417, "term": "full", "cat25k": 68, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 80, "ncat": 11, "s": 0.9775641025641026, "os": 0.4854700854700854, "bg": 5.871268521339959e-07}, {"x": 0.01550387596899225, "y": 0.26865671641791045, "ox": 0.01550387596899225, "oy": 0.26865671641791045, "term": "student", "cat25k": 31, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 2, "s": 0.8653846153846154, "os": 0.25099715099715103, "bg": 5.290819206219509e-07}, {"x": 0.0, "y": 0.2761194029850746, "ox": 0.0, "oy": 0.2761194029850746, "term": "gpa", "cat25k": 32, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 0, "s": 0.9006410256410257, "os": 0.2737891737891738, "bg": 2.349305542107016e-05}, {"x": 0.0, "y": 0.28358208955223874, "ox": 0.0, "oy": 0.28358208955223874, "term": "point", "cat25k": 32, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 0, "s": 0.9081196581196581, "os": 0.2811965811965812, "bg": 4.328443524855227e-07}, {"x": 0.09302325581395349, "y": 0.26865671641791045, "ox": 0.09302325581395349, "oy": 0.26865671641791045, "term": "availability", "cat25k": 31, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 12, "s": 0.8076923076923078, "os": 0.17407407407407408, "bg": 1.190493847106165e-06}, {"x": 0.09302325581395349, "y": 0.29104477611940294, "ox": 0.09302325581395349, "oy": 0.29104477611940294, "term": "two", "cat25k": 33, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 12, "s": 0.8237179487179488, "os": 0.19629629629629627, "bg": 2.3105499718510453e-07}, {"x": 0.10077519379844961, "y": 0.29850746268656714, "ox": 0.10077519379844961, "oy": 0.29850746268656714, "term": "day", "cat25k": 34, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 13, "s": 0.8226495726495727, "os": 0.196011396011396, "bg": 2.3751316545559227e-07}, {"x": 0.0, "y": 0.2462686567164179, "ox": 0.0, "oy": 0.2462686567164179, "term": "tours", "cat25k": 28, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 0, "s": 0.8643162393162395, "os": 0.24415954415954413, "bg": 1.7362975475218065e-06}, {"x": 0.0, "y": 0.25373134328358204, "ox": 0.0, "oy": 0.25373134328358204, "term": "attending", "cat25k": 29, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 0, "s": 0.8771367521367522, "os": 0.2515669515669515, "bg": 6.990893333360749e-06}, {"x": 0.0, "y": 0.26865671641791045, "ox": 0.0, "oy": 0.26865671641791045, "term": "internship", "cat25k": 31, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 0, "s": 0.8920940170940171, "os": 0.26638176638176636, "bg": 1.3750778972079617e-05}, {"x": 0.09302325581395349, "y": 0.29104477611940294, "ox": 0.09302325581395349, "oy": 0.29104477611940294, "term": "initiative", "cat25k": 33, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 12, "s": 0.8237179487179488, "os": 0.19629629629629627, "bg": 4.660995301625344e-06}, {"x": 0.08527131782945736, "y": 0.29104477611940294, "ox": 0.08527131782945736, "oy": 0.29104477611940294, "term": "integrity", "cat25k": 33, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 39, "ncat": 11, "s": 0.8344017094017095, "os": 0.20398860398860397, "bg": 7.5177021211271706e-06}, {"x": 0.12403100775193797, "y": 0.38805970149253727, "ox": 0.12403100775193797, "oy": 0.38805970149253727, "term": "leadership", "cat25k": 44, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 52, "ncat": 16, "s": 0.8867521367521368, "os": 0.26182336182336186, "bg": 3.7606953415077765e-06}, {"x": 0.023255813953488372, "y": 0.32089552238805963, "ox": 0.023255813953488372, "oy": 0.32089552238805963, "term": "abilities", "cat25k": 37, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 3, "s": 0.9177350427350428, "os": 0.2951566951566952, "bg": 1.2052043869439684e-05}, {"x": 0.0697674418604651, "y": 0.34328358208955223, "ox": 0.0697674418604651, "oy": 0.34328358208955223, "term": "interest", "cat25k": 39, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 46, "ncat": 9, "s": 0.8974358974358975, "os": 0.2712250712250712, "bg": 9.14169065363188e-07}, {"x": 0.01550387596899225, "y": 0.30597014925373134, "ox": 0.01550387596899225, "oy": 0.30597014925373134, "term": "stem", "cat25k": 35, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 41, "ncat": 2, "s": 0.9134615384615384, "os": 0.288034188034188, "bg": 7.940695928131161e-06}, {"x": 0.031007751937984492, "y": 0.3656716417910447, "ox": 0.031007751937984492, "oy": 0.3656716417910447, "term": "finance", "cat25k": 42, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 49, "ncat": 4, "s": 0.936965811965812, "os": 0.3319088319088319, "bg": 1.0972055148530263e-06}, {"x": 0.0, "y": 0.26119402985074625, "ox": 0.0, "oy": 0.26119402985074625, "term": "psychological", "cat25k": 30, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 0, "s": 0.8824786324786326, "os": 0.25897435897435894, "bg": 6.853308361183058e-06}, {"x": 0.0, "y": 0.26119402985074625, "ox": 0.0, "oy": 0.26119402985074625, "term": "exam", "cat25k": 30, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 0, "s": 0.8824786324786326, "os": 0.25897435897435894, "bg": 3.7996558705958797e-06}, {"x": 0.01550387596899225, "y": 0.26119402985074625, "ox": 0.01550387596899225, "oy": 0.26119402985074625, "term": "polygraph", "cat25k": 30, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 2, "s": 0.8611111111111113, "os": 0.24358974358974358, "bg": 0.0001558560816517375}, {"x": 0.007751937984496123, "y": 0.26119402985074625, "ox": 0.007751937984496123, "oy": 0.26119402985074625, "term": "interview", "cat25k": 30, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 1, "s": 0.8675213675213677, "os": 0.2512820512820513, "bg": 2.216364111930451e-06}, {"x": 0.023255813953488372, "y": 0.32089552238805963, "ox": 0.023255813953488372, "oy": 0.32089552238805963, "term": "comprehensive", "cat25k": 37, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 3, "s": 0.9177350427350428, "os": 0.2951566951566952, "bg": 2.446854454235231e-06}, {"x": 0.007751937984496123, "y": 0.31343283582089554, "ox": 0.007751937984496123, "oy": 0.31343283582089554, "term": "investigation", "cat25k": 36, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 1, "s": 0.9241452991452992, "os": 0.30313390313390315, "bg": 3.5167002163956496e-06}, {"x": 0.1705426356589147, "y": 0.0373134328358209, "ox": 0.1705426356589147, "oy": 0.0373134328358209, "term": "modelling", "cat25k": 4, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 22, "s": 0.13675213675213677, "os": -0.13247863247863248, "bg": 8.19345288491476e-06}, {"x": 0.10852713178294572, "y": 0.29850746268656714, "ox": 0.10852713178294572, "oy": 0.29850746268656714, "term": "people", "cat25k": 34, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 40, "ncat": 14, "s": 0.8162393162393162, "os": 0.1883190883190883, "bg": 2.248321819805671e-07}, {"x": 0.3565891472868217, "y": 0.11194029850746266, "ox": 0.3565891472868217, "oy": 0.11194029850746266, "term": "nosql", "cat25k": 13, "ncat25k": 50, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 48, "s": 0.07692307692307693, "os": -0.243019943019943, "bg": 0.0022945804195804195}, {"x": 0.062015503875968984, "y": 0.0373134328358209, "ox": 0.062015503875968984, "oy": 0.0373134328358209, "term": "events", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 8, "s": 0.46260683760683763, "os": -0.02478632478632479, "bg": 1.2916667088942322e-07}, {"x": 0.07751937984496123, "y": 0.14179104477611937, "ox": 0.07751937984496123, "oy": 0.14179104477611937, "term": "desired", "cat25k": 16, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 10, "s": 0.6570512820512822, "os": 0.06353276353276352, "bg": 3.7813158144537014e-06}, {"x": 0.062015503875968984, "y": 0.19402985074626863, "ox": 0.062015503875968984, "oy": 0.19402985074626863, "term": "organization", "cat25k": 22, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 8, "s": 0.7446581196581197, "os": 0.13076923076923078, "bg": 8.987698946185689e-07}, {"x": 0.11627906976744183, "y": 0.11940298507462686, "ox": 0.11627906976744183, "oy": 0.11940298507462686, "term": "provide", "cat25k": 14, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 15, "s": 0.6239316239316239, "os": 0.0028490028490028296, "bg": 3.4236009153913686e-07}, {"x": 0.023255813953488372, "y": 0.11194029850746266, "ox": 0.023255813953488372, "oy": 0.11194029850746266, "term": "transportation", "cat25k": 13, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 3, "s": 0.6784188034188035, "os": 0.08774928774928775, "bg": 6.769754100153283e-07}, {"x": 0.1395348837209302, "y": 0.26865671641791045, "ox": 0.1395348837209302, "oy": 0.26865671641791045, "term": "writing", "cat25k": 31, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 18, "s": 0.7425213675213675, "os": 0.12792022792022792, "bg": 1.2977396979294564e-06}, {"x": 0.27131782945736427, "y": 0.17164179104477612, "ox": 0.27131782945736427, "oy": 0.17164179104477612, "term": "appropriate", "cat25k": 20, "ncat25k": 36, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 35, "s": 0.19230769230769235, "os": -0.09914529914529915, "bg": 1.66365230079958e-06}, {"x": 0.062015503875968984, "y": 0.2761194029850746, "ox": 0.062015503875968984, "oy": 0.2761194029850746, "term": "include", "cat25k": 32, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 37, "ncat": 8, "s": 0.8386752136752137, "os": 0.21225071225071226, "bg": 4.927884004005844e-07}, {"x": 0.5581395348837208, "y": 0.029850746268656716, "ox": 0.5581395348837208, "oy": 0.029850746268656716, "term": "operating", "cat25k": 3, "ncat25k": 83, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 80, "s": 0.014957264957264956, "os": -0.5245014245014246, "bg": 2.6208290324956907e-06}, {"x": 0.09302325581395349, "y": 0.04477611940298507, "ox": 0.09302325581395349, "oy": 0.04477611940298507, "term": "hours", "cat25k": 5, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 12, "s": 0.3418803418803419, "os": -0.048148148148148155, "bg": 1.8154515715660767e-07}, {"x": 0.046511627906976744, "y": 0.04477611940298507, "ox": 0.046511627906976744, "oy": 0.04477611940298507, "term": "individuals", "cat25k": 5, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 6, "s": 0.6121794871794872, "os": -0.0019943019943019988, "bg": 5.276214094786e-07}, {"x": 0.12403100775193797, "y": 0.19402985074626863, "ox": 0.12403100775193797, "oy": 0.19402985074626863, "term": "different", "cat25k": 22, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 16, "s": 0.6613247863247864, "os": 0.06923076923076923, "bg": 4.670581781845298e-07}, {"x": 0.046511627906976744, "y": 0.029850746268656716, "ox": 0.046511627906976744, "oy": 0.029850746268656716, "term": "assistance", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 6, "s": 0.513888888888889, "os": -0.016809116809116814, "bg": 3.643924448328058e-07}, {"x": 0.20930232558139533, "y": 0.09701492537313433, "ox": 0.20930232558139533, "oy": 0.09701492537313433, "term": "access", "cat25k": 11, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 27, "s": 0.1623931623931624, "os": -0.1116809116809117, "bg": 3.6690200326796866e-07}, {"x": 0.05426356589147286, "y": 0.022388059701492536, "ox": 0.05426356589147286, "oy": 0.022388059701492536, "term": "components", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.41346153846153844, "os": -0.03190883190883191, "bg": 3.6350685605737376e-07}, {"x": 0.19379844961240308, "y": 0.16417910447761191, "ox": 0.19379844961240308, "oy": 0.16417910447761191, "term": "health", "cat25k": 19, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 25, "s": 0.45085470085470086, "os": -0.029629629629629645, "bg": 2.134077617270193e-07}, {"x": 0.20930232558139533, "y": 0.16417910447761191, "ox": 0.20930232558139533, "oy": 0.16417910447761191, "term": "effective", "cat25k": 19, "ncat25k": 28, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 27, "s": 0.38782051282051283, "os": -0.04501424501424503, "bg": 1.289207021915993e-06}, {"x": 0.0697674418604651, "y": 0.05223880597014925, "ox": 0.0697674418604651, "oy": 0.05223880597014925, "term": "communicating", "cat25k": 6, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 9, "s": 0.5042735042735044, "os": -0.017663817663817666, "bg": 7.639127611626753e-06}, {"x": 0.05426356589147286, "y": 0.13432835820895522, "ox": 0.05426356589147286, "oy": 0.13432835820895522, "term": "translate", "cat25k": 15, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 7, "s": 0.6666666666666667, "os": 0.07920227920227921, "bg": 4.624945159712769e-06}, {"x": 0.1705426356589147, "y": 0.25373134328358204, "ox": 0.1705426356589147, "oy": 0.25373134328358204, "term": "build", "cat25k": 29, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 34, "ncat": 22, "s": 0.670940170940171, "os": 0.08233618233618231, "bg": 1.4496884334796224e-06}, {"x": 0.046511627906976744, "y": 0.029850746268656716, "ox": 0.046511627906976744, "oy": 0.029850746268656716, "term": "task", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 6, "s": 0.513888888888889, "os": -0.016809116809116814, "bg": 5.01409210586353e-07}, {"x": 0.23255813953488366, "y": 0.26119402985074625, "ox": 0.23255813953488366, "oy": 0.26119402985074625, "term": "oriented", "cat25k": 30, "ncat25k": 31, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 35, "ncat": 30, "s": 0.6367521367521368, "os": 0.028205128205128188, "bg": 7.719385836162092e-06}, {"x": 0.12403100775193797, "y": 0.007462686567164179, "ox": 0.12403100775193797, "oy": 0.007462686567164179, "term": "optimizing", "cat25k": 1, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 16, "s": 0.15705128205128208, "os": -0.11595441595441597, "bg": 1.2250667391137505e-05}, {"x": 0.10852713178294572, "y": 0.029850746268656716, "ox": 0.10852713178294572, "oy": 0.029850746268656716, "term": "ecosystem", "cat25k": 3, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 14, "s": 0.22329059829059833, "os": -0.07834757834757836, "bg": 7.71579966440558e-06}, {"x": 0.21705426356589144, "y": 0.0373134328358209, "ox": 0.21705426356589144, "oy": 0.0373134328358209, "term": "improve", "cat25k": 4, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 28, "s": 0.09615384615384616, "os": -0.17863247863247864, "bg": 1.075610601322388e-06}, {"x": 0.1472868217054263, "y": 0.28358208955223874, "ox": 0.1472868217054263, "oy": 0.28358208955223874, "term": "effectively", "cat25k": 32, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 38, "ncat": 19, "s": 0.7532051282051283, "os": 0.135042735042735, "bg": 5.827411396893387e-06}, {"x": 0.046511627906976744, "y": 0.01492537313432836, "ox": 0.046511627906976744, "oy": 0.01492537313432836, "term": "sharing", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.4198717948717949, "os": -0.03162393162393162, "bg": 5.291725553650093e-07}, {"x": 0.5426356589147285, "y": 0.05223880597014925, "ox": 0.5426356589147285, "oy": 0.05223880597014925, "term": "others", "cat25k": 6, "ncat25k": 78, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 76, "s": 0.020299145299145296, "os": -0.48689458689458687, "bg": 1.4894217944363564e-06}, {"x": 0.0, "y": 0.11194029850746266, "ox": 0.0, "oy": 0.11194029850746266, "term": "recommender", "cat25k": 13, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.7190170940170941, "os": 0.11082621082621083, "bg": 0.00013526979560733886}, {"x": 0.09302325581395349, "y": 0.0373134328358209, "ox": 0.09302325581395349, "oy": 0.0373134328358209, "term": "unit", "cat25k": 4, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 12, "s": 0.3098290598290598, "os": -0.05555555555555556, "bg": 3.6259230333635997e-07}, {"x": 0.031007751937984492, "y": 0.15671641791044774, "ox": 0.031007751937984492, "oy": 0.15671641791044774, "term": "excel", "cat25k": 18, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 4, "s": 0.7350427350427351, "os": 0.12450142450142451, "bg": 3.5240199312928884e-06}, {"x": 0.11627906976744183, "y": 0.0373134328358209, "ox": 0.11627906976744183, "oy": 0.0373134328358209, "term": "employees", "cat25k": 4, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 15, "s": 0.22115384615384617, "os": -0.07863247863247863, "bg": 7.196985283622483e-07}, {"x": 0.031007751937984492, "y": 0.022388059701492536, "ox": 0.031007751937984492, "oy": 0.022388059701492536, "term": "strategic", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 4, "s": 0.5801282051282052, "os": -0.008831908831908833, "bg": 4.5182250514343445e-07}, {"x": 0.11627906976744183, "y": 0.11940298507462686, "ox": 0.11627906976744183, "oy": 0.11940298507462686, "term": "tasks", "cat25k": 14, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 15, "s": 0.6239316239316239, "os": 0.0028490028490028296, "bg": 3.049942363468223e-06}, {"x": 0.10077519379844961, "y": 0.029850746268656716, "ox": 0.10077519379844961, "oy": 0.029850746268656716, "term": "limited", "cat25k": 3, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 13, "s": 0.2371794871794872, "os": -0.07065527065527066, "bg": 3.201210132752772e-07}, {"x": 0.0, "y": 0.10447761194029849, "ox": 0.0, "oy": 0.10447761194029849, "term": "marketing", "cat25k": 12, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 14, "ncat": 0, "s": 0.7061965811965812, "os": 0.10341880341880341, "bg": 2.553554513227029e-07}, {"x": 0.046511627906976744, "y": 0.029850746268656716, "ox": 0.046511627906976744, "oy": 0.029850746268656716, "term": "direct", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 6, "s": 0.513888888888889, "os": -0.016809116809116814, "bg": 1.9552454292985258e-07}, {"x": 0.1395348837209302, "y": 0.19402985074626863, "ox": 0.1395348837209302, "oy": 0.19402985074626863, "term": "extensive", "cat25k": 22, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 18, "s": 0.6485042735042736, "os": 0.05384615384615385, "bg": 4.0536229831210364e-06}, {"x": 0.07751937984496123, "y": 0.05223880597014925, "ox": 0.07751937984496123, "oy": 0.05223880597014925, "term": "php", "cat25k": 6, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 10, "s": 0.45726495726495725, "os": -0.02535612535612536, "bg": 6.575477302777116e-07}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "html", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 1.4431944763367037e-07}, {"x": 0.10852713178294572, "y": 0.0373134328358209, "ox": 0.10852713178294572, "oy": 0.0373134328358209, "term": "mongodb", "cat25k": 4, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 14, "s": 0.23611111111111113, "os": -0.07094017094017094, "bg": 0.0006925712619377415}, {"x": 0.8759689922480619, "y": 0.04477611940298507, "ox": 0.8759689922480619, "oy": 0.04477611940298507, "term": "apache", "cat25k": 5, "ncat25k": 152, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 147, "s": 0.0, "os": -0.8250712250712251, "bg": 1.842444280511058e-05}, {"x": 0.023255813953488372, "y": 0.2014925373134328, "ox": 0.023255813953488372, "oy": 0.2014925373134328, "term": "packages", "cat25k": 23, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 3, "s": 0.8098290598290598, "os": 0.17663817663817663, "bg": 1.3158935331596618e-06}, {"x": 0.15503875968992245, "y": 0.007462686567164179, "ox": 0.15503875968992245, "oy": 0.007462686567164179, "term": "ownership", "cat25k": 1, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 20, "s": 0.1185897435897436, "os": -0.14672364672364674, "bg": 2.2623895724956346e-06}, {"x": 0.21705426356589144, "y": 0.35074626865671643, "ox": 0.21705426356589144, "oy": 0.35074626865671643, "term": "scripting", "cat25k": 40, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 28, "s": 0.748931623931624, "os": 0.13247863247863248, "bg": 2.0506639776537778e-05}, {"x": 0.08527131782945736, "y": 0.0373134328358209, "ox": 0.08527131782945736, "oy": 0.0373134328358209, "term": "automation", "cat25k": 4, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.34294871794871795, "os": -0.04786324786324787, "bg": 2.5301675034141452e-06}, {"x": 0.2403100775193798, "y": 0.17164179104477612, "ox": 0.2403100775193798, "oy": 0.17164179104477612, "term": "across", "cat25k": 20, "ncat25k": 32, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 31, "s": 0.25427350427350426, "os": -0.06837606837606838, "bg": 1.4089642007505761e-06}, {"x": 0.0, "y": 0.11940298507462686, "ox": 0.0, "oy": 0.11940298507462686, "term": "valued", "cat25k": 14, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 0, "s": 0.7307692307692308, "os": 0.11823361823361822, "bg": 4.990959345048882e-06}, {"x": 0.1395348837209302, "y": 0.23134328358208953, "ox": 0.1395348837209302, "oy": 0.23134328358208953, "term": "role", "cat25k": 26, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 18, "s": 0.6805555555555557, "os": 0.09088319088319088, "bg": 1.3289234770604754e-06}, {"x": 0.11627906976744183, "y": 0.11940298507462686, "ox": 0.11627906976744183, "oy": 0.11940298507462686, "term": "cross", "cat25k": 14, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 15, "s": 0.6239316239316239, "os": 0.0028490028490028296, "bg": 8.34613769958745e-07}, {"x": 0.1395348837209302, "y": 0.09701492537313433, "ox": 0.1395348837209302, "oy": 0.09701492537313433, "term": "functional", "cat25k": 11, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 13, "ncat": 18, "s": 0.3888888888888889, "os": -0.04245014245014246, "bg": 1.479084657485861e-06}, {"x": 0.2015503875968992, "y": 0.13432835820895522, "ox": 0.2015503875968992, "oy": 0.13432835820895522, "term": "company", "cat25k": 15, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 26, "s": 0.2553418803418803, "os": -0.06695156695156695, "bg": 2.7133096852566626e-07}, {"x": 0.12403100775193797, "y": 0.029850746268656716, "ox": 0.12403100775193797, "oy": 0.029850746268656716, "term": "compensation", "cat25k": 3, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 16, "s": 0.1997863247863248, "os": -0.09373219373219374, "bg": 1.6100087805853871e-06}, {"x": 0.05426356589147286, "y": 0.01492537313432836, "ox": 0.05426356589147286, "oy": 0.01492537313432836, "term": "equity", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.39743589743589747, "os": -0.039316239316239315, "bg": 5.766738503141783e-07}, {"x": 0.023255813953488372, "y": 0.11940298507462686, "ox": 0.023255813953488372, "oy": 0.11940298507462686, "term": "qualifications", "cat25k": 14, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 3, "s": 0.6891025641025641, "os": 0.09515669515669514, "bg": 3.417721370984751e-06}, {"x": 0.10852713178294572, "y": 0.19402985074626863, "ox": 0.10852713178294572, "oy": 0.19402985074626863, "term": "education", "cat25k": 22, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 14, "s": 0.6741452991452992, "os": 0.08461538461538462, "bg": 2.998579906290405e-07}, {"x": 0.19379844961240308, "y": 0.12686567164179102, "ox": 0.19379844961240308, "oy": 0.12686567164179102, "term": "take", "cat25k": 15, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 25, "s": 0.2564102564102564, "os": -0.06666666666666668, "bg": 3.1769481609653124e-07}, {"x": 0.023255813953488372, "y": 0.11194029850746266, "ox": 0.023255813953488372, "oy": 0.11194029850746266, "term": "account", "cat25k": 13, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 3, "s": 0.6784188034188035, "os": 0.08774928774928775, "bg": 1.7675980193003635e-07}, {"x": 0.007751937984496123, "y": 0.11194029850746266, "ox": 0.007751937984496123, "oy": 0.11194029850746266, "term": "hiring", "cat25k": 13, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 1, "s": 0.701923076923077, "os": 0.10313390313390314, "bg": 3.6972779715254137e-06}, {"x": 0.07751937984496123, "y": 0.05970149253731343, "ox": 0.07751937984496123, "oy": 0.05970149253731343, "term": "reporting", "cat25k": 7, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 10, "s": 0.5010683760683762, "os": -0.017948717948717954, "bg": 9.461148686471935e-07}, {"x": 0.031007751937984492, "y": 0.16417910447761191, "ox": 0.031007751937984492, "oy": 0.16417910447761191, "term": "drive", "cat25k": 19, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 4, "s": 0.7467948717948718, "os": 0.1319088319088319, "bg": 4.821321545375228e-07}, {"x": 0.5968992248062014, "y": 0.18656716417910446, "ox": 0.5968992248062014, "oy": 0.18656716417910446, "term": "datasets", "cat25k": 21, "ncat25k": 89, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 86, "s": 0.05555555555555555, "os": -0.4074074074074074, "bg": 0.00011633677505026849}, {"x": 0.0697674418604651, "y": 0.029850746268656716, "ox": 0.0697674418604651, "oy": 0.029850746268656716, "term": "responsible", "cat25k": 3, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.39316239316239315, "os": -0.03988603988603989, "bg": 3.9770845892698777e-07}, {"x": 0.05426356589147286, "y": 0.0373134328358209, "ox": 0.05426356589147286, "oy": 0.0373134328358209, "term": "willingness", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.5074786324786326, "os": -0.017094017094017096, "bg": 6.469381093179844e-06}, {"x": 0.046511627906976744, "y": 0.01492537313432836, "ox": 0.046511627906976744, "oy": 0.01492537313432836, "term": "number", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.4198717948717949, "os": -0.03162393162393162, "bg": 4.168374593025306e-08}, {"x": 0.046511627906976744, "y": 0.022388059701492536, "ox": 0.046511627906976744, "oy": 0.022388059701492536, "term": "judgment", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.4700854700854701, "os": -0.02421652421652422, "bg": 1.1576089766659471e-06}, {"x": 0.046511627906976744, "y": 0.022388059701492536, "ox": 0.046511627906976744, "oy": 0.022388059701492536, "term": "sound", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.4700854700854701, "os": -0.02421652421652422, "bg": 1.798818338245423e-07}, {"x": 0.09302325581395349, "y": 0.029850746268656716, "ox": 0.09302325581395349, "oy": 0.029850746268656716, "term": "supporting", "cat25k": 3, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 12, "s": 0.26175213675213677, "os": -0.06296296296296297, "bg": 1.3041947958389012e-06}, {"x": 0.1472868217054263, "y": 0.06716417910447761, "ox": 0.1472868217054263, "oy": 0.06716417910447761, "term": "internal", "cat25k": 8, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 19, "s": 0.21794871794871795, "os": -0.07977207977207978, "bg": 1.0652438660880413e-06}, {"x": 0.17829457364341084, "y": 0.05970149253731343, "ox": 0.17829457364341084, "oy": 0.05970149253731343, "term": "external", "cat25k": 7, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 23, "s": 0.15064102564102566, "os": -0.11794871794871796, "bg": 1.2238546161782686e-06}, {"x": 0.1395348837209302, "y": 0.029850746268656716, "ox": 0.1395348837209302, "oy": 0.029850746268656716, "term": "maintaining", "cat25k": 3, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 18, "s": 0.1655982905982906, "os": -0.10911680911680913, "bg": 3.279263006507325e-06}, {"x": 0.07751937984496123, "y": 0.05970149253731343, "ox": 0.07751937984496123, "oy": 0.05970149253731343, "term": "positive", "cat25k": 7, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 10, "s": 0.5010683760683762, "os": -0.017948717948717954, "bg": 6.407160414669286e-07}, {"x": 0.3100775193798449, "y": 0.23134328358208953, "ox": 0.3100775193798449, "oy": 0.23134328358208953, "term": "intelligence", "cat25k": 26, "ncat25k": 41, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 31, "ncat": 40, "s": 0.22435897435897437, "os": -0.07834757834757833, "bg": 4.495342698335152e-06}, {"x": 0.046511627906976744, "y": 0.022388059701492536, "ox": 0.046511627906976744, "oy": 0.022388059701492536, "term": "timely", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.4700854700854701, "os": -0.02421652421652422, "bg": 1.8675337556726338e-06}, {"x": 0.07751937984496123, "y": 0.04477611940298507, "ox": 0.07751937984496123, "oy": 0.04477611940298507, "term": "transformation", "cat25k": 5, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 10, "s": 0.4081196581196581, "os": -0.03276353276353277, "bg": 2.7941365742523455e-06}, {"x": 0.05426356589147286, "y": 0.0373134328358209, "ox": 0.05426356589147286, "oy": 0.0373134328358209, "term": "programs", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.5074786324786326, "os": -0.017094017094017096, "bg": 1.6835832527323697e-07}, {"x": 0.03875968992248062, "y": 0.022388059701492536, "ox": 0.03875968992248062, "oy": 0.022388059701492536, "term": "innovation", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 5, "s": 0.5181623931623932, "os": -0.016524216524216526, "bg": 8.725756508091857e-07}, {"x": 0.18604651162790695, "y": 0.04477611940298507, "ox": 0.18604651162790695, "oy": 0.04477611940298507, "term": "continuous", "cat25k": 5, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 24, "s": 0.12500000000000003, "os": -0.14045584045584047, "bg": 2.736247245112071e-06}, {"x": 0.03875968992248062, "y": 0.13432835820895522, "ox": 0.03875968992248062, "oy": 0.13432835820895522, "term": "improvement", "cat25k": 15, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 18, "ncat": 5, "s": 0.6869658119658121, "os": 0.0945868945868946, "bg": 1.236937868879746e-06}, {"x": 0.05426356589147286, "y": 0.022388059701492536, "ox": 0.05426356589147286, "oy": 0.022388059701492536, "term": "capacity", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.41346153846153844, "os": -0.03190883190883191, "bg": 4.099889208693913e-07}, {"x": 0.0697674418604651, "y": 0.007462686567164179, "ox": 0.0697674418604651, "oy": 0.007462686567164179, "term": "stakeholder", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.27350427350427353, "os": -0.06210826210826211, "bg": 7.483355147313588e-06}, {"x": 0.07751937984496123, "y": 0.029850746268656716, "ox": 0.07751937984496123, "oy": 0.029850746268656716, "term": "engagement", "cat25k": 3, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.3450854700854701, "os": -0.047578347578347585, "bg": 2.4101761769923205e-06}, {"x": 0.046511627906976744, "y": 0.01492537313432836, "ox": 0.046511627906976744, "oy": 0.01492537313432836, "term": "long", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.4198717948717949, "os": -0.03162393162393162, "bg": 6.334765973651848e-08}, {"x": 0.09302325581395349, "y": 0.05223880597014925, "ox": 0.09302325581395349, "oy": 0.05223880597014925, "term": "term", "cat25k": 6, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 12, "s": 0.3899572649572649, "os": -0.040740740740740744, "bg": 3.266353264082289e-07}, {"x": 0.03875968992248062, "y": 0.029850746268656716, "ox": 0.03875968992248062, "oy": 0.029850746268656716, "term": "short", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.5726495726495727, "os": -0.009116809116809121, "bg": 1.6852338870574974e-07}, {"x": 0.031007751937984492, "y": 0.022388059701492536, "ox": 0.031007751937984492, "oy": 0.022388059701492536, "term": "physical", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 4, "s": 0.5801282051282052, "os": -0.008831908831908833, "bg": 2.182558278827058e-07}, {"x": 0.062015503875968984, "y": 0.04477611940298507, "ox": 0.062015503875968984, "oy": 0.04477611940298507, "term": "current", "cat25k": 5, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 8, "s": 0.5064102564102565, "os": -0.017378917378917384, "bg": 1.2900728891182353e-07}, {"x": 0.08527131782945736, "y": 0.07462686567164178, "ox": 0.08527131782945736, "oy": 0.07462686567164178, "term": "unix", "cat25k": 9, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 11, "s": 0.55982905982906, "os": -0.010826210826210825, "bg": 1.7859919041837458e-06}, {"x": 0.03875968992248062, "y": 0.022388059701492536, "ox": 0.03875968992248062, "oy": 0.022388059701492536, "term": "solr", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 5, "s": 0.5181623931623932, "os": -0.016524216524216526, "bg": 0.00011924814047430949}, {"x": 0.0697674418604651, "y": 0.022388059701492536, "ox": 0.0697674418604651, "oy": 0.022388059701492536, "term": "interface", "cat25k": 3, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 9, "s": 0.35149572649572647, "os": -0.0472934472934473, "bg": 4.0429383638043137e-07}, {"x": 0.062015503875968984, "y": 0.022388059701492536, "ox": 0.062015503875968984, "oy": 0.022388059701492536, "term": "thrive", "cat25k": 3, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.3952991452991453, "os": -0.039601139601139604, "bg": 1.1716729945884684e-05}, {"x": 0.05426356589147286, "y": 0.029850746268656716, "ox": 0.05426356589147286, "oy": 0.029850746268656716, "term": "start", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.4658119658119658, "os": -0.024501424501424507, "bg": 1.358290748654545e-07}, {"x": 0.007751937984496123, "y": 0.17164179104477612, "ox": 0.007751937984496123, "oy": 0.17164179104477612, "term": "humor", "cat25k": 20, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 1, "s": 0.7884615384615385, "os": 0.1623931623931624, "bg": 2.6460323186387394e-06}, {"x": 0.046511627906976744, "y": 0.0373134328358209, "ox": 0.046511627906976744, "oy": 0.0373134328358209, "term": "online", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.5662393162393163, "os": -0.009401709401709403, "bg": 3.6583018436549243e-08}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "demonstrate", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 8.074296446179162e-07}, {"x": 0.03875968992248062, "y": 0.32089552238805963, "ox": 0.03875968992248062, "oy": 0.32089552238805963, "term": "us", "cat25k": 37, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 43, "ncat": 5, "s": 0.9059829059829061, "os": 0.2797720797720798, "bg": 7.810164075772329e-08}, {"x": 0.062015503875968984, "y": 0.0373134328358209, "ox": 0.062015503875968984, "oy": 0.0373134328358209, "term": "offer", "cat25k": 4, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 8, "s": 0.46260683760683763, "os": -0.02478632478632479, "bg": 2.0588542491940577e-07}, {"x": 0.046511627906976744, "y": 0.01492537313432836, "ox": 0.046511627906976744, "oy": 0.01492537313432836, "term": "defined", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.4198717948717949, "os": -0.03162393162393162, "bg": 3.131849084805188e-07}, {"x": 0.05426356589147286, "y": 0.022388059701492536, "ox": 0.05426356589147286, "oy": 0.022388059701492536, "term": "vacation", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.41346153846153844, "os": -0.03190883190883191, "bg": 3.636279605247667e-07}, {"x": 0.01550387596899225, "y": 0.18656716417910446, "ox": 0.01550387596899225, "oy": 0.18656716417910446, "term": "want", "cat25k": 21, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 2, "s": 0.8012820512820513, "os": 0.16951566951566951, "bg": 2.0869950676306402e-07}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "www", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 2.8482305403371037e-07}, {"x": 0.05426356589147286, "y": 0.01492537313432836, "ox": 0.05426356589147286, "oy": 0.01492537313432836, "term": "com", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.39743589743589747, "os": -0.039316239316239315, "bg": 2.488248004673925e-07}, {"x": 0.19379844961240308, "y": 0.11194029850746266, "ox": 0.19379844961240308, "oy": 0.11194029850746266, "term": "opportunities", "cat25k": 13, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 25, "s": 0.21688034188034191, "os": -0.08148148148148149, "bg": 1.1414138739312943e-06}, {"x": 0.031007751937984492, "y": 0.022388059701492536, "ox": 0.031007751937984492, "oy": 0.022388059701492536, "term": "stitch", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 4, "s": 0.5801282051282052, "os": -0.008831908831908833, "bg": 2.7427425074152e-06}, {"x": 0.046511627906976744, "y": 0.0373134328358209, "ox": 0.046511627906976744, "oy": 0.0373134328358209, "term": "cyber", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.5662393162393163, "os": -0.009401709401709403, "bg": 2.354449535049727e-06}, {"x": 0.10852713178294572, "y": 0.05970149253731343, "ox": 0.10852713178294572, "oy": 0.05970149253731343, "term": "specific", "cat25k": 7, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 14, "s": 0.3386752136752137, "os": -0.048717948717948725, "bg": 4.0800941018066936e-07}, {"x": 0.05426356589147286, "y": 0.0373134328358209, "ox": 0.05426356589147286, "oy": 0.0373134328358209, "term": "elastic", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.5074786324786326, "os": -0.017094017094017096, "bg": 5.55556069959324e-06}, {"x": 0.1395348837209302, "y": 0.04477611940298507, "ox": 0.1395348837209302, "oy": 0.04477611940298507, "term": "hdfs", "cat25k": 5, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 18, "s": 0.19658119658119658, "os": -0.09430199430199432, "bg": 0.0004720878083323498}, {"x": 0.10077519379844961, "y": 0.029850746268656716, "ox": 0.10077519379844961, "oy": 0.029850746268656716, "term": "impala", "cat25k": 3, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 13, "s": 0.2371794871794872, "os": -0.07065527065527066, "bg": 3.11500507104502e-05}, {"x": 0.046511627906976744, "y": 0.0373134328358209, "ox": 0.046511627906976744, "oy": 0.0373134328358209, "term": "pyspark", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.5662393162393163, "os": -0.009401709401709403, "bg": 0.00040102078016769963}, {"x": 0.0697674418604651, "y": 0.04477611940298507, "ox": 0.0697674418604651, "oy": 0.04477611940298507, "term": "ruby", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 9, "s": 0.45833333333333337, "os": -0.025071225071225077, "bg": 2.7559121665070687e-06}, {"x": 0.031007751937984492, "y": 0.022388059701492536, "ox": 0.031007751937984492, "oy": 0.022388059701492536, "term": "healthy", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 4, "s": 0.5801282051282052, "os": -0.008831908831908833, "bg": 4.602919454268664e-07}, {"x": 0.0697674418604651, "y": 0.05970149253731343, "ox": 0.0697674418604651, "oy": 0.05970149253731343, "term": "either", "cat25k": 7, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 9, "s": 0.561965811965812, "os": -0.010256410256410262, "bg": 3.4274927073288067e-07}, {"x": 0.1317829457364341, "y": 0.0373134328358209, "ox": 0.1317829457364341, "oy": 0.0373134328358209, "term": "service", "cat25k": 4, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 17, "s": 0.19764957264957267, "os": -0.09401709401709402, "bg": 8.4681812122957e-08}, {"x": 0.2015503875968992, "y": 0.2462686567164179, "ox": 0.2015503875968992, "oy": 0.2462686567164179, "term": "requirements", "cat25k": 28, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 33, "ncat": 26, "s": 0.6431623931623932, "os": 0.04415954415954412, "bg": 1.2128869443410294e-06}, {"x": 0.09302325581395349, "y": 0.07462686567164178, "ox": 0.09302325581395349, "oy": 0.07462686567164178, "term": "planning", "cat25k": 9, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 12, "s": 0.49999999999999994, "os": -0.018518518518518517, "bg": 4.2334749097144127e-07}, {"x": 0.03875968992248062, "y": 0.01492537313432836, "ox": 0.03875968992248062, "oy": 0.01492537313432836, "term": "workplace", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47970085470085466, "os": -0.023931623931623933, "bg": 9.087554367105389e-07}, {"x": 0.031007751937984492, "y": 0.022388059701492536, "ox": 0.031007751937984492, "oy": 0.022388059701492536, "term": "sponsored", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 4, "s": 0.5801282051282052, "os": -0.008831908831908833, "bg": 1.8203098076277093e-07}, {"x": 0.18604651162790695, "y": 0.17164179104477612, "ox": 0.18604651162790695, "oy": 0.17164179104477612, "term": "implementation", "cat25k": 20, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 24, "s": 0.5587606837606838, "os": -0.014529914529914534, "bg": 2.0012966699189866e-06}, {"x": 0.10077519379844961, "y": 0.01492537313432836, "ox": 0.10077519379844961, "oy": 0.01492537313432836, "term": "administration", "cat25k": 2, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 13, "s": 0.20940170940170943, "os": -0.08547008547008547, "bg": 3.8674725946702183e-07}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "acquisition", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 6.240617945989012e-07}, {"x": 0.2790697674418604, "y": 0.0373134328358209, "ox": 0.2790697674418604, "oy": 0.0373134328358209, "term": "storage", "cat25k": 4, "ncat25k": 37, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 36, "s": 0.07905982905982906, "os": -0.24017094017094015, "bg": 1.007676021246775e-06}, {"x": 0.0, "y": 0.1791044776119403, "ox": 0.0, "oy": 0.1791044776119403, "term": "improved", "cat25k": 20, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 0, "s": 0.813034188034188, "os": 0.17749287749287748, "bg": 1.7484841280078345e-06}, {"x": 0.28682170542635654, "y": 0.14179104477611937, "ox": 0.28682170542635654, "oy": 0.14179104477611937, "term": "manage", "cat25k": 16, "ncat25k": 38, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 37, "s": 0.12286324786324786, "os": -0.14415954415954418, "bg": 3.0689690510912104e-06}, {"x": 0.11627906976744183, "y": 0.01492537313432836, "ox": 0.11627906976744183, "oy": 0.01492537313432836, "term": "changes", "cat25k": 2, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 15, "s": 0.1826923076923077, "os": -0.10085470085470086, "bg": 2.60286848202402e-07}, {"x": 0.0697674418604651, "y": 0.04477611940298507, "ox": 0.0697674418604651, "oy": 0.04477611940298507, "term": "group", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 9, "s": 0.45833333333333337, "os": -0.025071225071225077, "bg": 9.319726797303721e-08}, {"x": 0.007751937984496123, "y": 0.11194029850746266, "ox": 0.007751937984496123, "oy": 0.11194029850746266, "term": "necessary", "cat25k": 13, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 1, "s": 0.701923076923077, "os": 0.10313390313390314, "bg": 4.404118015498917e-07}, {"x": 0.07751937984496123, "y": 0.07462686567164178, "ox": 0.07751937984496123, "oy": 0.07462686567164178, "term": "form", "cat25k": 9, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 10, "ncat": 10, "s": 0.6068376068376069, "os": -0.003133903133903132, "bg": 1.9856037670795243e-07}, {"x": 0.05426356589147286, "y": 0.0373134328358209, "ox": 0.05426356589147286, "oy": 0.0373134328358209, "term": "personnel", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.5074786324786326, "os": -0.017094017094017096, "bg": 7.200126434220184e-07}, {"x": 0.19379844961240308, "y": 0.16417910447761191, "ox": 0.19379844961240308, "oy": 0.16417910447761191, "term": "program", "cat25k": 19, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 25, "s": 0.45085470085470086, "os": -0.029629629629629645, "bg": 3.06446580774841e-07}, {"x": 0.01550387596899225, "y": 0.16417910447761191, "ox": 0.01550387596899225, "oy": 0.16417910447761191, "term": "numbers", "cat25k": 19, "ncat25k": 2, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 2, "s": 0.763888888888889, "os": 0.1472934472934473, "bg": 7.369647856577073e-07}, {"x": 0.03875968992248062, "y": 0.01492537313432836, "ox": 0.03875968992248062, "oy": 0.01492537313432836, "term": "part", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47970085470085466, "os": -0.023931623931623933, "bg": 4.623755762599192e-08}, {"x": 0.17829457364341084, "y": 0.14925373134328357, "ox": 0.17829457364341084, "oy": 0.14925373134328357, "term": "application", "cat25k": 17, "ncat25k": 24, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 23, "s": 0.452991452991453, "os": -0.029059829059829068, "bg": 5.627112690462798e-07}, {"x": 0.062015503875968984, "y": 0.01492537313432836, "ox": 0.062015503875968984, "oy": 0.01492537313432836, "term": "additional", "cat25k": 2, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.3547008547008547, "os": -0.04700854700854701, "bg": 1.5123724048266125e-07}, {"x": 0.03875968992248062, "y": 0.01492537313432836, "ox": 0.03875968992248062, "oy": 0.01492537313432836, "term": "several", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47970085470085466, "os": -0.023931623931623933, "bg": 1.3005780883805983e-07}, {"x": 0.10852713178294572, "y": 0.05223880597014925, "ox": 0.10852713178294572, "oy": 0.05223880597014925, "term": "personal", "cat25k": 6, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 14, "s": 0.3076923076923077, "os": -0.05612535612535613, "bg": 1.9559740329917027e-07}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "location", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 5.679177901894738e-08}, {"x": 0.03875968992248062, "y": 0.01492537313432836, "ox": 0.03875968992248062, "oy": 0.01492537313432836, "term": "splunk", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47970085470085466, "os": -0.023931623931623933, "bg": 3.3496268754919767e-05}, {"x": 0.07751937984496123, "y": 0.0373134328358209, "ox": 0.07751937984496123, "oy": 0.0373134328358209, "term": "power", "cat25k": 4, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 10, "s": 0.3920940170940171, "os": -0.040170940170940174, "bg": 1.3236195424695507e-07}, {"x": 0.7829457364341085, "y": 0.11940298507462686, "ox": 0.7829457364341085, "oy": 0.11940298507462686, "term": "implement", "cat25k": 14, "ncat25k": 120, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 16, "ncat": 116, "s": 0.0032051282051282055, "os": -0.6586894586894587, "bg": 1.2705342273980024e-05}, {"x": 0.062015503875968984, "y": 0.007462686567164179, "ox": 0.062015503875968984, "oy": 0.007462686567164179, "term": "maintenance", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3162393162393162, "os": -0.05441595441595442, "bg": 3.457450757401648e-07}, {"x": 0.046511627906976744, "y": 0.022388059701492536, "ox": 0.046511627906976744, "oy": 0.022388059701492536, "term": "handling", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.4700854700854701, "os": -0.02421652421652422, "bg": 5.682073767522687e-07}, {"x": 0.046511627906976744, "y": 0.022388059701492536, "ox": 0.046511627906976744, "oy": 0.022388059701492536, "term": "optimize", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.4700854700854701, "os": -0.02421652421652422, "bg": 3.249841299416545e-06}, {"x": 0.2635658914728682, "y": 0.029850746268656716, "ox": 0.2635658914728682, "oy": 0.029850746268656716, "term": "infrastructure", "cat25k": 3, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 34, "s": 0.0844017094017094, "os": -0.23219373219373218, "bg": 2.607636993324552e-06}, {"x": 0.05426356589147286, "y": 0.05223880597014925, "ox": 0.05426356589147286, "oy": 0.05223880597014925, "term": "engineers", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 7, "s": 0.6100427350427351, "os": -0.0022792022792022804, "bg": 1.592740833364098e-06}, {"x": 0.15503875968992245, "y": 0.007462686567164179, "ox": 0.15503875968992245, "oy": 0.007462686567164179, "term": "ensure", "cat25k": 1, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 20, "s": 0.1185897435897436, "os": -0.14672364672364674, "bg": 7.05141942050898e-07}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "aligned", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 3.256343424832061e-06}, {"x": 0.0, "y": 0.16417910447761191, "ox": 0.0, "oy": 0.16417910447761191, "term": "concentration", "cat25k": 19, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 0, "s": 0.7895299145299145, "os": 0.16267806267806267, "bg": 2.717762411851075e-06}, {"x": 0.0, "y": 0.15671641791044774, "ox": 0.0, "oy": 0.15671641791044774, "term": "geek", "cat25k": 18, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7735042735042735, "os": 0.15527065527065528, "bg": 5.132030652885943e-06}, {"x": 0.023255813953488372, "y": 0.4925373134328358, "ox": 0.023255813953488372, "oy": 0.4925373134328358, "term": "looking", "cat25k": 57, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 67, "ncat": 3, "s": 0.9743589743589745, "os": 0.4655270655270655, "bg": 9.292132328327274e-07}, {"x": 0.007751937984496123, "y": 0.2014925373134328, "ox": 0.007751937984496123, "oy": 0.2014925373134328, "term": "love", "cat25k": 23, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 27, "ncat": 1, "s": 0.8205128205128206, "os": 0.19202279202279202, "bg": 2.784429428867332e-07}, {"x": 0.08527131782945736, "y": 0.3731343283582089, "ox": 0.08527131782945736, "oy": 0.3731343283582089, "term": "patterns", "cat25k": 43, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 50, "ncat": 11, "s": 0.9102564102564102, "os": 0.28547008547008546, "bg": 4.716848730583885e-06}, {"x": 0.0, "y": 0.15671641791044774, "ox": 0.0, "oy": 0.15671641791044774, "term": "puzzles", "cat25k": 18, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7735042735042735, "os": 0.15527065527065528, "bg": 4.241609339296633e-06}, {"x": 0.0, "y": 0.16417910447761191, "ox": 0.0, "oy": 0.16417910447761191, "term": "folks", "cat25k": 19, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 0, "s": 0.7895299145299145, "os": 0.16267806267806267, "bg": 3.184759247762761e-06}, {"x": 0.023255813953488372, "y": 0.26865671641791045, "ox": 0.023255813953488372, "oy": 0.26865671641791045, "term": "see", "cat25k": 31, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 36, "ncat": 3, "s": 0.8600427350427351, "os": 0.2433048433048433, "bg": 1.144592448987437e-07}, {"x": 0.007751937984496123, "y": 0.15671641791044774, "ox": 0.007751937984496123, "oy": 0.15671641791044774, "term": "choose", "cat25k": 18, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 1, "s": 0.764957264957265, "os": 0.1475783475783476, "bg": 4.885358132003376e-07}, {"x": 0.0, "y": 0.15671641791044774, "ox": 0.0, "oy": 0.15671641791044774, "term": "technique", "cat25k": 18, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7735042735042735, "os": 0.15527065527065528, "bg": 2.223921816228233e-06}, {"x": 0.023255813953488372, "y": 0.31343283582089554, "ox": 0.023255813953488372, "oy": 0.31343283582089554, "term": "given", "cat25k": 36, "ncat25k": 3, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 42, "ncat": 3, "s": 0.9123931623931625, "os": 0.28774928774928776, "bg": 7.165727186753022e-07}, {"x": 0.007751937984496123, "y": 0.1791044776119403, "ox": 0.007751937984496123, "oy": 0.1791044776119403, "term": "even", "cat25k": 20, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 24, "ncat": 1, "s": 0.8023504273504274, "os": 0.1698005698005698, "bg": 2.0345666774803883e-07}, {"x": 0.12403100775193797, "y": 0.18656716417910446, "ox": 0.12403100775193797, "oy": 0.18656716417910446, "term": "solution", "cat25k": 21, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 16, "s": 0.65491452991453, "os": 0.061823361823361817, "bg": 1.1072779638627414e-06}, {"x": 0.0, "y": 0.15671641791044774, "ox": 0.0, "oy": 0.15671641791044774, "term": "involve", "cat25k": 18, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7735042735042735, "os": 0.15527065527065528, "bg": 3.4909447801523496e-06}, {"x": 0.062015503875968984, "y": 0.17164179104477612, "ox": 0.062015503875968984, "oy": 0.17164179104477612, "term": "gcp", "cat25k": 20, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 23, "ncat": 8, "s": 0.7115384615384616, "os": 0.10854700854700855, "bg": 0.00022616594013898263}, {"x": 0.44961240310077516, "y": 0.21641791044776115, "ox": 0.44961240310077516, "oy": 0.21641791044776115, "term": "mindset", "cat25k": 25, "ncat25k": 64, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 29, "ncat": 62, "s": 0.08547008547008547, "os": -0.23162393162393163, "bg": 0.00014639710294612104}, {"x": 0.0697674418604651, "y": 0.18656716417910446, "ox": 0.0697674418604651, "oy": 0.18656716417910446, "term": "lead", "cat25k": 21, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 9, "s": 0.7243589743589743, "os": 0.11566951566951567, "bg": 9.944381367493415e-07}, {"x": 0.7984496124031008, "y": 0.18656716417910446, "ox": 0.7984496124031008, "oy": 0.18656716417910446, "term": "excited", "cat25k": 21, "ncat25k": 129, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 25, "ncat": 125, "s": 0.006410256410256411, "os": -0.6074074074074074, "bg": 3.10361447976852e-05}, {"x": 0.0, "y": 0.15671641791044774, "ox": 0.0, "oy": 0.15671641791044774, "term": "kinds", "cat25k": 18, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7735042735042735, "os": 0.15527065527065528, "bg": 1.2160868133105219e-06}, {"x": 0.05426356589147286, "y": 0.35074626865671643, "ox": 0.05426356589147286, "oy": 0.35074626865671643, "term": "challenges", "cat25k": 40, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 47, "ncat": 7, "s": 0.9155982905982907, "os": 0.294017094017094, "bg": 4.860667393936524e-06}, {"x": 0.0, "y": 0.15671641791044774, "ox": 0.0, "oy": 0.15671641791044774, "term": "talk", "cat25k": 18, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7735042735042735, "os": 0.15527065527065528, "bg": 4.2898418577376704e-07}, {"x": 0.0, "y": 0.15671641791044774, "ox": 0.0, "oy": 0.15671641791044774, "term": "intelligently", "cat25k": 18, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7735042735042735, "os": 0.15527065527065528, "bg": 5.2098257313292874e-05}, {"x": 0.0, "y": 0.15671641791044774, "ox": 0.0, "oy": 0.15671641791044774, "term": "passionately", "cat25k": 18, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7735042735042735, "os": 0.15527065527065528, "bg": 5.5344864391904886e-05}, {"x": 0.0, "y": 0.15671641791044774, "ox": 0.0, "oy": 0.15671641791044774, "term": "interesting", "cat25k": 18, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7735042735042735, "os": 0.15527065527065528, "bg": 8.928929420937244e-07}, {"x": 0.0, "y": 0.16417910447761191, "ox": 0.0, "oy": 0.16417910447761191, "term": "presented", "cat25k": 19, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 0, "s": 0.7895299145299145, "os": 0.16267806267806267, "bg": 9.404820427885558e-07}, {"x": 0.0, "y": 0.16417910447761191, "ox": 0.0, "oy": 0.16417910447761191, "term": "perspective", "cat25k": 19, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 0, "s": 0.7895299145299145, "os": 0.16267806267806267, "bg": 2.0362233018730663e-06}, {"x": 0.046511627906976744, "y": 0.15671641791044774, "ox": 0.046511627906976744, "oy": 0.15671641791044774, "term": "preference", "cat25k": 18, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 6, "s": 0.7147435897435898, "os": 0.10911680911680913, "bg": 4.964793180507193e-06}, {"x": 0.0, "y": 0.15671641791044774, "ox": 0.0, "oy": 0.15671641791044774, "term": "mass", "cat25k": 18, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 0, "s": 0.7735042735042735, "os": 0.15527065527065528, "bg": 1.0390944183286757e-06}, {"x": 0.05426356589147286, "y": 0.15671641791044774, "ox": 0.05426356589147286, "oy": 0.15671641791044774, "term": "relocation", "cat25k": 18, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 21, "ncat": 7, "s": 0.6987179487179488, "os": 0.10142450142450143, "bg": 6.233143381998971e-06}, {"x": 0.007751937984496123, "y": 0.16417910447761191, "ox": 0.007751937984496123, "oy": 0.16417910447761191, "term": "offered", "cat25k": 19, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 22, "ncat": 1, "s": 0.7724358974358976, "os": 0.15498575498575498, "bg": 9.126581760621586e-07}, {"x": 0.25581395348837205, "y": 0.01492537313432836, "ox": 0.25581395348837205, "oy": 0.01492537313432836, "term": "career", "cat25k": 2, "ncat25k": 34, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 33, "s": 0.0811965811965812, "os": -0.23931623931623933, "bg": 9.81716360237228e-07}, {"x": 0.03875968992248062, "y": 0.029850746268656716, "ox": 0.03875968992248062, "oy": 0.029850746268656716, "term": "committed", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.5726495726495727, "os": -0.009116809116809121, "bg": 8.288273520393527e-07}, {"x": 0.1317829457364341, "y": 0.007462686567164179, "ox": 0.1317829457364341, "oy": 0.007462686567164179, "term": "docker", "cat25k": 1, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 17, "s": 0.14743589743589747, "os": -0.12364672364672366, "bg": 0.0002285975540061721}, {"x": 0.565891472868217, "y": 0.007462686567164179, "ox": 0.565891472868217, "oy": 0.007462686567164179, "term": "stream", "cat25k": 1, "ncat25k": 84, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 81, "s": 0.010683760683760684, "os": -0.5544159544159544, "bg": 4.243498901464221e-06}, {"x": 0.1472868217054263, "y": 0.029850746268656716, "ox": 0.1472868217054263, "oy": 0.029850746268656716, "term": "batch", "cat25k": 3, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 19, "s": 0.15384615384615388, "os": -0.11680911680911682, "bg": 5.830222402705629e-06}, {"x": 0.05426356589147286, "y": 0.007462686567164179, "ox": 0.05426356589147286, "oy": 0.007462686567164179, "term": "ingest", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.36004273504273504, "os": -0.046723646723646726, "bg": 4.0023814169430814e-05}, {"x": 0.5891472868217054, "y": 0.007462686567164179, "ox": 0.5891472868217054, "oy": 0.007462686567164179, "term": "storm", "cat25k": 1, "ncat25k": 88, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 85, "s": 0.008547008547008548, "os": -0.5774928774928775, "bg": 7.714007997183849e-06}, {"x": 0.7596899224806201, "y": 0.022388059701492536, "ox": 0.7596899224806201, "oy": 0.022388059701492536, "term": "kafka", "cat25k": 3, "ncat25k": 116, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 112, "s": 0.0010683760683760685, "os": -0.7319088319088318, "bg": 0.0004174084741179524}, {"x": 0.05426356589147286, "y": 0.022388059701492536, "ox": 0.05426356589147286, "oy": 0.022388059701492536, "term": "get", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.41346153846153844, "os": -0.03190883190883191, "bg": 3.300115650737916e-08}, {"x": 0.08527131782945736, "y": 0.0373134328358209, "ox": 0.08527131782945736, "oy": 0.0373134328358209, "term": "extracting", "cat25k": 4, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 11, "s": 0.34294871794871795, "os": -0.04786324786324787, "bg": 1.7131501404783116e-05}, {"x": 0.03875968992248062, "y": 0.022388059701492536, "ox": 0.03875968992248062, "oy": 0.022388059701492536, "term": "analyst", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 5, "s": 0.5181623931623932, "os": -0.016524216524216526, "bg": 9.426682708523967e-07}, {"x": 0.07751937984496123, "y": 0.029850746268656716, "ox": 0.07751937984496123, "oy": 0.029850746268656716, "term": "improving", "cat25k": 3, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.3450854700854701, "os": -0.047578347578347585, "bg": 1.5633004063185252e-06}, {"x": 0.44186046511627897, "y": 0.01492537313432836, "ox": 0.44186046511627897, "oy": 0.01492537313432836, "term": "possessing", "cat25k": 2, "ncat25k": 63, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 61, "s": 0.05235042735042735, "os": -0.423931623931624, "bg": 9.005924611833928e-05}, {"x": 0.19379844961240308, "y": 0.0373134328358209, "ox": 0.19379844961240308, "oy": 0.0373134328358209, "term": "stack", "cat25k": 4, "ncat25k": 26, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 25, "s": 0.10897435897435898, "os": -0.15555555555555556, "bg": 5.383358853445053e-06}, {"x": 0.1705426356589147, "y": 0.14179104477611937, "ox": 0.1705426356589147, "oy": 0.14179104477611937, "term": "methodologies", "cat25k": 16, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 19, "ncat": 22, "s": 0.45405982905982906, "os": -0.028774928774928793, "bg": 2.6173005481329547e-05}, {"x": 0.03875968992248062, "y": 0.029850746268656716, "ox": 0.03875968992248062, "oy": 0.029850746268656716, "term": "intermediate", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.5726495726495727, "os": -0.009116809116809121, "bg": 1.3618912856662302e-06}, {"x": 0.05426356589147286, "y": 0.05223880597014925, "ox": 0.05426356589147286, "oy": 0.05223880597014925, "term": "k", "cat25k": 6, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 7, "s": 0.6100427350427351, "os": -0.0022792022792022804, "bg": 1.4307407819824115e-07}, {"x": 0.5581395348837208, "y": 0.19402985074626863, "ox": 0.5581395348837208, "oy": 0.19402985074626863, "term": "etl", "cat25k": 22, "ncat25k": 83, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 26, "ncat": 80, "s": 0.060897435897435896, "os": -0.36153846153846153, "bg": 0.0004071074821362389}, {"x": 0.6511627906976744, "y": 0.01492537313432836, "ox": 0.6511627906976744, "oy": 0.01492537313432836, "term": "dimensional", "cat25k": 2, "ncat25k": 96, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 93, "s": 0.005341880341880343, "os": -0.6316239316239316, "bg": 1.796350062489346e-05}, {"x": 0.10852713178294572, "y": 0.01492537313432836, "ox": 0.10852713178294572, "oy": 0.01492537313432836, "term": "together", "cat25k": 2, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 14, "s": 0.20085470085470086, "os": -0.09316239316239316, "bg": 3.3981592383790655e-07}, {"x": 0.11627906976744183, "y": 0.007462686567164179, "ox": 0.11627906976744183, "oy": 0.007462686567164179, "term": "automated", "cat25k": 1, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 15, "s": 0.16880341880341881, "os": -0.10826210826210828, "bg": 2.4860516962019965e-06}, {"x": 0.0697674418604651, "y": 0.007462686567164179, "ox": 0.0697674418604651, "oy": 0.007462686567164179, "term": "hoc", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.27350427350427353, "os": -0.06210826210826211, "bg": 4.4929109727444295e-06}, {"x": 0.09302325581395349, "y": 0.0373134328358209, "ox": 0.09302325581395349, "oy": 0.0373134328358209, "term": "around", "cat25k": 4, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 12, "s": 0.3098290598290598, "os": -0.05555555555555556, "bg": 1.9008760349340536e-07}, {"x": 0.046511627906976744, "y": 0.01492537313432836, "ox": 0.046511627906976744, "oy": 0.01492537313432836, "term": "space", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.4198717948717949, "os": -0.03162393162393162, "bg": 1.3162210772963498e-07}, {"x": 0.05426356589147286, "y": 0.01492537313432836, "ox": 0.05426356589147286, "oy": 0.01492537313432836, "term": "underlying", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.39743589743589747, "os": -0.039316239316239315, "bg": 1.6281911528977323e-06}, {"x": 0.10077519379844961, "y": 0.007462686567164179, "ox": 0.10077519379844961, "oy": 0.007462686567164179, "term": "apis", "cat25k": 1, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 13, "s": 0.20192307692307696, "os": -0.09287749287749289, "bg": 1.3980511167432599e-05}, {"x": 0.03875968992248062, "y": 0.0373134328358209, "ox": 0.03875968992248062, "oy": 0.0373134328358209, "term": "experimentation", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 5, "s": 0.6164529914529915, "os": -0.0017094017094017103, "bg": 1.042244786821857e-05}, {"x": 0.6666666666666665, "y": 0.029850746268656716, "ox": 0.6666666666666665, "oy": 0.029850746268656716, "term": "streaming", "cat25k": 3, "ncat25k": 98, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 95, "s": 0.004273504273504274, "os": -0.6321937321937321, "bg": 2.0912278063168173e-05}, {"x": 0.07751937984496123, "y": 0.022388059701492536, "ox": 0.07751937984496123, "oy": 0.022388059701492536, "term": "compute", "cat25k": 3, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 10, "s": 0.313034188034188, "os": -0.05498575498575499, "bg": 5.04267166951416e-06}, {"x": 0.03875968992248062, "y": 0.022388059701492536, "ox": 0.03875968992248062, "oy": 0.022388059701492536, "term": "exposure", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 5, "s": 0.5181623931623932, "os": -0.016524216524216526, "bg": 5.869517471737631e-07}, {"x": 0.21705426356589144, "y": 0.04477611940298507, "ox": 0.21705426356589144, "oy": 0.04477611940298507, "term": "structures", "cat25k": 5, "ncat25k": 29, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 28, "s": 0.10256410256410256, "os": -0.17122507122507125, "bg": 2.664579517204837e-06}, {"x": 0.0, "y": 0.11194029850746266, "ox": 0.0, "oy": 0.11194029850746266, "term": "prototype", "cat25k": 13, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 0, "s": 0.7190170940170941, "os": 0.11082621082621083, "bg": 5.126081966392381e-06}, {"x": 0.10077519379844961, "y": 0.14925373134328357, "ox": 0.10077519379844961, "oy": 0.14925373134328357, "term": "financial", "cat25k": 17, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 20, "ncat": 13, "s": 0.6474358974358975, "os": 0.04786324786324786, "bg": 4.4478847709944853e-07}, {"x": 0.007751937984496123, "y": 0.12686567164179102, "ox": 0.007751937984496123, "oy": 0.12686567164179102, "term": "feasibility", "cat25k": 15, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 1, "s": 0.7297008547008548, "os": 0.11794871794871795, "bg": 8.744713395662087e-06}, {"x": 0.007751937984496123, "y": 0.11194029850746266, "ox": 0.007751937984496123, "oy": 0.11194029850746266, "term": "please", "cat25k": 13, "ncat25k": 1, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 15, "ncat": 1, "s": 0.701923076923077, "os": 0.10313390313390314, "bg": 8.41881028146048e-08}, {"x": 0.0, "y": 0.12686567164179102, "ox": 0.0, "oy": 0.12686567164179102, "term": "sponsorship", "cat25k": 15, "ncat25k": 0, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 17, "ncat": 0, "s": 0.7382478632478633, "os": 0.12564102564102564, "bg": 4.6817897656075715e-06}, {"x": 0.10852713178294572, "y": 0.04477611940298507, "ox": 0.10852713178294572, "oy": 0.04477611940298507, "term": "principles", "cat25k": 5, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 14, "s": 0.2596153846153846, "os": -0.06353276353276355, "bg": 1.285958287113849e-06}, {"x": 0.05426356589147286, "y": 0.022388059701492536, "ox": 0.05426356589147286, "oy": 0.022388059701492536, "term": "metadata", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.41346153846153844, "os": -0.03190883190883191, "bg": 3.024830836335478e-06}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "assurance", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 1.001354415292885e-06}, {"x": 0.046511627906976744, "y": 0.022388059701492536, "ox": 0.046511627906976744, "oy": 0.022388059701492536, "term": "individual", "cat25k": 3, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 6, "s": 0.4700854700854701, "os": -0.02421652421652422, "bg": 1.897334392748641e-07}, {"x": 0.046511627906976744, "y": 0.01492537313432836, "ox": 0.046511627906976744, "oy": 0.01492537313432836, "term": "strongly", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.4198717948717949, "os": -0.03162393162393162, "bg": 9.704083672491056e-07}, {"x": 0.0697674418604651, "y": 0.01492537313432836, "ox": 0.0697674418604651, "oy": 0.01492537313432836, "term": "objects", "cat25k": 2, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 9, "s": 0.31517094017094016, "os": -0.0547008547008547, "bg": 6.48967073033132e-07}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "tune", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 1.112864490942396e-06}, {"x": 0.1395348837209302, "y": 0.007462686567164179, "ox": 0.1395348837209302, "oy": 0.007462686567164179, "term": "ssis", "cat25k": 1, "ncat25k": 19, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 18, "s": 0.139957264957265, "os": -0.13133903133903135, "bg": 0.00020944601528955907}, {"x": 0.15503875968992245, "y": 0.007462686567164179, "ox": 0.15503875968992245, "oy": 0.007462686567164179, "term": "owners", "cat25k": 1, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 20, "s": 0.1185897435897436, "os": -0.14672364672364674, "bg": 6.926626362387006e-07}, {"x": 0.03875968992248062, "y": 0.0373134328358209, "ox": 0.03875968992248062, "oy": 0.0373134328358209, "term": "creative", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 5, "s": 0.6164529914529915, "os": -0.0017094017094017103, "bg": 4.1847716452762414e-07}, {"x": 0.47286821705426346, "y": 0.04477611940298507, "ox": 0.47286821705426346, "oy": 0.04477611940298507, "term": "growing", "cat25k": 5, "ncat25k": 67, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 65, "s": 0.05128205128205128, "os": -0.4250712250712251, "bg": 3.66128069071762e-06}, {"x": 0.08527131782945736, "y": 0.007462686567164179, "ox": 0.08527131782945736, "oy": 0.007462686567164179, "term": "membership", "cat25k": 1, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 11, "s": 0.22542735042735043, "os": -0.0774928774928775, "bg": 4.546061547990783e-07}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "chance", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 2.757575156619354e-07}, {"x": 0.03875968992248062, "y": 0.0373134328358209, "ox": 0.03875968992248062, "oy": 0.0373134328358209, "term": "fluent", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 5, "s": 0.6164529914529915, "os": -0.0017094017094017103, "bg": 1.2612503531500987e-05}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "ibm", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 2.9765953285908234e-07}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "resource", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 1.1997727870295923e-07}, {"x": 0.062015503875968984, "y": 0.029850746268656716, "ox": 0.062015503875968984, "oy": 0.029850746268656716, "term": "global", "cat25k": 3, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.40918803418803423, "os": -0.0321937321937322, "bg": 2.566695145575249e-07}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "root", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 2.8646439272664043e-07}, {"x": 0.031007751937984492, "y": 0.022388059701492536, "ox": 0.031007751937984492, "oy": 0.022388059701492536, "term": "structure", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 4, "s": 0.5801282051282052, "os": -0.008831908831908833, "bg": 2.081896669728195e-07}, {"x": 0.15503875968992245, "y": 0.0373134328358209, "ox": 0.15503875968992245, "oy": 0.0373134328358209, "term": "existing", "cat25k": 4, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 20, "s": 0.15170940170940173, "os": -0.1170940170940171, "bg": 8.495028989371377e-07}, {"x": 0.031007751937984492, "y": 0.022388059701492536, "ox": 0.031007751937984492, "oy": 0.022388059701492536, "term": "many", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 4, "s": 0.5801282051282052, "os": -0.008831908831908833, "bg": 4.388421754802156e-08}, {"x": 0.28682170542635654, "y": 0.05970149253731343, "ox": 0.28682170542635654, "oy": 0.05970149253731343, "term": "maintain", "cat25k": 7, "ncat25k": 38, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 37, "s": 0.08653846153846154, "os": -0.22564102564102567, "bg": 2.7275239074289296e-06}, {"x": 0.062015503875968984, "y": 0.01492537313432836, "ox": 0.062015503875968984, "oy": 0.01492537313432836, "term": "speed", "cat25k": 2, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.3547008547008547, "os": -0.04700854700854701, "bg": 2.404117541730191e-07}, {"x": 0.03875968992248062, "y": 0.01492537313432836, "ox": 0.03875968992248062, "oy": 0.01492537313432836, "term": "matter", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47970085470085466, "os": -0.023931623931623933, "bg": 2.0993388866944963e-07}, {"x": 0.046511627906976744, "y": 0.01492537313432836, "ox": 0.046511627906976744, "oy": 0.01492537313432836, "term": "happy", "cat25k": 2, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 6, "s": 0.4198717948717949, "os": -0.03162393162393162, "bg": 2.5186228944489693e-07}, {"x": 0.03875968992248062, "y": 0.022388059701492536, "ox": 0.03875968992248062, "oy": 0.022388059701492536, "term": "grasp", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 5, "s": 0.5181623931623932, "os": -0.016524216524216526, "bg": 4.812200854045347e-06}, {"x": 0.03875968992248062, "y": 0.01492537313432836, "ox": 0.03875968992248062, "oy": 0.01492537313432836, "term": "visa", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47970085470085466, "os": -0.023931623931623933, "bg": 6.097908630766448e-07}, {"x": 0.05426356589147286, "y": 0.029850746268656716, "ox": 0.05426356589147286, "oy": 0.029850746268656716, "term": "available", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.4658119658119658, "os": -0.024501424501424507, "bg": 5.794058660055934e-08}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "microstrategy", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 2.0905194940942824e-05}, {"x": 0.0697674418604651, "y": 0.04477611940298507, "ox": 0.0697674418604651, "oy": 0.04477611940298507, "term": "higher", "cat25k": 5, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 6, "ncat": 9, "s": 0.45833333333333337, "os": -0.025071225071225077, "bg": 3.6332606116037146e-07}, {"x": 0.05426356589147286, "y": 0.007462686567164179, "ox": 0.05426356589147286, "oy": 0.007462686567164179, "term": "completed", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.36004273504273504, "os": -0.046723646723646726, "bg": 2.916223195693978e-07}, {"x": 0.05426356589147286, "y": 0.0373134328358209, "ox": 0.05426356589147286, "oy": 0.0373134328358209, "term": "public", "cat25k": 4, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 7, "s": 0.5074786324786326, "os": -0.017094017094017096, "bg": 6.870078547669058e-08}, {"x": 0.062015503875968984, "y": 0.022388059701492536, "ox": 0.062015503875968984, "oy": 0.022388059701492536, "term": "extract", "cat25k": 3, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 8, "s": 0.3952991452991453, "os": -0.039601139601139604, "bg": 1.9969202044301673e-06}, {"x": 0.1472868217054263, "y": 0.007462686567164179, "ox": 0.1472868217054263, "oy": 0.007462686567164179, "term": "load", "cat25k": 1, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 19, "s": 0.12927350427350429, "os": -0.13903133903133905, "bg": 1.053988089354897e-06}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "certified", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 3.2833321969355784e-07}, {"x": 0.05426356589147286, "y": 0.007462686567164179, "ox": 0.05426356589147286, "oy": 0.007462686567164179, "term": "rapidly", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.36004273504273504, "os": -0.046723646723646726, "bg": 1.3317804772968051e-06}, {"x": 0.03875968992248062, "y": 0.029850746268656716, "ox": 0.03875968992248062, "oy": 0.029850746268656716, "term": "purpose", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.5726495726495727, "os": -0.009116809116809121, "bg": 2.79634194974321e-07}, {"x": 0.10077519379844961, "y": 0.01492537313432836, "ox": 0.10077519379844961, "oy": 0.01492537313432836, "term": "accuracy", "cat25k": 2, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 13, "s": 0.20940170940170943, "os": -0.08547008547008547, "bg": 1.0138257444421058e-06}, {"x": 0.07751937984496123, "y": 0.01492537313432836, "ox": 0.07751937984496123, "oy": 0.01492537313432836, "term": "json", "cat25k": 2, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.2692307692307692, "os": -0.062393162393162394, "bg": 0.00016055444802718722}, {"x": 0.05426356589147286, "y": 0.007462686567164179, "ox": 0.05426356589147286, "oy": 0.007462686567164179, "term": "xml", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.36004273504273504, "os": -0.046723646723646726, "bg": 2.792486521802007e-07}, {"x": 0.12403100775193797, "y": 0.007462686567164179, "ox": 0.12403100775193797, "oy": 0.007462686567164179, "term": "jenkins", "cat25k": 1, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 16, "s": 0.15705128205128208, "os": -0.11595441595441597, "bg": 8.415329165600311e-06}, {"x": 0.4651162790697673, "y": 0.029850746268656716, "ox": 0.4651162790697673, "oy": 0.029850746268656716, "term": "semi", "cat25k": 3, "ncat25k": 66, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 64, "s": 0.04807692307692308, "os": -0.4321937321937322, "bg": 8.239642738784544e-06}, {"x": 0.031007751937984492, "y": 0.022388059701492536, "ox": 0.031007751937984492, "oy": 0.022388059701492536, "term": "critical", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 4, "s": 0.5801282051282052, "os": -0.008831908831908833, "bg": 3.0308988446603287e-07}, {"x": 0.2015503875968992, "y": 0.06716417910447761, "ox": 0.2015503875968992, "oy": 0.06716417910447761, "term": "microsoft", "cat25k": 8, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 26, "s": 0.1356837606837607, "os": -0.13361823361823363, "bg": 6.848345846630864e-07}, {"x": 0.15503875968992245, "y": 0.01492537313432836, "ox": 0.15503875968992245, "oy": 0.01492537313432836, "term": "object", "cat25k": 2, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 20, "s": 0.12713675213675213, "os": -0.13931623931623932, "bg": 6.297037078041986e-07}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "release", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 9.569706061932697e-08}, {"x": 0.05426356589147286, "y": 0.029850746268656716, "ox": 0.05426356589147286, "oy": 0.029850746268656716, "term": "volume", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 7, "s": 0.4658119658119658, "os": -0.024501424501424507, "bg": 2.959442282878264e-07}, {"x": 0.046511627906976744, "y": 0.0373134328358209, "ox": 0.046511627906976744, "oy": 0.0373134328358209, "term": "neo4j", "cat25k": 4, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 6, "s": 0.5662393162393163, "os": -0.009401709401709403, "bg": 0.0}, {"x": 0.09302325581395349, "y": 0.06716417910447761, "ox": 0.09302325581395349, "oy": 0.06716417910447761, "term": "test", "cat25k": 8, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 9, "ncat": 12, "s": 0.4561965811965812, "os": -0.025925925925925936, "bg": 2.7087257478835327e-07}, {"x": 0.09302325581395349, "y": 0.029850746268656716, "ox": 0.09302325581395349, "oy": 0.029850746268656716, "term": "share", "cat25k": 3, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 12, "s": 0.26175213675213677, "os": -0.06296296296296297, "bg": 2.6812098617646955e-07}, {"x": 0.03875968992248062, "y": 0.01492537313432836, "ox": 0.03875968992248062, "oy": 0.01492537313432836, "term": "holidays", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47970085470085466, "os": -0.023931623931623933, "bg": 3.350465540007215e-07}, {"x": 0.18604651162790695, "y": 0.05970149253731343, "ox": 0.18604651162790695, "oy": 0.05970149253731343, "term": "efficient", "cat25k": 7, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 8, "ncat": 24, "s": 0.14636752136752137, "os": -0.12564102564102564, "bg": 2.78036433720502e-06}, {"x": 0.0697674418604651, "y": 0.029850746268656716, "ox": 0.0697674418604651, "oy": 0.029850746268656716, "term": "always", "cat25k": 3, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 9, "s": 0.39316239316239315, "os": -0.03988603988603989, "bg": 2.0361872928615056e-07}, {"x": 0.2015503875968992, "y": 0.007462686567164179, "ox": 0.2015503875968992, "oy": 0.007462686567164179, "term": "airflow", "cat25k": 1, "ncat25k": 27, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 26, "s": 0.09081196581196582, "os": -0.1928774928774929, "bg": 4.922870471073122e-05}, {"x": 0.07751937984496123, "y": 0.01492537313432836, "ox": 0.07751937984496123, "oy": 0.01492537313432836, "term": "near", "cat25k": 2, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.2692307692307692, "os": -0.062393162393162394, "bg": 2.937302255304731e-07}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "foundation", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 1.8516127137839252e-07}, {"x": 0.10077519379844961, "y": 0.05223880597014925, "ox": 0.10077519379844961, "oy": 0.05223880597014925, "term": "plan", "cat25k": 6, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 7, "ncat": 13, "s": 0.33974358974358976, "os": -0.04843304843304844, "bg": 2.4875449711594966e-07}, {"x": 0.062015503875968984, "y": 0.007462686567164179, "ox": 0.062015503875968984, "oy": 0.007462686567164179, "term": "parties", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3162393162393162, "os": -0.05441595441595442, "bg": 3.4055814831723595e-07}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "easily", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 2.8568611162203954e-07}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "table", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 7.255301050947891e-08}, {"x": 0.44186046511627897, "y": 0.007462686567164179, "ox": 0.44186046511627897, "oy": 0.007462686567164179, "term": "humble", "cat25k": 1, "ncat25k": 63, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 61, "s": 0.049145299145299144, "os": -0.43133903133903134, "bg": 3.3080675229937375e-05}, {"x": 0.03875968992248062, "y": 0.022388059701492536, "ox": 0.03875968992248062, "oy": 0.022388059701492536, "term": "friendly", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 5, "s": 0.5181623931623932, "os": -0.016524216524216526, "bg": 2.476889536980797e-07}, {"x": 0.11627906976744183, "y": 0.007462686567164179, "ox": 0.11627906976744183, "oy": 0.007462686567164179, "term": "lambda", "cat25k": 1, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 15, "s": 0.16880341880341881, "os": -0.10826210826210828, "bg": 3.4281822074558037e-06}, {"x": 0.07751937984496123, "y": 0.029850746268656716, "ox": 0.07751937984496123, "oy": 0.029850746268656716, "term": "architectures", "cat25k": 3, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.3450854700854701, "os": -0.047578347578347585, "bg": 8.165803132985354e-06}, {"x": 0.03875968992248062, "y": 0.0373134328358209, "ox": 0.03875968992248062, "oy": 0.0373134328358209, "term": "message", "cat25k": 4, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 5, "s": 0.6164529914529915, "os": -0.0017094017094017103, "bg": 5.3599745364761694e-08}, {"x": 0.05426356589147286, "y": 0.022388059701492536, "ox": 0.05426356589147286, "oy": 0.022388059701492536, "term": "shell", "cat25k": 3, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 7, "s": 0.41346153846153844, "os": -0.03190883190883191, "bg": 9.683271330540066e-07}, {"x": 0.031007751937984492, "y": 0.022388059701492536, "ox": 0.031007751937984492, "oy": 0.022388059701492536, "term": "line", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 4, "s": 0.5801282051282052, "os": -0.008831908831908833, "bg": 4.998849318445994e-08}, {"x": 0.11627906976744183, "y": 0.0373134328358209, "ox": 0.11627906976744183, "oy": 0.0373134328358209, "term": "practical", "cat25k": 4, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 15, "s": 0.22115384615384617, "os": -0.07863247863247863, "bg": 1.3701547346570328e-06}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "developer", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 3.3384258237252745e-07}, {"x": 0.08527131782945736, "y": 0.022388059701492536, "ox": 0.08527131782945736, "oy": 0.022388059701492536, "term": "delivery", "cat25k": 3, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 11, "s": 0.26495726495726496, "os": -0.06267806267806268, "bg": 2.389601254151495e-07}, {"x": 0.05426356589147286, "y": 0.007462686567164179, "ox": 0.05426356589147286, "oy": 0.007462686567164179, "term": "restful", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.36004273504273504, "os": -0.046723646723646726, "bg": 3.5294143598635e-05}, {"x": 0.1472868217054263, "y": 0.01492537313432836, "ox": 0.1472868217054263, "oy": 0.01492537313432836, "term": "api", "cat25k": 2, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 19, "s": 0.1388888888888889, "os": -0.13162393162393163, "bg": 2.9409643422078097e-06}, {"x": 0.11627906976744183, "y": 0.01492537313432836, "ox": 0.11627906976744183, "oy": 0.01492537313432836, "term": "interfaces", "cat25k": 2, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 15, "s": 0.1826923076923077, "os": -0.10085470085470086, "bg": 2.768949038063032e-06}, {"x": 0.07751937984496123, "y": 0.01492537313432836, "ox": 0.07751937984496123, "oy": 0.01492537313432836, "term": "stores", "cat25k": 2, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.2692307692307692, "os": -0.062393162393162394, "bg": 1.698931090604604e-07}, {"x": 0.0697674418604651, "y": 0.007462686567164179, "ox": 0.0697674418604651, "oy": 0.007462686567164179, "term": "rest", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.27350427350427353, "os": -0.06210826210826211, "bg": 3.747643645635034e-07}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "mssql", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 2.7190476082577477e-05}, {"x": 0.11627906976744183, "y": 0.029850746268656716, "ox": 0.11627906976744183, "oy": 0.029850746268656716, "term": "postgresql", "cat25k": 3, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 15, "s": 0.20833333333333334, "os": -0.08603988603988605, "bg": 8.130978944188104e-06}, {"x": 0.05426356589147286, "y": 0.01492537313432836, "ox": 0.05426356589147286, "oy": 0.01492537313432836, "term": "certification", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.39743589743589747, "os": -0.039316239316239315, "bg": 6.483364892312571e-07}, {"x": 0.03875968992248062, "y": 0.01492537313432836, "ox": 0.03875968992248062, "oy": 0.01492537313432836, "term": "family", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47970085470085466, "os": -0.023931623931623933, "bg": 5.507064736029807e-08}, {"x": 0.031007751937984492, "y": 0.022388059701492536, "ox": 0.031007751937984492, "oy": 0.022388059701492536, "term": "dependent", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 4, "s": 0.5801282051282052, "os": -0.008831908831908833, "bg": 7.2478421362e-07}, {"x": 0.05426356589147286, "y": 0.007462686567164179, "ox": 0.05426356589147286, "oy": 0.007462686567164179, "term": "days", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.36004273504273504, "os": -0.046723646723646726, "bg": 5.242418473381003e-08}, {"x": 0.08527131782945736, "y": 0.01492537313432836, "ox": 0.08527131782945736, "oy": 0.01492537313432836, "term": "reimbursement", "cat25k": 2, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 11, "s": 0.23931623931623933, "os": -0.07008547008547009, "bg": 5.242560151824542e-06}, {"x": 0.062015503875968984, "y": 0.029850746268656716, "ox": 0.062015503875968984, "oy": 0.029850746268656716, "term": "actuarial", "cat25k": 3, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 8, "s": 0.40918803418803423, "os": -0.0321937321937322, "bg": 1.3307760254045144e-05}, {"x": 0.062015503875968984, "y": 0.01492537313432836, "ox": 0.062015503875968984, "oy": 0.01492537313432836, "term": "facing", "cat25k": 2, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.3547008547008547, "os": -0.04700854700854701, "bg": 1.488098116856476e-06}, {"x": 0.05426356589147286, "y": 0.007462686567164179, "ox": 0.05426356589147286, "oy": 0.007462686567164179, "term": "matching", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.36004273504273504, "os": -0.046723646723646726, "bg": 4.59001669647261e-07}, {"x": 0.0697674418604651, "y": 0.022388059701492536, "ox": 0.0697674418604651, "oy": 0.022388059701492536, "term": "snacks", "cat25k": 3, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 9, "s": 0.35149572649572647, "os": -0.0472934472934473, "bg": 5.262654711084642e-06}, {"x": 0.03875968992248062, "y": 0.01492537313432836, "ox": 0.03875968992248062, "oy": 0.01492537313432836, "term": "cases", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47970085470085466, "os": -0.023931623931623933, "bg": 1.6220500745627453e-07}, {"x": 0.05426356589147286, "y": 0.007462686567164179, "ox": 0.05426356589147286, "oy": 0.007462686567164179, "term": "along", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.36004273504273504, "os": -0.046723646723646726, "bg": 1.9161765456326127e-07}, {"x": 0.15503875968992245, "y": 0.01492537313432836, "ox": 0.15503875968992245, "oy": 0.01492537313432836, "term": "enable", "cat25k": 2, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 20, "s": 0.12713675213675213, "os": -0.13931623931623932, "bg": 1.1058644544954119e-06}, {"x": 0.1705426356589147, "y": 0.01492537313432836, "ox": 0.1705426356589147, "oy": 0.01492537313432836, "term": "warehousing", "cat25k": 2, "ncat25k": 23, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 22, "s": 0.11004273504273505, "os": -0.1547008547008547, "bg": 1.4830345482166975e-05}, {"x": 0.07751937984496123, "y": 0.007462686567164179, "ox": 0.07751937984496123, "oy": 0.007462686567164179, "term": "usage", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 10, "s": 0.24145299145299146, "os": -0.06980056980056981, "bg": 8.629052938690657e-07}, {"x": 0.0697674418604651, "y": 0.007462686567164179, "ox": 0.0697674418604651, "oy": 0.007462686567164179, "term": "actively", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.27350427350427353, "os": -0.06210826210826211, "bg": 2.3570454859102303e-06}, {"x": 0.5426356589147285, "y": 0.029850746268656716, "ox": 0.5426356589147285, "oy": 0.029850746268656716, "term": "growth", "cat25k": 3, "ncat25k": 78, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 76, "s": 0.01816239316239316, "os": -0.509116809116809, "bg": 1.998665665818179e-06}, {"x": 0.16279069767441856, "y": 0.007462686567164179, "ox": 0.16279069767441856, "oy": 0.007462686567164179, "term": "continuously", "cat25k": 1, "ncat25k": 22, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 21, "s": 0.11111111111111112, "os": -0.15441595441595443, "bg": 7.766622867620737e-06}, {"x": 0.34883720930232553, "y": 0.022388059701492536, "ox": 0.34883720930232553, "oy": 0.022388059701492536, "term": "redshift", "cat25k": 3, "ncat25k": 49, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 47, "s": 0.06517094017094016, "os": -0.32421652421652425, "bg": 0.00014670048601871018}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "columnar", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 4.022620536148273e-05}, {"x": 0.062015503875968984, "y": 0.01492537313432836, "ox": 0.062015503875968984, "oy": 0.01492537313432836, "term": "finding", "cat25k": 2, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.3547008547008547, "os": -0.04700854700854701, "bg": 5.38251241631236e-07}, {"x": 0.27131782945736427, "y": 0.029850746268656716, "ox": 0.27131782945736427, "oy": 0.029850746268656716, "term": "server", "cat25k": 3, "ncat25k": 36, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 35, "s": 0.08012820512820513, "os": -0.2398860398860399, "bg": 5.070089700026973e-07}, {"x": 0.12403100775193797, "y": 0.0373134328358209, "ox": 0.12403100775193797, "oy": 0.0373134328358209, "term": "enterprise", "cat25k": 4, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 5, "ncat": 16, "s": 0.20726495726495728, "os": -0.08632478632478632, "bg": 7.437596704252149e-07}, {"x": 0.031007751937984492, "y": 0.022388059701492536, "ox": 0.031007751937984492, "oy": 0.022388059701492536, "term": "ingestion", "cat25k": 3, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 4, "s": 0.5801282051282052, "os": -0.008831908831908833, "bg": 1.1261061982493874e-05}, {"x": 0.062015503875968984, "y": 0.01492537313432836, "ox": 0.062015503875968984, "oy": 0.01492537313432836, "term": "five", "cat25k": 2, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 8, "s": 0.3547008547008547, "os": -0.04700854700854701, "bg": 1.9190058996765977e-07}, {"x": 0.0697674418604651, "y": 0.007462686567164179, "ox": 0.0697674418604651, "oy": 0.007462686567164179, "term": "commuter", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.27350427350427353, "os": -0.06210826210826211, "bg": 1.099020662687479e-05}, {"x": 0.3643410852713178, "y": 0.01492537313432836, "ox": 0.3643410852713178, "oy": 0.01492537313432836, "term": "warehouse", "cat25k": 2, "ncat25k": 51, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 49, "s": 0.06303418803418803, "os": -0.34700854700854705, "bg": 6.9934365911960594e-06}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "semantic", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 2.5823828550437486e-06}, {"x": 0.11627906976744183, "y": 0.007462686567164179, "ox": 0.11627906976744183, "oy": 0.007462686567164179, "term": "tuning", "cat25k": 1, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 15, "s": 0.16880341880341881, "os": -0.10826210826210828, "bg": 4.2465645624452676e-06}, {"x": 0.07751937984496123, "y": 0.01492537313432836, "ox": 0.07751937984496123, "oy": 0.01492537313432836, "term": "schema", "cat25k": 2, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 10, "s": 0.2692307692307692, "os": -0.062393162393162394, "bg": 3.6149031612629388e-06}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "elt", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 1.3921436692266642e-05}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "leave", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 1.6898408388200938e-07}, {"x": 0.44961240310077516, "y": 0.007462686567164179, "ox": 0.44961240310077516, "oy": 0.007462686567164179, "term": "massive", "cat25k": 1, "ncat25k": 64, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 62, "s": 0.04059829059829059, "os": -0.439031339031339, "bg": 6.458341747971722e-06}, {"x": 0.05426356589147286, "y": 0.007462686567164179, "ox": 0.05426356589147286, "oy": 0.007462686567164179, "term": "presto", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.36004273504273504, "os": -0.046723646723646726, "bg": 1.6471242240500725e-05}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "millions", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 5.751874763663854e-07}, {"x": 0.03875968992248062, "y": 0.01492537313432836, "ox": 0.03875968992248062, "oy": 0.01492537313432836, "term": "users", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47970085470085466, "os": -0.023931623931623933, "bg": 9.372379290281934e-08}, {"x": 0.07751937984496123, "y": 0.029850746268656716, "ox": 0.07751937984496123, "oy": 0.029850746268656716, "term": "elasticsearch", "cat25k": 3, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 10, "s": 0.3450854700854701, "os": -0.047578347578347585, "bg": 0.0005103621748719538}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "authoring", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 2.7076484299023666e-06}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "post", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 3.053347390978647e-08}, {"x": 0.03875968992248062, "y": 0.029850746268656716, "ox": 0.03875968992248062, "oy": 0.029850746268656716, "term": "integrating", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.5726495726495727, "os": -0.009116809116809121, "bg": 2.9700407044078538e-06}, {"x": 0.05426356589147286, "y": 0.01492537313432836, "ox": 0.05426356589147286, "oy": 0.01492537313432836, "term": "architecting", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.39743589743589747, "os": -0.039316239316239315, "bg": 9.414570617125105e-05}, {"x": 0.046511627906976744, "y": 0.007462686567164179, "ox": 0.046511627906976744, "oy": 0.007462686567164179, "term": "schemas", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.40384615384615385, "os": -0.039031339031339034, "bg": 1.0225621023876824e-05}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "transforming", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 3.5879405729402906e-06}, {"x": 0.0697674418604651, "y": 0.022388059701492536, "ox": 0.0697674418604651, "oy": 0.022388059701492536, "term": "robust", "cat25k": 3, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 9, "s": 0.35149572649572647, "os": -0.0472934472934473, "bg": 3.325545091481588e-06}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "microservices", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 0.00021875854525567405}, {"x": 0.08527131782945736, "y": 0.022388059701492536, "ox": 0.08527131782945736, "oy": 0.022388059701492536, "term": "contribute", "cat25k": 3, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 11, "s": 0.26495726495726496, "os": -0.06267806267806268, "bg": 1.2550437068970928e-06}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "answer", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 1.8545875688571658e-07}, {"x": 0.1317829457364341, "y": 0.01492537313432836, "ox": 0.1317829457364341, "oy": 0.01492537313432836, "term": "scrum", "cat25k": 2, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 17, "s": 0.15598290598290598, "os": -0.11623931623931624, "bg": 7.904513477195479e-05}, {"x": 0.10852713178294572, "y": 0.007462686567164179, "ox": 0.10852713178294572, "oy": 0.007462686567164179, "term": "devops", "cat25k": 1, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 14, "s": 0.18482905982905984, "os": -0.10056980056980058, "bg": 0.0005468066491688539}, {"x": 0.11627906976744183, "y": 0.007462686567164179, "ox": 0.11627906976744183, "oy": 0.007462686567164179, "term": "gym", "cat25k": 1, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 15, "s": 0.16880341880341881, "os": -0.10826210826210828, "bg": 3.22341326223021e-06}, {"x": 0.12403100775193797, "y": 0.007462686567164179, "ox": 0.12403100775193797, "oy": 0.007462686567164179, "term": "tables", "cat25k": 1, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 16, "s": 0.15705128205128208, "os": -0.11595441595441597, "bg": 9.406711251342427e-07}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "dynamodb", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 0.00021875854525567405}, {"x": 0.18604651162790695, "y": 0.022388059701492536, "ox": 0.18604651162790695, "oy": 0.022388059701492536, "term": "s3", "cat25k": 3, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 3, "ncat": 24, "s": 0.1047008547008547, "os": -0.16267806267806267, "bg": 0.0}, {"x": 0.12403100775193797, "y": 0.007462686567164179, "ox": 0.12403100775193797, "oy": 0.007462686567164179, "term": "ec2", "cat25k": 1, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 16, "s": 0.15705128205128208, "os": -0.11595441595441597, "bg": 0.0}, {"x": 0.03875968992248062, "y": 0.029850746268656716, "ox": 0.03875968992248062, "oy": 0.029850746268656716, "term": "three", "cat25k": 3, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 4, "ncat": 5, "s": 0.5726495726495727, "os": -0.009116809116809121, "bg": 8.054691282208943e-08}, {"x": 0.03875968992248062, "y": 0.01492537313432836, "ox": 0.03875968992248062, "oy": 0.01492537313432836, "term": "patient", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47970085470085466, "os": -0.023931623931623933, "bg": 3.285200838993363e-07}, {"x": 0.05426356589147286, "y": 0.007462686567164179, "ox": 0.05426356589147286, "oy": 0.007462686567164179, "term": "workflows", "cat25k": 1, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 7, "s": 0.36004273504273504, "os": -0.046723646723646726, "bg": 2.8626637085808347e-05}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "particular", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 1.6640832929157547e-07}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "exhibits", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 1.5812880909240655e-06}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "privacy", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 3.6000653555864655e-08}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "companies", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 1.015564069461705e-07}, {"x": 0.046511627906976744, "y": 0.007462686567164179, "ox": 0.046511627906976744, "oy": 0.007462686567164179, "term": "intellectual", "cat25k": 1, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 6, "s": 0.40384615384615385, "os": -0.039031339031339034, "bg": 7.125924709617295e-07}, {"x": 0.1472868217054263, "y": 0.007462686567164179, "ox": 0.1472868217054263, "oy": 0.007462686567164179, "term": "kinesis", "cat25k": 1, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 19, "s": 0.12927350427350429, "os": -0.13903133903133905, "bg": 0.00017148099561866055}, {"x": 0.08527131782945736, "y": 0.01492537313432836, "ox": 0.08527131782945736, "oy": 0.01492537313432836, "term": "active", "cat25k": 2, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 11, "s": 0.23931623931623933, "os": -0.07008547008547009, "bg": 3.090101684074517e-07}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "case", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 5.092992634195741e-08}, {"x": 0.08527131782945736, "y": 0.007462686567164179, "ox": 0.08527131782945736, "oy": 0.007462686567164179, "term": "party", "cat25k": 1, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 11, "s": 0.22542735042735043, "os": -0.0774928774928775, "bg": 1.6578895117633514e-07}, {"x": 0.1472868217054263, "y": 0.007462686567164179, "ox": 0.1472868217054263, "oy": 0.007462686567164179, "term": "assist", "cat25k": 1, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 19, "s": 0.12927350427350429, "os": -0.13903133903133905, "bg": 1.3325049150276606e-06}, {"x": 0.08527131782945736, "y": 0.007462686567164179, "ox": 0.08527131782945736, "oy": 0.007462686567164179, "term": "dynamics", "cat25k": 1, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 11, "s": 0.22542735042735043, "os": -0.0774928774928775, "bg": 1.7660710812703968e-06}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "participate", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 4.900878306821692e-07}, {"x": 0.031007751937984492, "y": 0.01492537313432836, "ox": 0.031007751937984492, "oy": 0.01492537313432836, "term": "pertains", "cat25k": 2, "ncat25k": 4, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 4, "s": 0.5288461538461539, "os": -0.01623931623931624, "bg": 1.3139467785857334e-05}, {"x": 0.09302325581395349, "y": 0.007462686567164179, "ox": 0.09302325581395349, "oy": 0.007462686567164179, "term": "governance", "cat25k": 1, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 12, "s": 0.21153846153846156, "os": -0.0851851851851852, "bg": 1.8952000996000546e-06}, {"x": 0.062015503875968984, "y": 0.007462686567164179, "ox": 0.062015503875968984, "oy": 0.007462686567164179, "term": "thereof", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3162393162393162, "os": -0.05441595441595442, "bg": 1.745470286422947e-06}, {"x": 0.0697674418604651, "y": 0.007462686567164179, "ox": 0.0697674418604651, "oy": 0.007462686567164179, "term": "glue", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.27350427350427353, "os": -0.06210826210826211, "bg": 4.4538926353549105e-06}, {"x": 0.1317829457364341, "y": 0.007462686567164179, "ox": 0.1317829457364341, "oy": 0.007462686567164179, "term": "visualisation", "cat25k": 1, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 17, "s": 0.14743589743589747, "os": -0.12364672364672366, "bg": 5.178783101055321e-05}, {"x": 0.062015503875968984, "y": 0.007462686567164179, "ox": 0.062015503875968984, "oy": 0.007462686567164179, "term": "course", "cat25k": 1, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 8, "s": 0.3162393162393162, "os": -0.05441595441595442, "bg": 1.0110569752511788e-07}, {"x": 0.03875968992248062, "y": 0.01492537313432836, "ox": 0.03875968992248062, "oy": 0.01492537313432836, "term": "reliable", "cat25k": 2, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 5, "s": 0.47970085470085466, "os": -0.023931623931623933, "bg": 6.8605053448237e-07}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "offerings", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 1.426439580655292e-06}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "custom", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 1.8004617013963974e-07}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "transformations", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 4.420786686358815e-06}, {"x": 0.05426356589147286, "y": 0.01492537313432836, "ox": 0.05426356589147286, "oy": 0.01492537313432836, "term": "framework", "cat25k": 2, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 2, "ncat": 7, "s": 0.39743589743589747, "os": -0.039316239316239315, "bg": 5.542435208701476e-07}, {"x": 0.07751937984496123, "y": 0.007462686567164179, "ox": 0.07751937984496123, "oy": 0.007462686567164179, "term": "bash", "cat25k": 1, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 10, "s": 0.24145299145299146, "os": -0.06980056980056981, "bg": 4.605639018806289e-06}, {"x": 0.08527131782945736, "y": 0.007462686567164179, "ox": 0.08527131782945736, "oy": 0.007462686567164179, "term": "native", "cat25k": 1, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 11, "s": 0.22542735042735043, "os": -0.0774928774928775, "bg": 6.815181395849673e-07}, {"x": 0.0697674418604651, "y": 0.007462686567164179, "ox": 0.0697674418604651, "oy": 0.007462686567164179, "term": "aggregation", "cat25k": 1, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 9, "s": 0.27350427350427353, "os": -0.06210826210826211, "bg": 7.403102640316556e-06}, {"x": 0.03875968992248062, "y": 0.007462686567164179, "ox": 0.03875968992248062, "oy": 0.007462686567164179, "term": "deploy", "cat25k": 1, "ncat25k": 5, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 1, "ncat": 5, "s": 0.42735042735042733, "os": -0.03133903133903134, "bg": 3.0354034278810913e-06}, {"x": 0.15503875968992245, "y": 0.0, "ox": 0.15503875968992245, "oy": 0.0, "term": "architect", "cat25k": 0, "ncat25k": 21, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 20, "s": 0.11217948717948717, "os": -0.15413105413105413, "bg": 4.439924108377215e-06}, {"x": 0.046511627906976744, "y": 0.0, "ox": 0.046511627906976744, "oy": 0.0, "term": "parquet", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.37179487179487175, "os": -0.046438746438746445, "bg": 3.187725133087525e-05}, {"x": 0.0697674418604651, "y": 0.0, "ox": 0.0697674418604651, "oy": 0.0, "term": "match", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.24465811965811968, "os": -0.06951566951566952, "bg": 3.359002394203601e-07}, {"x": 0.062015503875968984, "y": 0.0, "ox": 0.062015503875968984, "oy": 0.0, "term": "integrate", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2831196581196581, "os": -0.06182336182336183, "bg": 2.104695443992081e-06}, {"x": 0.062015503875968984, "y": 0.0, "ox": 0.062015503875968984, "oy": 0.0, "term": "networking", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2831196581196581, "os": -0.06182336182336183, "bg": 3.4802821569153073e-07}, {"x": 0.05426356589147286, "y": 0.0, "ox": 0.05426356589147286, "oy": 0.0, "term": "servers", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.32264957264957267, "os": -0.05413105413105414, "bg": 3.5546875347137455e-07}, {"x": 0.10852713178294572, "y": 0.0, "ox": 0.10852713178294572, "oy": 0.0, "term": "snowflake", "cat25k": 0, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 14, "s": 0.1730769230769231, "os": -0.10797720797720799, "bg": 3.022514494576206e-05}, {"x": 0.062015503875968984, "y": 0.0, "ox": 0.062015503875968984, "oy": 0.0, "term": "dataflow", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2831196581196581, "os": -0.06182336182336183, "bg": 5.1956317725871975e-05}, {"x": 0.046511627906976744, "y": 0.0, "ox": 0.046511627906976744, "oy": 0.0, "term": "scripts", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.37179487179487175, "os": -0.046438746438746445, "bg": 6.900012437272419e-07}, {"x": 0.046511627906976744, "y": 0.0, "ox": 0.046511627906976744, "oy": 0.0, "term": "luigi", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.37179487179487175, "os": -0.046438746438746445, "bg": 8.257115569341881e-06}, {"x": 0.1317829457364341, "y": 0.0, "ox": 0.1317829457364341, "oy": 0.0, "term": "rds", "cat25k": 0, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 17, "s": 0.14102564102564102, "os": -0.13105413105413105, "bg": 2.5516847498560924e-05}, {"x": 0.062015503875968984, "y": 0.0, "ox": 0.062015503875968984, "oy": 0.0, "term": "preparation", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2831196581196581, "os": -0.06182336182336183, "bg": 6.318159115887359e-07}, {"x": 0.10852713178294572, "y": 0.0, "ox": 0.10852713178294572, "oy": 0.0, "term": "mentoring", "cat25k": 0, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 14, "s": 0.1730769230769231, "os": -0.10797720797720799, "bg": 6.8966094789956364e-06}, {"x": 0.062015503875968984, "y": 0.0, "ox": 0.062015503875968984, "oy": 0.0, "term": "detailed", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2831196581196581, "os": -0.06182336182336183, "bg": 3.050424682984138e-07}, {"x": 0.046511627906976744, "y": 0.0, "ox": 0.046511627906976744, "oy": 0.0, "term": "oozie", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.37179487179487175, "os": -0.046438746438746445, "bg": 0.00021875854525567405}, {"x": 0.05426356589147286, "y": 0.0, "ox": 0.05426356589147286, "oy": 0.0, "term": "avro", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.32264957264957267, "os": -0.05413105413105414, "bg": 5.473004405768547e-05}, {"x": 0.062015503875968984, "y": 0.0, "ox": 0.062015503875968984, "oy": 0.0, "term": "cloudera", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2831196581196581, "os": -0.06182336182336183, "bg": 0.00029166742621725573}, {"x": 0.07751937984496123, "y": 0.0, "ox": 0.07751937984496123, "oy": 0.0, "term": "rdbms", "cat25k": 0, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.22970085470085472, "os": -0.07720797720797722, "bg": 3.5921669208124764e-05}, {"x": 0.046511627906976744, "y": 0.0, "ox": 0.046511627906976744, "oy": 0.0, "term": "preferable", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.37179487179487175, "os": -0.046438746438746445, "bg": 6.798362274528066e-06}, {"x": 0.0697674418604651, "y": 0.0, "ox": 0.0697674418604651, "oy": 0.0, "term": "beam", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.24465811965811968, "os": -0.06951566951566952, "bg": 1.5148161644349777e-06}, {"x": 0.046511627906976744, "y": 0.0, "ox": 0.046511627906976744, "oy": 0.0, "term": "debugging", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.37179487179487175, "os": -0.046438746438746445, "bg": 2.8041766341514596e-06}, {"x": 0.046511627906976744, "y": 0.0, "ox": 0.046511627906976744, "oy": 0.0, "term": "reliability", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.37179487179487175, "os": -0.046438746438746445, "bg": 7.078026214531789e-07}, {"x": 0.05426356589147286, "y": 0.0, "ox": 0.05426356589147286, "oy": 0.0, "term": "helpful", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.32264957264957267, "os": -0.05413105413105414, "bg": 3.026137636996763e-07}, {"x": 0.05426356589147286, "y": 0.0, "ox": 0.05426356589147286, "oy": 0.0, "term": "catered", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.32264957264957267, "os": -0.05413105413105414, "bg": 1.0667707022094345e-05}, {"x": 0.0697674418604651, "y": 0.0, "ox": 0.0697674418604651, "oy": 0.0, "term": "unlimited", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.24465811965811968, "os": -0.06951566951566952, "bg": 7.547236897870957e-07}, {"x": 0.07751937984496123, "y": 0.0, "ox": 0.07751937984496123, "oy": 0.0, "term": "definition", "cat25k": 0, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.22970085470085472, "os": -0.07720797720797722, "bg": 4.341040757141755e-07}, {"x": 0.046511627906976744, "y": 0.0, "ox": 0.046511627906976744, "oy": 0.0, "term": "eg", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.37179487179487175, "os": -0.046438746438746445, "bg": 1.8561239158225532e-07}, {"x": 0.05426356589147286, "y": 0.0, "ox": 0.05426356589147286, "oy": 0.0, "term": "jira", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.32264957264957267, "os": -0.05413105413105414, "bg": 8.223959566315367e-06}, {"x": 0.046511627906976744, "y": 0.0, "ox": 0.046511627906976744, "oy": 0.0, "term": "designs", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.37179487179487175, "os": -0.046438746438746445, "bg": 3.635203345550343e-07}, {"x": 0.10077519379844961, "y": 0.0, "ox": 0.10077519379844961, "oy": 0.0, "term": "mpp", "cat25k": 0, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.1858974358974359, "os": -0.1002849002849003, "bg": 4.7304464267845156e-05}, {"x": 0.05426356589147286, "y": 0.0, "ox": 0.05426356589147286, "oy": 0.0, "term": "vertica", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.32264957264957267, "os": -0.05413105413105414, "bg": 0.00025521365028438093}, {"x": 0.0697674418604651, "y": 0.0, "ox": 0.0697674418604651, "oy": 0.0, "term": "mapping", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.24465811965811968, "os": -0.06951566951566952, "bg": 9.107099950776124e-07}, {"x": 0.062015503875968984, "y": 0.0, "ox": 0.062015503875968984, "oy": 0.0, "term": "perl", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2831196581196581, "os": -0.06182336182336183, "bg": 6.5673020189323e-07}, {"x": 0.0697674418604651, "y": 0.0, "ox": 0.0697674418604651, "oy": 0.0, "term": "organisation", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.24465811965811968, "os": -0.06951566951566952, "bg": 7.97780892668952e-07}, {"x": 0.07751937984496123, "y": 0.0, "ox": 0.07751937984496123, "oy": 0.0, "term": "third", "cat25k": 0, "ncat25k": 10, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 10, "s": 0.22970085470085472, "os": -0.07720797720797722, "bg": 2.0932742272235986e-07}, {"x": 0.4573643410852712, "y": 0.0, "ox": 0.4573643410852712, "oy": 0.0, "term": "streams", "cat25k": 0, "ncat25k": 65, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 63, "s": 0.03311965811965812, "os": -0.45413105413105415, "bg": 1.4097715968617142e-05}, {"x": 0.10077519379844961, "y": 0.0, "ox": 0.10077519379844961, "oy": 0.0, "term": "informatica", "cat25k": 0, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.1858974358974359, "os": -0.1002849002849003, "bg": 3.944503864855229e-05}, {"x": 0.05426356589147286, "y": 0.0, "ox": 0.05426356589147286, "oy": 0.0, "term": "flows", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.32264957264957267, "os": -0.05413105413105414, "bg": 1.2900821616469262e-06}, {"x": 0.08527131782945736, "y": 0.0, "ox": 0.08527131782945736, "oy": 0.0, "term": "athena", "cat25k": 0, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 11, "s": 0.21260683760683763, "os": -0.08490028490028491, "bg": 1.1031688525288892e-05}, {"x": 0.05426356589147286, "y": 0.0, "ox": 0.05426356589147286, "oy": 0.0, "term": "option", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.32264957264957267, "os": -0.05413105413105414, "bg": 2.266388339069373e-07}, {"x": 0.046511627906976744, "y": 0.0, "ox": 0.046511627906976744, "oy": 0.0, "term": "kanban", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.37179487179487175, "os": -0.046438746438746445, "bg": 8.657946190864425e-05}, {"x": 0.062015503875968984, "y": 0.0, "ox": 0.062015503875968984, "oy": 0.0, "term": "kubernetes", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2831196581196581, "os": -0.06182336182336183, "bg": 0.00029166742621725573}, {"x": 0.10852713178294572, "y": 0.0, "ox": 0.10852713178294572, "oy": 0.0, "term": "ci", "cat25k": 0, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 14, "s": 0.1730769230769231, "os": -0.10797720797720799, "bg": 3.181090507364622e-06}, {"x": 0.05426356589147286, "y": 0.0, "ox": 0.05426356589147286, "oy": 0.0, "term": "cd", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.32264957264957267, "os": -0.05413105413105414, "bg": 7.3331485310064e-08}, {"x": 0.10077519379844961, "y": 0.0, "ox": 0.10077519379844961, "oy": 0.0, "term": "gathering", "cat25k": 0, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.1858974358974359, "os": -0.1002849002849003, "bg": 2.356942646520246e-06}, {"x": 0.046511627906976744, "y": 0.0, "ox": 0.046511627906976744, "oy": 0.0, "term": "pl", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.37179487179487175, "os": -0.046438746438746445, "bg": 8.782813322473873e-07}, {"x": 0.10077519379844961, "y": 0.0, "ox": 0.10077519379844961, "oy": 0.0, "term": "teradata", "cat25k": 0, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.1858974358974359, "os": -0.1002849002849003, "bg": 0.00011782779920330282}, {"x": 0.0697674418604651, "y": 0.0, "ox": 0.0697674418604651, "oy": 0.0, "term": "specialist", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.24465811965811968, "os": -0.06951566951566952, "bg": 7.354684909771907e-07}, {"x": 0.046511627906976744, "y": 0.0, "ox": 0.046511627906976744, "oy": 0.0, "term": "lake", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.37179487179487175, "os": -0.046438746438746445, "bg": 1.406859895746992e-07}, {"x": 0.10077519379844961, "y": 0.0, "ox": 0.10077519379844961, "oy": 0.0, "term": "add", "cat25k": 0, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.1858974358974359, "os": -0.1002849002849003, "bg": 6.713374522347598e-08}, {"x": 0.0697674418604651, "y": 0.0, "ox": 0.0697674418604651, "oy": 0.0, "term": "usability", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.24465811965811968, "os": -0.06951566951566952, "bg": 3.91430711871115e-06}, {"x": 0.08527131782945736, "y": 0.0, "ox": 0.08527131782945736, "oy": 0.0, "term": "ssrs", "cat25k": 0, "ncat25k": 11, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 11, "s": 0.21260683760683763, "os": -0.08490028490028491, "bg": 0.00020412897239619577}, {"x": 0.10077519379844961, "y": 0.0, "ox": 0.10077519379844961, "oy": 0.0, "term": "ssas", "cat25k": 0, "ncat25k": 13, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 13, "s": 0.1858974358974359, "os": -0.1002849002849003, "bg": 0.00021381930623858947}, {"x": 0.09302325581395349, "y": 0.0, "ox": 0.09302325581395349, "oy": 0.0, "term": "exchange", "cat25k": 0, "ncat25k": 12, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 12, "s": 0.20299145299145302, "os": -0.0925925925925926, "bg": 3.1114559295715726e-07}, {"x": 0.2635658914728682, "y": 0.0, "ox": 0.2635658914728682, "oy": 0.0, "term": "enhance", "cat25k": 0, "ncat25k": 35, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 34, "s": 0.07371794871794872, "os": -0.2618233618233618, "bg": 3.0404238207727386e-06}, {"x": 0.4573643410852712, "y": 0.0, "ox": 0.4573643410852712, "oy": 0.0, "term": "awareness", "cat25k": 0, "ncat25k": 65, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 63, "s": 0.03311965811965812, "os": -0.45413105413105415, "bg": 6.120662448040555e-06}, {"x": 0.11627906976744183, "y": 0.0, "ox": 0.11627906976744183, "oy": 0.0, "term": "optimising", "cat25k": 0, "ncat25k": 15, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 15, "s": 0.16132478632478636, "os": -0.11566951566951568, "bg": 7.913333175066671e-05}, {"x": 0.44186046511627897, "y": 0.0, "ox": 0.44186046511627897, "oy": 0.0, "term": "digging", "cat25k": 0, "ncat25k": 63, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 61, "s": 0.041666666666666664, "os": -0.43874643874643876, "bg": 4.5665843558552035e-05}, {"x": 0.44186046511627897, "y": 0.0, "ox": 0.44186046511627897, "oy": 0.0, "term": "petabyte", "cat25k": 0, "ncat25k": 63, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 61, "s": 0.041666666666666664, "os": -0.43874643874643876, "bg": 0.00127103193207272}, {"x": 0.4806201550387596, "y": 0.0, "ox": 0.4806201550387596, "oy": 0.0, "term": "pragmatic", "cat25k": 0, "ncat25k": 68, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 66, "s": 0.022435897435897433, "os": -0.4772079772079772, "bg": 7.896838572466955e-05}, {"x": 0.47286821705426346, "y": 0.0, "ox": 0.47286821705426346, "oy": 0.0, "term": "letting", "cat25k": 0, "ncat25k": 67, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 65, "s": 0.027777777777777776, "os": -0.46951566951566953, "bg": 1.5485710501074707e-05}, {"x": 0.47286821705426346, "y": 0.0, "ox": 0.47286821705426346, "oy": 0.0, "term": "perfect", "cat25k": 0, "ncat25k": 67, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 65, "s": 0.027777777777777776, "os": -0.46951566951566953, "bg": 2.158250031107854e-06}, {"x": 0.47286821705426346, "y": 0.0, "ox": 0.47286821705426346, "oy": 0.0, "term": "enemy", "cat25k": 0, "ncat25k": 67, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 65, "s": 0.027777777777777776, "os": -0.46951566951566953, "bg": 9.19436433559826e-06}, {"x": 0.47286821705426346, "y": 0.0, "ox": 0.47286821705426346, "oy": 0.0, "term": "directed", "cat25k": 0, "ncat25k": 67, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 65, "s": 0.027777777777777776, "os": -0.46951566951566953, "bg": 5.32743741736325e-06}, {"x": 0.44186046511627897, "y": 0.0, "ox": 0.44186046511627897, "oy": 0.0, "term": "amidst", "cat25k": 0, "ncat25k": 63, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 61, "s": 0.041666666666666664, "os": -0.43874643874643876, "bg": 8.14989936210337e-05}, {"x": 0.44961240310077516, "y": 0.0, "ox": 0.44961240310077516, "oy": 0.0, "term": "continually", "cat25k": 0, "ncat25k": 64, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 62, "s": 0.035256410256410256, "os": -0.4464387464387464, "bg": 2.3248698072907915e-05}, {"x": 0.44961240310077516, "y": 0.0, "ox": 0.44961240310077516, "oy": 0.0, "term": "extras", "cat25k": 0, "ncat25k": 64, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 62, "s": 0.035256410256410256, "os": -0.4464387464387464, "bg": 8.870587499725477e-06}, {"x": 0.1472868217054263, "y": 0.0, "ox": 0.1472868217054263, "oy": 0.0, "term": "categories", "cat25k": 0, "ncat25k": 20, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 19, "s": 0.12179487179487179, "os": -0.14643874643874644, "bg": 2.1852408419489137e-07}, {"x": 0.05426356589147286, "y": 0.0, "ox": 0.05426356589147286, "oy": 0.0, "term": "messaging", "cat25k": 0, "ncat25k": 7, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 7, "s": 0.32264957264957267, "os": -0.05413105413105414, "bg": 1.198785322209519e-06}, {"x": 0.10852713178294572, "y": 0.0, "ox": 0.10852713178294572, "oy": 0.0, "term": "troubleshoot", "cat25k": 0, "ncat25k": 14, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 14, "s": 0.1730769230769231, "os": -0.10797720797720799, "bg": 2.8565016016812553e-05}, {"x": 0.12403100775193797, "y": 0.0, "ox": 0.12403100775193797, "oy": 0.0, "term": "traditional", "cat25k": 0, "ncat25k": 17, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 16, "s": 0.1495726495726496, "os": -0.12336182336182337, "bg": 6.680241474028682e-07}, {"x": 0.22480620155038755, "y": 0.0, "ox": 0.22480620155038755, "oy": 0.0, "term": "analysts", "cat25k": 0, "ncat25k": 30, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 29, "s": 0.08760683760683761, "os": -0.22336182336182336, "bg": 7.779482259360158e-06}, {"x": 0.062015503875968984, "y": 0.0, "ox": 0.062015503875968984, "oy": 0.0, "term": "profiling", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2831196581196581, "os": -0.06182336182336183, "bg": 5.973146971147833e-06}, {"x": 0.18604651162790695, "y": 0.0, "ox": 0.18604651162790695, "oy": 0.0, "term": "loads", "cat25k": 0, "ncat25k": 25, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 24, "s": 0.09401709401709402, "os": -0.1849002849002849, "bg": 5.1999708368302234e-06}, {"x": 0.062015503875968984, "y": 0.0, "ox": 0.062015503875968984, "oy": 0.0, "term": "speech", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2831196581196581, "os": -0.06182336182336183, "bg": 5.132396750050899e-07}, {"x": 0.0697674418604651, "y": 0.0, "ox": 0.0697674418604651, "oy": 0.0, "term": "olap", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.24465811965811968, "os": -0.06951566951566952, "bg": 2.3970468382952204e-05}, {"x": 0.062015503875968984, "y": 0.0, "ox": 0.062015503875968984, "oy": 0.0, "term": "summary", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2831196581196581, "os": -0.06182336182336183, "bg": 1.9855679243613783e-07}, {"x": 0.1317829457364341, "y": 0.0, "ox": 0.1317829457364341, "oy": 0.0, "term": "indexing", "cat25k": 0, "ncat25k": 18, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 17, "s": 0.14102564102564102, "os": -0.13105413105413105, "bg": 9.702152479599086e-06}, {"x": 0.062015503875968984, "y": 0.0, "ox": 0.062015503875968984, "oy": 0.0, "term": "supplement", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2831196581196581, "os": -0.06182336182336183, "bg": 1.2267033696314862e-06}, {"x": 0.062015503875968984, "y": 0.0, "ox": 0.062015503875968984, "oy": 0.0, "term": "exports", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2831196581196581, "os": -0.06182336182336183, "bg": 1.7297108431572802e-06}, {"x": 0.0697674418604651, "y": 0.0, "ox": 0.0697674418604651, "oy": 0.0, "term": "monitor", "cat25k": 0, "ncat25k": 9, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 9, "s": 0.24465811965811968, "os": -0.06951566951566952, "bg": 4.0999420085980337e-07}, {"x": 0.062015503875968984, "y": 0.0, "ox": 0.062015503875968984, "oy": 0.0, "term": "meta", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2831196581196581, "os": -0.06182336182336183, "bg": 1.251350285167088e-06}, {"x": 0.062015503875968984, "y": 0.0, "ox": 0.062015503875968984, "oy": 0.0, "term": "archiving", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2831196581196581, "os": -0.06182336182336183, "bg": 6.496651991000513e-06}, {"x": 0.062015503875968984, "y": 0.0, "ox": 0.062015503875968984, "oy": 0.0, "term": "innovating", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2831196581196581, "os": -0.06182336182336183, "bg": 5.657828666803397e-05}, {"x": 0.046511627906976744, "y": 0.0, "ox": 0.046511627906976744, "oy": 0.0, "term": "referral", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.37179487179487175, "os": -0.046438746438746445, "bg": 1.1542465886242073e-06}, {"x": 0.062015503875968984, "y": 0.0, "ox": 0.062015503875968984, "oy": 0.0, "term": "wellness", "cat25k": 0, "ncat25k": 8, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 8, "s": 0.2831196581196581, "os": -0.06182336182336183, "bg": 1.6953060366457353e-06}, {"x": 0.046511627906976744, "y": 0.0, "ox": 0.046511627906976744, "oy": 0.0, "term": "sqoop", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.37179487179487175, "os": -0.046438746438746445, "bg": 0.00021875854525567405}, {"x": 0.046511627906976744, "y": 0.0, "ox": 0.046511627906976744, "oy": 0.0, "term": "programmes", "cat25k": 0, "ncat25k": 6, "neut25k": 0, "neut": 0, "extra25k": 0, "extra": 0, "cat": 0, "ncat": 6, "s": 0.37179487179487175, "os": -0.046438746438746445, "bg": 6.726449104154692e-07}], "docs": {"categories": ["data scientist", "data engineer"], "labels": [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], "texts": ["", "years relevant experience preferred healthcare industry knowledge experience preferred", "pursuing phd ms cs math statistics physics economics quantitative field expected graduation date winter spring summer also consider candidates quantitative backgrounds currently enrolled business school ability think execute multiple altitudes strategy vision execution interpersonal skills demonstrated ability influence outcomes communicate technical content general audiences including ability \"story tell\" data ability write mentor code development sql python r experience dashboard design data viz tools e tableau plus experience building data pipelines plus stock yearly employee travel coupon competitive salary paid time medical dental vision insurance life disability coverage 401k flexible spending accounts apple equipment daily breakfast lunch dinner", "", "phd ms computational biology computer science statistics related quantitative science field years ms project based work computational quantitative data analysis biology related fields proficiency analysing data r matlab python strong interested biology immunological diseases liaise scientists immunology inflammation help define address biological questions computational analytical approach proficiency biostatistics linear non linear regression models dimensionality reduction clustering ai machine learning methods ability develop benchmark apply predictive algorithms identify novel biomarkers dissect gene disease relationships generate hypotheses experience bayesian analysis causal inference excellent written oral communication skills excellent interpersonal team skills", "bachelor' degree accredited college university computer science computational linguistics statistics mathematics engineering bioinformatics physics operations research related fields minimum years relevant work experience bachelor' degree minimum years relevant work experience master' degree proven track record driving value commercial setting using data science skills depth knowledge various modeling algorithms e g linear glms trees based models neural networks clustering pca time series models proficiency r e g ggplot2 cluster dplyr caret python e g pandas scikit learn bokeh nltk spark - mllib h20 statistical tools depth knowledge databases data modeling hadoop distributed computing frameworks experience software development environment agile code management versioning e g git strong sql skills experience knowledge ability understand complex ambiguous business needs applying right tools approaches must curious self motivating driven passion problem solving collaborative team player excellent communication skills written verbal experience developing testing machine learning statistical projects master' degree accredited college university business analytics computer science machine learning computational linguistics statistics mathematics engineering bioinformatics physics operations research related fields experience agriculture commodity businesses experience deep learning neural networks experience weather geospatial data experience \"computer vision\" applications images video experience back testing strategies experience working cloud environment e g amazon web services experience big data development hadoop spark frameworks", "strong working knowledge variety machine learning analytical techniques understanding applicable software engineering programing skills coding experience using java scala python addition machine learning data mining libraries experience working high performance computing distributed parallel systems e g hadoop tez spark strong mathematical statistical background thorough understanding probability experience data science machine learning degree computer science mathematics another empirical science commercial experience building deploying scalable software solutions employ machine learning algorithms production environments self starter comfortable juggling multiple projects strong communication skills experience nlp techniques tools plus developed production data science solutions healthcare ad tech web search prior experience building recommendation systems natural language processing solutions healthcare domain expertise", "", "", "", "master' degree relevant technical discipline math engineering computer science statistics similar field least years job experience bachelor' degree relevant technical discipline years job experience minimum years experience programming languages data science python java r etc strong mathematical statistics skills strong interpersonal communication skills written oral ability communicate complex technical statistical concepts non technical audience phd relevant technical discipline preferred credit risk management fraud detection industry experience developing scoring models experiences advanced machine learning algorithms including deep learning boosting tree based methods experience using cloud distributed computing frameworks hadoop spark aws emr bigquery etc demonstrated ability apply modern data exploration visualization techniques deliver actionable insights", "", "", "bachelor' degree plus years experience data analytics master' degree phd least year experience open source programming languages large scale data analysis least year experience machine learning master' degree phd experience working aws least years' experience python scala r least years' experience machine learning", "", "", "", "", "phd ms computational biology computer science statistics related quantitative science field years ms project based work computational quantitative data analysis biology related fields proficiency analyzing data r matlab python strong interested biology immunological diseases liaise scientists immunology inflammation help define address biological questions computational analytical approache proficiency biostatistics linear non linear regression models dimensionality reduction clustering ai machine learning methods ability develop benchmark apply predictive algorithms identify novel biomarkers dissect gene disease relationships generate hypotheses experience bayesian analysis causal inference excellent written oral communication skills excellent interpersonal team skills", "bachelors degree computer science related degree years related work experience experience working open source project plus required good english language skills", "phd ms degree computer science statistics electrical engineering applied math operations research econometrics related fields deep understanding statistical modeling machine learning deep learning data mining concepts track record solving problems methods proficient one programming languages python java scala c familiar one machine learning statistical modeling tools r scikit learn spark mllib knowledge experience working relational databases sql strong analytical quantitative problem solving ability experience big data techniques hadoop mapreduce hive pig spark years experience machine learning data mining information retrieval statistical analysis knowledge cloud platforms aws azure experience developing applications cloud platforms using various cloud services", "", "", "", "", "ph degree highly quantitive field computer science machine learning operational research statistics mathematics etc equivalent experience years hands experience principal data scientist experience building dmp dsp id graph martech adtech highly desirable ability develop experimental analytic plans data modeling processes use strong baselines ability accurately determine cause effect relations demonstrable track record dealing well ambiguity prioritizing needs delivering results dynamic environment job function engineeringinformation technology", "", "upstream oil gas industry experience preferred particularly areas drilling completion production operations", "", "", "", "", "", "", "", "", "", "", "bachelor' degree plus years experience data analytics master' degree plus year experience data analytics phd least year experience open source programming languages large scale data analysis least year experience machine learning least year experience relational databases master' degree phd least year experience working aws least years' experience python scala r least years' experience machine learning least years' experience sql", "", "", "", "", "", "", "", "", "", "", "experience data visualizaiton using tools tableau", "experience data mining understanding machine learning operations research analytical mind business acumen strong math skills e g statistics algebra problem solving aptitude excellent communication presentation skills bsc ba computer science engineering relevant field graduate degree data science quantitative field preferred", "", "", "phd bioinformatics computer science similar experience experience relational database management systems solid skills r python linux experience systems biology research data solid data visualization skills basic knowledge web development experience data workflow management tools advantage experience big data cloud computing advantage ability work team pursue goals focused way excellent written oral communication skills english", "employment payment social benefits consistent research institutes", "", "", "", "master phd degree engineering neuroscience bioinformatics quantitative fields strong analysis programming experience excellent understanding machine learning techniques algorithms experience common data science toolkits libraries scikit learn pandas numpy scipy matlab etc programming languages python working knowledge linux os sql years python programming product development experience great verbal written skills team player able work independently", "", "", "", "", "", "", "bachelor' master' degree quantitative field knowledge python r knowledge algorithms data mining machine learning natural language processing possess understanding statistical procedures used advanced analytics experience processing large amounts structured unstructured data using spark hive big data technologies experience building scalable data models performing complex relational database queries using sql oracle mysql etc attention detail demonstrated ability detect resolve data analytics quality issues outstanding verbal written communication skills experienced user data visualization tools e g tableau matplotlib ggplot2 etc", "", "", "", "experience multi touch attribution modeling media mix modeling data visualization big plus", "", "", "strong experience designing quantitative modeling experiments solve \"fuzzy\" real world problems good communication skills clearly understand problems experts outside field well collaborate data scientists deep understanding deep learning models methods allow design redesign models solve new applications strong experimental design allow verify utility models practice passion research curiosity calls go beyond \"good enough\" create something innovative exciting masters quantitative field computer science electrical engineering statistics biostatistics applied math etc phd preferred years working data science deep learning required experience deep learning libraries pytorch tensorflow keras etc well statistical modeling software scikit learn statsmodels python r experience directly interfacing customers particularly medical professionals experience turning research projects consumer products ability write beautiful production ready code", "", "", "", "social environment built bars", "", "ph life sciences related field expertise molecular cellular biology experience \"omics\" data analysis experience data analysis compound screens strong plus experience toxicogenomics plus experience r based data analysis plus excellent communication skills ability interact professionally levels staff collaborators customers experience working interdisciplinary teams fluency english permanent position within vigorous exciting professional environment promoted open culture spirit community diverse international workforce dynamic working environment fosters creativity innovations teamwork capital forming benefits holiday pay annual bonus payment depending performance", "phd required computer science related discipline equivalent combination educational training relevant experience accomplishments strong coding algorithm prototyping skills ability explain document work proficiency one following python c c sql experience working data analysis statistics machine learning scientific computing address basic research questions commensurate achievements strong problem solving skills passion answering hard questions data ability communicate complex ideas relevant stakeholders experience collaborative multi disciplinary research environment eagerness collaborate technical non technical colleagues experience database design building data driven web applications", "", "", "", "", "", "first must true startup spirit willing wear multiple hats deliver end end ability thinking box evaluating results based customer value years industry experience applying ai ml preferably well known security products services malware detection anomaly detection security analytics data security experience applying ai ml one domains highly desirable hands experience relevant technology stacks cuda python r spark flink tensorflow hands experience using modern big data pipeline natural language process nlp data mining experience highly desirable security research experience strong security domain knowledge highly desirable energetic self starter desire work dynamic fast paced environment excellent verbal written communication skills ability influence without authority phd computer science statistics electrical engineering equivalent technical degree", "", "bachelors quantitative discipline mathematics statistics physics computer science engineering background linear algebra multivariable calculus experience python plus experience cleaning visualizing data plus", "phd bioinformatics related field proficiency database management scientific curation bioinformatics software engineering proficiency mysql oracle graph databases python r java programming skills demonstrated understanding posttranslational modifications cell signaling", "", "ms statistics machine learning operations research applied math equivalent - years data science experience experience statistical mathematical software proven ability develop system prototypes sql skills excellent oral written communication skills strong documentation skills understanding machine learning techniques ability invent big data experience hadoop spark map reduce hive etc light lifting lbs office environment office standard office equipment work usually performed office setting free disagreeable elements", "ph degree bioinformatics computer science biostatistics applied mathematics applied physics related discipline experience analyzing multimodal omics data cancer immunotherapy patients knowledge immunology tumor immunology tumor biology genetics proficiency python r", "", "degree computer science statistics econometrics mathematics information science related field years experience data science years experience digital analytics ideally media proven ability digital metrics systems google analytics adobe analytics bigquery experience plus facile statistical computing using r sas spss plus data science studio experience plus hands knowledge database manipulation environments mysql proven ability python similar solid experience machine learning data mining expertise decomposing problem working technical approach attacking problem systematic way experience working digital product teams agile environment strong analysis experimental design skills keen sense data gaps inconsistencies experience managing data driven research project inception client communication experience developing data visualization independently tools excellent oral written visual communication skills particularly explaining complex quantitative information non technical audiences strong collaboration skills tolerance zeal multitasking time show clear sense priorities commitment follow ability work independently little supervision well collaborative team environment self starter ability work constantly changing environment", "", "minimum years experience upstream g reservoir engineer data scientist petrophysicist msc petroleum engineering related major algorithmic thinking working knowledge upstream data experience analytical simulation tools field familiarity reservoir simulation phd petroleum engineering related major demonstrated experience statistical analysis quantitative analytics forecasting predictive analytics multivariate testing optimization algorithms", "", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "restaurant retail industry experience nice", "", "", "jobs rated report best jobs toughest jobs fill best jobs retail time holiday shopping jobs rated report best jobs best jobs advertising", "", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "managing functions complex environment multiple constituencies complex modeling analytical methodology including longitudinal analyses multi level modeling multivariate analyses variance summarizing presenting findings complex analytical modeling exercises creating standard reports presents complex data developing selling driving suitable partnerships alliances transactions closure performing ongoing analytic projects support business strategies tactics including network optimization incorporation unstructured information roi analysis working multi source data integration development bi tool cognos tableau applying industry best practices analytic processes create time efficiencies making recommendations data architecture strategy developing deploying analytical tools data science techniques analyze complex data sets monitoring trends data science alternative sources", "", "", "strong quantitative skills knowledge deep machine learning highly motivated infinitely curious self starter passion ai python proficiency expertise pytorch tensorflow essential perseverant capable thinking outside box excellent communication skills team player capable leading independent research efforts master' degree phd quantitative field data science machine learning mathematics physics statistics engineering computational neuroscience biology computer science deep understanding ai machine learning theory algorithms techniques evidenced completion foundational deep learning relevant coursework linear algebra advanced calculus mathematical optimization advanced statistics signal processing information theory ai machine learning classes etc substantial experience designing implementing neural networks python using pytorch tensorflow experience working range real world data types stages data science machine learning pipeline data fetching pre processing visualization modeling interpretation etc ability distill complex ideas results core essence communicate clearly non specialists presentations top tier ai deep machine learning conferences publications ai machine deep learning peer reviewed journals enthusiasm ai deep learning demonstrated kaggle contributions github commits relevant community activities working knowledge high performance distributed computing code optimization parallelization gpu clusters git method version control proficient c c solid understanding sql query development mysql sqlite postgres", "", "", "passionate subjects experimental design data visualization statistical analysis techniques experience looker tableau data visualization software plus", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "", "", "msc phd level field computer science machine learning applied statistics mathematics experience statistical modelling machine learning techniques programming experience least two following languages r python scala sql experience applying data science methods business problems experience applying advanced analytical statistical methods commercial world strong presentation communication skills ability explain complex analytical concepts people fields knowledge distributed computing nosql technologies bonus technical non technical training courses attendance one international ml conference per year e g nips icml mckinsey benefits competitive salary annual bonus generous pension scheme etc excellent healthcare office events", "", "bachelor' degree stem field science technology engineering math accredited college university minimum years experience analytics development industrial applications commercial industrial setting desired characteristics master' degree stem field science technology engineering math accredited college university ph stem field science technology engineering math accredited college university demonstrated skill data management methods demonstrated skill feature extraction realtime analytics development deployment", "", "", "", "", "subject call back times ability travel area organization local remote needed must provide transportation required sit extended periods ability understand complex verbal written communications respond verbally writing appropriate typical mediums communication include face face dialog telephone memos electronic mail ability interpret equipment status indicators determine appropriate operating condition indicators may include visual auditory techniques cues ability read understand technical manuals documentation determine correct action safety precautions conditions proper hardware software operation ability work varying hours due accessibility individuals equipment involved different projects need minimize system downtime user interruption recover hardware software failures occasionally experience stressful working conditions due tight project schedules hardware software problems ability occasionally lift move equipment pounds without assistance must occasionally lift move equipment pounds assistance ability occasionally crouch kneel bend crawl access inspect connect position perform operations equipment locations user equipment locations may present close quarters ability occasionally use small hand tools able manipulate small equipment components screws nuts fastening devices usually found computer equipment subject regular periods repetitive hand motion operation computer terminals equipment", "", "", "phd computational quantitative discipline e g statistics computer science biomedical informatics genetics physics epidemiology health economics master' degree similar field study deep understanding ml including strong knowledge mathematical underpinnings behind various methods e g regression techniques neural networks decision trees clustering pattern recognition dimensionality reduction proven experience applied statistics ml business setting deep understanding tools trade including variety modern programming languages r python javascript open source technologies linux tensorflow hadoop spark experience effective data visualization approaches keen eye detail visual communication findings comfort working communicating non technical teams translate business questions analytically actionable questions strong desire build meaningful solutions life sciences business task oriented ability set goals complete deliverables domain knowledge clinical data real world data life sciences related research data expertise data science related tools e g sql tableau d3", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "", "years professional experience data scientist machine learning engineer working developing optimizing implementing machine learning models least projects professional experience machine learning libraries scikit learn mllib professional experience implementing multiple machine learning models production environment bachelor data science analytics statistics mathematics physics economics computer science equivalent experience deep understanding statistical concepts applying real world problems master degree phd data science analytics statistics mathematics physics economics computer science another quantitative discipline experience petabytes data experience deep learning frameworks tensorflow keras", "familiarity aws ecosystem plus", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "bachelor' degree preferably mathematically intensive field enthusiasm analytically intensive work excitement learn technology customer operations analytics enthusiasm improve business performance journey analytics clients understanding statistical advanced analytic methods descriptive statistics predictive analytics machine learning ability work collaboratively team environment effectively people levels organization confidence sharing views perspectives senior clients colleagues", "", "bachelors degree statistics computer science economics physics quantitative field experience using applied statistics machine learning proficiency python r experience working imperfect data passion eagerness constantly learn teach others masters phd student statistics computer science economics physics quantitative field internship work experience applied statistics machine learning years experience", "strong data mining machine learning background experience big data environment phd computer science math related quantitative field experience recommender systems information retrieval graph analysis statistical modeling neural network natural language processing experience large scale data analysis building high performance computational software experience e commerce analyzing clickstream data experience big data environment including spark hadoop hive etc proficiency least one statistics data analysis package python r proficiency least one programming language java python etc solid coding practices including good design documentation unit testing integration testing experience working real world noisy data ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "", "", "", "experience ms office word access excel powerpoint outlook required experience managing employees experience sas sql logistic regression multiple regression required excellent written oral presentation skills ability explain present complicated advanced analytical methodology results non technical audiences ability provide strategic insights improve client campaign performance ability execute advanced analytic tasks including limited modeling segmentation doe forecasting etc create meaningful analytical outputs must experience modeling statistical concepts experiences applying statistical techniques regression anova cluster analysis factor analysis time series forecasting experimental design etc solve business problems possess core database marketing knowledge understanding merkle client data relational database concepts direct marketing concepts", "graduate degree computer science electrical engineering applied math related stem majors machine learning algorithms development experience years industry experience extensive knowledge details algorithm used machine learning experienced building large scale data analysis system extensive knowledge experience successfully developing implementing machine learning projects familiar python php r html css sql mongodb apache hadoop spark aws good written verbal presentation communication skills strong project management leadership skills", "ms years experience new ph deep technical skills including computer programming r python similar data management sql data visualization techniques multivariate analysis data science unit operation model building mechanistic machine learning bioprocess economic modelling packages biosolve superpro bio g similar total cost ownership net present value analysis approaches systems biology proteomics analysis understanding scripting approaches build automation across platforms deltav pi sipat tecan similar approaches chemometric modeling spectral data sources approaches complex residence time distribution modeling", "experience turn research results commercialization highly valued", "", "", "", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "", "", "b b statistics economics engineering computer science mathematics related quantitative field required position proven track record end end experience building analytical frameworks inform business strategy deep knowledge understanding statistical econometric modeling methods significance testing causal inference methods include limited regression analysis forecasting bootstrapping outlier detection feature selection decision trees least years industrial experience working analytics business strategy role exceptional programming skills python r knowledge tableau data visualization tools preferred strong knowledge databases related languages tools sql nosql hive etc ability work dynamic cross functional environment strong attention detail effective communication presentation skills ability explain complex analyses simple terms business leaders strong relationship building collaborative skills exceptional problem solving skills inspiring company mission amazing work environment san francisco ca competitive compensation including equity", "", "qualifications listed minimum acceptable considered position salary offers based candidates education level years experience relevant position also take account information provided hiring manager organization regarding work level position qualifications listed minimum acceptable considered position salary offers based candidates education level years experience relevant position also take account information provided hiring manager organization regarding work level position qualifications listed minimum acceptable considered position salary offers based candidates education level years experience relevant position also take account information provided hiring manager organization regarding work level position qualifications listed minimum acceptable considered position salary offers based candidates education level years experience relevant position also take account information provided hiring manager organization regarding work level position", "", "", "degrees physics mathematics computer science engineering plus", "degree analytical field e g computer science engineering mathematics statistics operations research management science years experience role data analysis metrics development years hands experience analyzing interpreting data drawing conclusions defining recommended actions reporting results across stakeholders years sql development experience writing queries years hands project management experience years experience data visualization tools years experience packages r tableau spss sas stata etc years experience scripting python php experience leveraging data driven models drive business decisions experience using data access tools building visualizations using large datasets multiple data sources experience thinking analytically experience communicating data organizational levels experienced packages numpy scipy pandas scikit learn dplyr ggplot2 knowledge statistics optimization techniques hands experience medium large datasets e data extraction cleaning analysis presentation", "", "", "master' degree required mathematics information technology statistics data science related field least plus years progressively responsible hands quantitative modeling skills sql sas r python experience significant experience working relational databases associated query extraction languages strong knowledge predictive models classification regression models decision trees time series data mining etc ability stay organized meet deadlines willingness work team member self starter able administer number open ongoing assignments one time assignments routinely unstructured requiring autonomy independent judgment depth experience successfully harmonizing diverse competing interests ability clearly articulate position sound logic supporting empirical evidence impartiality ability effectively represent organization variety internal external constituencies superior verbal written communication skills ensures behavior behavior others consistent highest ethical standards aligns values organization ability promote collaboration unifying teams setting common goals incentivizing collaborative behavior demonstrated success establishing maintaining positive working relationships others internally externally achieve goals organization strong ability build credibility organize effectively solve problems quickly communicate clearly possesses balance emotional intelligence required meet diverse needs divisions offices proven ability navigate resolve various types conflict timely productive manner proven transformation skills include ability consistently execute high level drive positive change desire build established programs teams demonstrated agility ability navigate complex environments ability foster environment creativity innovation focusing empowerment support staff tools continuous process improvement supports individuals teams process excellence project management problem solving value creation drive toward required outcomes surfaces capacity pacing resourcing issues requiring leadership attention ensures organizational alignment effective stakeholder engagement communication demonstrated ability think broadly strategically including ability translate long term goals objectives short term tactical plans operational activities effectively assesses progress identifying articulating clear consistent key performance indicators", "years relevant industry experience graduate degree statistics applied mathematics computer science physical sciences similar technical field experience developing deploying machine learning deep learning solutions versatility communicate clearly technical non technical audiences python numpy pandas sklearn xgboost tensorflow mysql hive java google cloud platform", "", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems ability build scalable systems analyze huge data sets make actionable recommendations strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "pursuing graduated masters quantitative field operation research computer science engineering applied math statistics physics analytics etc equivalent work experience proven leadership applying scaling analytic techniques deliver impactful insights data academics industry strong written verbal communication skills influence others take action able balance multiple priorities good social skills self motivated dynamic mentality strong enthusiasm curiosity intersection business technology years data science experience experience disrupting current business practices cpg related industries help crafting new go market models experience analytical tools applications including unix linux big data ecosystem hadoop spark mapreduce sql hiv scientific computing r python c java scala etc high performance parallel distributing computing deep learning frameworks keras tensorflow data visualization data management systems business intelligence tools knime tableau", "experience sql querying mongodb solr indexes", "", "", "pursuing bs computer science information systems mechanical engineering materials engineering chemical engineering electrical engineering chemistry physics expertise engineering analysis tools data analysis scripting methods order automate train standard analytical tasks required prior experience machine learning algorithms artificial intelligence image processing numerical computing plus", "", "", "", "", "", "", "bachelor degree experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics gpa least point scale experience real world data thesis research internships work experience creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment advanced degree data science equivalent field sub field experience working data rich problems research programs experience computer programming user experience user interface ability successfully complete projects large incomplete data provide solutions strong written verbal communication skills thorough medical psychological exam polygraph interview comprehensive background investigation", "", "phd computational quantitative discipline e g statistics computer science biomedical informatics genetics physics epidemiology health economics master' degree similar field study deep understanding ml including strong knowledge mathematical underpinnings behind various methods e g regression techniques neural networks decision trees clustering pattern recognition dimensionality reduction proven experience applied statistics ml business setting deep understanding tools trade including variety modern programming languages r python javascript open source technologies linux tensorflow hadoop spark experience effective data visualization approaches keen eye detail visual communication findings comfort working communicating non technical teams translate business questions analytically actionable questions strong desire build meaningful solutions life sciences business task oriented ability set goals complete deliverables domain knowledge clinical data real world data life sciences related research data expertise data science related tools e g sql tableau d3", "", "", "", "", "b b statistics math economics finance computer science similar field database experience including proficiency sql excellent communication organization analytical skills experience presenting various business contacts levels company real passion working product management engineering ability thrive fun dynamic start environment roll sleeves attitude excellent sense humor coding experience scripting languages r python experience visualization tools like tableau contributions data science community background online advertising masters degree statistics math related field", "", "", "", "", "", "", "", "master phd preferably engineering statistics technology science role analytics familiarity common advanced analysis tools sql python r sas preferred demonstrate familiarity work experience github account oop concepts python java scala skills big plus machine learning deep learning nlp experience also working hadoop spark environment ability ask tackle important analytical questions view driving product impact", "us citizenship required offer sponsorships contracting work position charlottesville bachelor' degree technical field prior work experience related field working portfolio clearly demonstrates abilities casual work environment intellectually challenging work health insurance short term disability insurance generous defined benefit retirement flexible vacation policy want know check recruitment video https www youtube com watch v w b2ey1tlrm commonwealth computer research inc discriminate basis race sex color religion age national origin marital status disability veteran status genetic information sexual orientation gender identity reason prohibited law provision employment opportunities benefits", "", "worked seriously huge datasets", "", "master' degree behavioral sciences relevant field training research methodology statistics two years experience various data analysis visualization tools strong communication data presentation skills experience spss excel powerpoint word ph behavioral sciences relevant field rigorous training research methodology statistics psychometrics expertise python r expertise using applying machine learning deep learning models systems professional experience human capital consulting", "", "", "", "qualifications may warrant placement different job level", "proud say work stitch fix know work brings joy clients every day", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems ability build scalable systems analyze huge data sets make actionable recommendations strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "years experience working intelligence community teams working fields related data science statistical modeling information retrieval text analysis data mining machine learning intelligence analysis cyber threat analysis image analysis network security statistical modeling geo spatial analytics data munging cleaning bachelor degree ability work datasets different sizes formats across multiple databases knowledge experience specific techniques neural networks cluster analysis feature engineering extraction reduction web scraping decision trees cart collaborative filtering geo spatial analysis experience pcap data elastic search hadoop hdfs git spark mllib sql os experience windows linux windows ability compile results deliver presentations senior level leadership strong communication skills ability present material audiences differing technical aptitude r r shiny r studio python sci kit tensorflow intelligence community experience machine learning statistical modeling experience multi tb dataset manipulation cleaning querying modeling experience scikit learn tensorflow r caret experience curating datasets supervised unsupervised machine learning methods experience c java r javascript php matlab pig hive impala pyspark scala ruby pytorch", "", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "", "", "", "bachelor' degree computer science information systems mathematics statistics related quantitative discipline applicants data related expertise professional background media also encouraged apply years professional experience media company preferred enthusiasm wall street journal understanding product must entrepreneurial attitude toward work sweat details well healthy skepticism status quo experience using analytics tools sql tableau excel interest advanced topics analytics artificial intelligence data engineering worked visualization machine learning libraries either r python experience building web applications agile development plus", "", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "messages sorted date thread subject author", "must u citizen national apply required pass background investigation fingerprint check registered selective service applicable successful completion one year probationary period meet education experience requirements complete occupational questionnaire submit resume supporting documents organizational performance analysis problem solving database management systems planning evaluating applicant disability needs accommodation equal opportunity apply job employee disability needs accommodation perform essential job duties gain access workplace employee disability needs accommodation receive equal access benefits details training office sponsored events", "", "", "conduct planning development implementation administration systems acquisition storage retrieval data consult customers apply analytical processes planning design implementation new improved information systems meet business requirements customer organizations identify adapt manage changes data analysis tools response evolving user needs provide technical advice group director deputy director collecting analyzing interpreting communicating insights data develop database system proposals coordinate efforts necessary translate business requirements effective data system solutions must u citizen national apply position subject background suitability investigation optional form declaration federal employment background suitability investigation background suitability investigation required selectees appointment subject successful completion investigation favorable adjudication failure successfully meet requirements may grounds appropriate personnel action addition hired reinvestigation supplemental investigation may required later time selected optional form required prior final job offer click obtain copy optional form form employment verification electronic eligibility verification program cms participates electronic employment eligibility verification program e verify e verify helps employers determine employment eligibility new hires validity social security numbers selected form required time processing click information e verify obtain copy form standard form appointment affidavits selected standard form required time processing click obtain copy standard form best qualified superior evaluation criteria well qualified excel evaluation criteria qualified meet minimum qualification requirements official position title include series grade federal job duties specific describing duties employer name address supervisor name phone number start end dates including month day year e g june april full time part time status include hours worked per week salary begin click apply access online application need logged usajobs account apply usajobs account need create one beginning application follow prompts select resume supporting documents included application package opportunity upload additional documents include application submitted uploaded documents may take several hours clear virus scan process acknowledging reviewed application package complete include personal information section deem appropriate click continue application process taken online application must complete order apply position complete online application verify required documentation included application package submit application first week september announcement open period midnight day applications received early september first round reviews written assessment mid september second round reviews hour phone calls early october hiring manager interviews qualifying applicants mid late october tentative job offers sent applicants applicant disability needs accommodation equal opportunity apply job employee disability needs accommodation perform essential job duties gain access workplace employee disability needs accommodation receive equal access benefits details training office sponsored events", "", "messages sorted date thread subject author", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "", "gated dog run located outside la location", "", "bachelor degree data science computer science mathematics statistics master' degree data science computer science mathematics statistics preferred years' experience leading data analytics initiatives using statistical modeling analytics platforms tools producing high quality data analytics products excellent verbal written communication skills ability effectively communicate collaborate internal stakeholders analytics vendor partners demonstrated ability analyze large complex data sets demonstrated aptitude conducting quantitative qualitative analysis understanding analytical techniques support cybersecurity program goals objectives deep understanding statistics artificial intelligence machine learning models application models support cybersecurity objectives ability develop intuitive analytics reports visualizations improves risk management decision making optimizes cybersecurity operations orchestration automation experience working large scale analytics event management platforms hadoop splunk qradar experience statistical analytics reporting visualization tools power bi excel tableau ssps statistics modeler mature self starting self motivating capable making decisions independently proficiency data management languages e g sas r python etc develop implement cybersecurity analytics product tools includes development maintenance continuous improvement cybersecurity statistical models predictive analytics reports visualizations partner stakeholders define analytics requirements program needs provide direct support incident handling vulnerability management teams risk teams providing expertise exploratory data analysis pattern discovery advanced analytical techniques anticipate detect undiscovered threats work internal stakeholders vendors partners optimize analytics systems detect manage external internal threats coordinate data infrastructure needs appropriate stakeholders develop high quality compelling intuitive data dashboards research papers visualizations stories presentations coordinate cybersecurity analytics program activities includes coordinating development activities initiatives analytics stakeholders architects engineers communicate analytics initiatives status ensure data analytics initiatives aligned support cyber analytics program objectives requirements effectively manage shifting priorities timelines mentor coach cybersecurity risk management staff team members communicate analytics gaps needs leadership effectively work within team support goals objectives analytics program", "", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems ability build scalable systems analyze huge data sets make actionable recommendations strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "", "", "", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "", "background engineering mechanics inertial sensing signal processing working knowledge system identification statistical inference modeling working knowledge matlab", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "", "", "experience digital sound processing excellent medical dental vision life disability benefits 401k program ability work closely customers hungry product make positive impact livelihood world focus community involvement career development equal opportunity employer value diversity company committed creating inclusive environment employees", "", "strong data mining machine learning background experience big data environment years experience working real world noisy data phd computer science math related quantitative field experience recommender systems information retrieval graph analysis statistical modeling neural network natural language processing experience large scale data analysis building high performance computational software experience e commerce analyzing clickstream data proficiency python sql statistics experienced software engineering practices providing solutions development implementation scaling execution validation monitoring improvement data science solutions ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "bachelor master degree computer science related field focus data science machine learning preferred minimum years hands experience data science machine learning years experience data engineering analytics business intelligence least years experience c c java least years current experience deep learning hands experience deep learning neural networks highly proficient python sql proficient tensor flow keras theano experience jupyter notebooks docker containers strong mathematics statistics data analytics abilities solid understanding relational nosql database technologies skilled working stream batch data extremely large data sets experience leading small technical teams people project management track record successful projects data engineering machine learning product experience advanced data analytics visualization techniques experience advanced machine learning techniques complex data pipelines ingest configurations systems experience developing leveraging distributed computing gpu systems frameworks experience developing implementing predictive solutions anomaly detection large scale systems prior experience research labs environments exploring evaluating new technologies products publishing research creating white papers etc experience complex networks big data mobile media wireless environments experience data processing storage frameworks like hadoop scala spark storm cassandra kafka etc", "degree computer science mathematics statistics equivalent experience years work experience data science analytics senior level sql python r alternate analytic programming language experience methodical detail oriented approach know information accurate able keep schedule tenacious unstoppable attitude want get job done quickly efficiently ability effectively work wide variety personalities skill levels", "", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "bachelor degree minimum year experience predictive statistical modeling using sas r python proficient ms office applications excel proficiency pivots v lookups formulas bilingual spanish english master' degree experience performing data analysis experience extracting data using sql data exploration tools sas r experience data analytics design experience working large databases health care industry experience demonstrated ability effectively gather requirements probe deeper understanding translate deep technical concepts non technical well technical senior stakeholders marketing customers data scientists demonstrated ability manage people prioritize deliverables proven organizational skills ability flexible work ambiguity", "years recent experience data science data analyst role familiarity measuring ux customer engagement planning analyzing ab experiments comfortable well versed working predictive causal problems passion improving customer experience refining product excellent presentation communication social skills strong attention detail strong business mindset possessing ability condense complex analysis technical concepts clear concise takeaways business leaders ability operate comfortably effectively dynamic highly cross functional fast paced environment excellent time management skills ability manage work tight deadlines handle pressure product launches executive requests well versed sql languages experienced big data technologies hadoop spark familiarity python r data visualization tools tableau full stack data analysis insight synthesis presentation ability comprehensively understand data elements sources relationships business technical terms", "", "", "united states preferred", "must least years actual working experience performing advanced quantitative analyses must actual working knowledge python sql working knowledge sas r plus working knowledge big data manipulation tool plus ability apply advanced statistical methodologies mixed model random fixed effects simultaneous equations arima neural networks multinomial discrete choice ability apply mathematical operations tasks cluster analytics sampling theory design experiments analysis variance correlation techniques factor analysis ability apply advanced optimization methodologies linear mixed integer optimization ability apply advanced simulation modeling methodologies techniques utilize complex computer operations intermediate programming 3rd 4th generation languages relational databases operating systems advanced features software packages word processing spreadsheet graphics etc must relational database experience strong passion empirical research answering tough questions data demonstrated experience organizing prioritizing coordinating complex team efforts experience working executives strategic planning departments set manage corporate level strategies plus experience business support software applications ms office word powerpoint excel project required", "strong data mining machine learning background experience big data environment years experience working real world noisy data phd computer science math related quantitative field experience recommender systems information retrieval graph analysis statistical modeling neural network natural language processing experience large scale data analysis building high performance computational software experience e commerce analyzing clickstream data proficiency python sql statistics experienced software engineering practices providing solutions development implementation scaling execution validation monitoring improvement data science solutions ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "", "bs graduate degree computer science statistics mathematics economics similar quantitative field emphasis predictive analytics data mining statistics machine learning algorithms etc years data science experience working closely product engineering ability translate business objectives problems analytical problems use quantitative qualitative skills deliver simple logical actionable solutions experience data visualization tableau presentation advanced proficiency r python excel proficiency sql hadoop map reduce scripting language proven ability data science initiatives end end ability build predictive modeling time series k nearest neighbors random forests ensemble methods understanding applied math topics probability statistics linear algebra basic optimization techniques etc", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "years experience quantitative analysis consulting role advanced data modeling experience sql skills etl design experience advanced data analytics data transformation data management projects dimensional modeling bi support depth experience analyzing data creating reports working data identify trends make recommendations", "", "ability tie together disparate unique vintage data sources cohesive automated stable data output utilization models expertise sql ms access vba excel sas query modeling languages tools ability leverage industry standard operations modeling techniques regression six sigma address ongoing ad hoc modeling needs e g working actuaries remove operations impact reserving models expertise one analytics packages sas r python required experience building advanced analytics models artificial neural nets k nearest neighbors ability creatively problem solve collaborative fashion ability explain highly technical complicated issues simple non technical language keen analytical thinker strong research capabilities ability perform independent research industry trends around analytics excellent written verbal skills years work experience", "years proven top performance data science analytics space must least years direct experience models built maintained production focused large commercial business problems tend billion dollar plus scope opportunity must expert level understanding ability explain code underlying math used algorithms models must excellent coding proficiency demonstrated expertise python java r etc expert level knowledge ai ml models frameworks keras pytorch etc libraries packages apis e g scikit must least years developing models algorithms independently writing code developing strategy algorithmic experimentation deploying production must worked batch streaming models production advanced knowledge math probability statistics models experience hadoop aws distributed compute services exposure data structures data modeling software architecture skills excellent communication skills excellent cross functional collaboration skills outstanding analytical problem solving skills", "", "", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "", "", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "ability work team oriented collaborative environment", "phd computer science computer engineering ms years experience related field demonstrated history driving delivering analytics models solutions deep knowledge fundamentals machine learning data mining statistical predictive modeling extensive experience applying methods real world problems strong skills software prototyping engineering expertise applicable programming analytics languages python r c c various open source machine learning analytics packages generate deliverable modules prototype demonstrations work desired interdisciplinary skills include big data technologies etl statistics causal inference deep learning modeling simulation breadth skills experience machine learning - diverse types data diverse data sources different types learning models diverse learning settings ability inclination work multi disciplinary environments desire see ideas realized practice experience knowledge services domains business process outsourcing systems transportation systems healthcare systems financial services valued demonstrated ability propose novel solutions problems performing experiments show feasibility solutions working refine solutions real world context prior experience similar role required please include requirements appropriate must currently eligible work us employer without sponsorship", "", "years working experience", "knowledge gis principles concepts methods working knowledge experience arcgis desktop address processing geocoding demonstrable expertise cartographic design production ability work identify patterns large data repositories cross correlate metadata across multiple data sources proficiency required programming languages scripting languages python javascript sql databases sql query language ability develop implement gis automation applications quality assurance protocols metadata standards proven ability conceptualize complete complex projects thorough documentation demonstration applied logic e flow charts ability communicate effectively supervisors project leader co workers orally writing ability work team environment well goal oriented individual functions highest level integrity professionalism ability multitask work fast paced environment deadlines", "", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "visualization tell story explain point", "", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "", "", "", "", "", "", "", "healthcare data analysis experience clinical environment - strongly preferred year equivalent exposure business objects tableau understands data integration processes tune performance years sql experience oracle preferred ssis ok ability develop work plans follow assignments minimal guidance ability work business system owners obtain requirements manage expectations years' experience sas oncology data experience preferred", "", "", "", "", "", "", "", "", "", "", "phd computer science computer engineering ms years experience related field demonstrated history driving delivering analytics models solutions deep knowledge fundamentals machine learning data mining statistical predictive modeling extensive experience applying methods real world problems strong skills software prototyping engineering expertise applicable programming analytics languages python r c c various open source machine learning analytics packages generate deliverable modules prototype demonstrations work desired interdisciplinary skills include big data technologies etl statistics causal inference deep learning modeling simulation breadth skills experience machine learning - diverse types data diverse data sources different types learning models diverse learning settings ability inclination work multi disciplinary environments desire see ideas realized practice experience knowledge services domains business process outsourcing systems transportation systems healthcare systems financial services valued demonstrated ability propose novel solutions problems performing experiments show feasibility solutions working refine solutions real world context prior experience similar role required please include requirements appropriate must currently eligible work us employer without sponsorship", "", "", "", "", "years experience performing statistical predictive analytics within video games industry expert machine learning expertise r python must passing familiarity behavioral science psychology social science eligibility requirements interested candidates must submit resume cv online considered must unrestricted work authorization work united states must covered solutions nbcu' alternative dispute resolution program desired characteristics expertise java c c plus understanding big data technologies cassandra spark hadoop strongly preferred fan fast furious jurassic world back future ever dreamed working innovative games based favorite movies tv shows universal games digital platforms group looking join diverse group creative talent crafting groundbreaking interactive experiences inspired world' valuable brands operating start mindset business unit nimble explores new emerging platforms ar vr stay forefront innovation ' currently looking even movers shakers group growing could next member team really 'fun games' nbcuniversal' policy provide equal employment opportunities applicants employees without regard race color religion creed gender gender identity expression age national origin ancestry citizenship disability sexual orientation marital status pregnancy veteran status membership uniformed services genetic information basis protected applicable law nbcuniversal consider employment qualified applicants criminal histories manner consistent relevant legal requirements including city los angeles fair chance initiative hiring ordinance applicable", "", "", "experience managing small teams projects", "", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "", "", "", "", "phd computational quantitative discipline e g statistics computer science biomedical informatics genetics physics epidemiology health economics master' degree similar field study deep understanding ml including strong knowledge mathematical underpinnings behind various methods e g regression techniques neural networks decision trees clustering pattern recognition dimensionality reduction proven experience applied statistics ml business setting deep understanding tools trade including variety modern programming languages r python javascript open source technologies linux tensorflow hadoop spark experience effective data visualization approaches keen eye detail visual communication findings comfort working communicating non technical teams translate business questions analytically actionable questions strong desire build meaningful solutions life sciences business task oriented ability set goals complete deliverables domain knowledge clinical data real world data life sciences related research data expertise data science related tools e g sql tableau d3", "", "", "technical expertise - years experience applying advanced analytics techniques data mining descriptive statistics visualization solve complex business problems including years deep technical experience predictive analytics machine learning optimization fluent multiple technologies python azure ml ibm spss modeler r comparable technologies required strong database skills required experience visualization techniques preferred experience optimization software preferred technology leadership strong working knowledge contemporary analysis technology software platforms methodologies ability apply manufacturing processes ability educate senior leaders impact benefit analytics descriptive predictive prescriptive cognitive operations project leadership - small medium scale project management experience including limited scope schedule cost risk resource change management possibly including initiatives global reach technology processes cross functional teams partner team members consulting skills proven track record influencing decision problem solving processes ability understand business economic drivers align goals across functional lines organizational boundaries execution global experience - understands communicates effectively interacts people across cultures achieve business results prior experience participating collaborating cross functional cultural teams beneficial analytics demonstrated experience applying statistical techniques solve business problems visualization - experience effective utilization visualization techniques explore data find root causes well presentation results optimization demonstrated experience various optimization techniques including linear programming integer programming non linear programming dynamic programming leadership recognized expert field ability help define problem move quickly resolution someone sought bring resolution issue timely cost effective manner consultative skills ability influence business partners decision making shape solutions helping partners articulate need problem solving - strong intrinsic problem solving skills ability structure solve problems conduct interpret analysis independently demonstrated analytical quantitative skills diversity understands communicates effectively interacts people across cultures effectively achieves business results working across multi national teams communication - strong presentation communication skills ability explain complex analytical concepts people fields external technology knowledge keeps abreast latest technological developments areas push technology roadmaps realization constantly looks opportunities incorporate new solution methods solve existing problems unconditional commitment safety competes analytics adaptability ability respond quickly demands moment flexible person stay productive demands work pull many different directions shifts focus necessary maintain effectiveness variety environments quickly come speed project contributor accountability knows needs done gets done willingly takes responsibility organization whole unafraid owning results actions decisions self organization committed follow completion - excuses rationalizations totally unacceptable curiosity naturally curious leading one seek knowledge people things stretch beyond one' work environment thrives dynamic work environment new subject matter learned quickly put practice information \"sponge\" - constantly absorbing new methods technologies approaches deliver business outcomes decisive able make decision competing analytics conditions high uncertainty weighs risks prioritizes actions deliver organizational effectiveness speed risk taker innovator willing push envelope meet stretch goals satisfied status quo willing aggressive implement next breakthrough technology get operations next level", "", "", "team outings sports games happy hours game nights", "", "", "years professional industry experience quantitative analysis proven track record using analysis impact key business product decisions ability clearly effectively communicate results complex analyses experience writing production datasets sql hive building internal production data tools etl experimentation exploration scripting language python r etc solid grasp basic statistical applications methods experimentation probabilities regression experience software engineering data engineering consulting academic research plus", "background image processing concepts e g filtering morphology transforms compression etc desired", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "never ending ping pong tournaments", "", "", "master' degree accredited institution minimum years experience data analytics employees must legally authorized work united states verification employment eligibility required time hire visa sponsorship available position ms phd accredited institution applied mathematics operations research industrial engineering mathematics focus machine learning physics similar quantitative discipline years experience delivering computation approach outcomes solving complex analytical problems using quantitative approaches unique blend analytical mathematical engineering skills experience manufacturing understanding statistical predictive modeling concepts machine learning approaches clustering classification techniques recommendation optimization algorithms accomplished use statistical analysis environments r matlab spss sas experience bi tools tableau microstrategy comfortable relational databases hadoop based data mining open sources frameworks familiar sql python java c c experience isogeometric analysis", "", "", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "", "", "", "bachelor' degree statistics computer science related field years experience working large datasets drawing business insights fluency python programming experience using machine learning solve complex business problems strong understanding statistics modeling techniques knowledge predictive modeling experienced leveraging structured unstructured data sources experience relational databases sql willingness ability learn new technologies job demonstrated ability communicate complex results technical non technical audiences demonstrated ability work minimal supervision experience distributed computing big data technologies spark related technologies experienced statistical methodologies tools r sas spss etc experience natural language processing information retrieval recommender systems", "", "possess bachelor' degree higher completed verified prior start accredited institution minimum two combined years experience one following areas data analytics data visualization statistical analysis predictive modeling application development private public government military environment project management experience extensive knowledge experience using excel access power point ms word experience working across functions influencing teams continuous improvement mindset experience data modeling tools e g sap pa python r sas ms azure experience business intelligence data visualization tools power bi google analytics tableau domo qlikview data integration experience including extract transform load etl processes experience databases complex data queries greenbelt certified strong organizational skills self motivated independent excellent oral written communication skills ability work rapidly changing environment", "", "master' degree higher statistics math computer science related field years industry work experience sql r python implement statistical models machine learning analysis recommenders prediction classification clustering etc big data environment experience large scale computing systems like cosmos hadoop mapreduce similar systems preferred experience programming skills e g java c plus familiarity deep learning toolkits e g cntk tensorflow etc plus exceptional written verbal communication educate work cross functional teams", "", "", "", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems experience big data distributed computing platforms hadoop spark nosql databases strong knowledge standard machine learning techniques concepts years hand experience either industry academia scripting language python r general purpose programming language java scala ability build scalable systems analyze huge data sets make actionable recommendations strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "", "", "creates statistical approaches analyze data generated multiple applications processes trends data understand opportunities focused investigation develops reports emphasizing clarity accuracy methodologies degree statistical support conclusions communicates effectively various channels including written reports oral presentations assists new business development proof concepts required", "currently pursuing bachelors masters computer science computer science engineering engineering computer science math computer science mathematical engineering mathematical science mathematics statistics strong data wrangling skills strong python sql skills ability manipulate json xml data experience splunk experience data manipulation platforms experience ansible puppet jenkins chef", "member analytics group may opportunities get hands dirty developing analytics solutions time time", "phd computer science computer engineering ms years experience related field demonstrated history driving delivering analytics models solutions deep knowledge fundamentals machine learning data mining statistical predictive modeling extensive experience applying methods real world problems strong skills software prototyping engineering expertise applicable programming analytics languages python r c c various open source machine learning analytics packages generate deliverable modules prototype demonstrations work desired interdisciplinary skills include big data technologies etl statistics causal inference deep learning modeling simulation breadth skills experience machine learning - diverse types data diverse data sources different types learning models diverse learning settings ability inclination work multi disciplinary environments desire see ideas realized practice experience knowledge services domains business process outsourcing systems transportation systems healthcare systems financial services valued demonstrated ability propose novel solutions problems performing experiments show feasibility solutions working refine solutions real world context prior experience similar role required please include requirements appropriate must currently eligible work us employer without sponsorship", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members", "", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "", "expertise natural language processing machine learning classification feature engineering information extraction structured prediction clustering semi supervised learning topic modeling ranking proficiency data science analytics including statistical analyses b testing experience designing conducting analyzing interpreting experiments investigations strong programming skills expert knowledge algorithms data structures python java equivalent excellent problem solving critical thinking creativity organizational design interpersonal skills ability work well levels engineers confirmed ability handle multiple projects strict deadlines", "", "", "", "years experience related work building statistical models advanced data analysis master' degree phd candidate mat economics statistics data science experience logistic regression linear regression time series analysis decision trees cluster analysis advanced programming skills include knowledge statistical programs e g sql sas spss r python must eligible full time employment salary range full benefits cigna healthcare metlife dental vsp vision 401k voya paid time", "", "", "strong data mining machine learning background experience big data environment years experience working real world noisy data phd computer science math related quantitative field experience recommender systems information retrieval graph analysis statistical modeling neural network natural language processing experience large scale data analysis building high performance computational software experience e commerce analyzing clickstream data proficiency python sql statistics experienced software engineering practices providing solutions development implementation scaling execution validation monitoring improvement data science solutions strong communication data presentation skills ability communicate data driven stories ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "", "", "", "", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "", "connect everything - people process data things - use connections change world better", "", "", "minimally candidates bachelor' degree statistics math computer science informatics economics related fields advanced degrees subject areas preferred general knowledge statistical techniques concepts regression properties distributions statistical tests etc novice understanding python sql understanding hadoop java preferred strong problem solving skills emphasis data driven solutions excellent written verbal communication skills coordinating across teams ability analyze policies procedures able recommend improvements excellent oral written communication skills ability work independently anticipate problems initiate corrective actions ability effectively prioritize variety projects functions ability research document findings ability establish maintain effective working relationships", "advanced statistics modeling data visualization knowledge strong data visualization skills substantial data analysis experience working large scale data ability learn new technologies quickly grasp complex problems significant experience using relational databases mysql preferred excellent written verbal communication skills strong scripting language skills e g r python experience working hadoop map reduce experience working hadoop map reduce", "", "point successful candidate shown prowess data scientist right promoted leading projects making major impact business", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "working knowledge microsoft office suite working knowledge tableau working knowledge presentation software e g microsoft powerpoint working knowledge analytical statistics software e g spss sas r stata demonstrated ability collaborate work effectively cross functional teams ability develop apply advanced mathematical statistical techniques ability convey complex technical ideas processes easy understand terms diverse audiences excellent written verbal communication skills", "", "least years experience generating implementing short term trading alphas production trading strategies advanced knowledge modern statistical machine learning techniques proficiency scripting language python r matlab proficiency least one statistical package one machine learning package one languages demonstrated experience working tick data degree quantitative discipline statistics computer science mathematics engineering etc experience developing software systems object oriented language plus", "demonstrated energy passion extends beyond field study - computer scientist writes poetry mathematician loves psychology engineer passionate public policy want build something", "ability apply common sense understanding carry instructions furnished written oral diagram form", "", "", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "strong experience implementing real time analytics dashboard business intelligence reporting strong experience data visualization tools tableau d3 js experience building end end data science workflow experience programming python r software engineering mindset documentation source control release cycles repeatability sharing experience implementing etl report generation large volume data ability work within small high achieving team well independently self driven highly motivated innovative strong communication skills written verbal computer science degree another highly quantitative degree engineering physics mathematics graph databases janus neo4j apache spark scala aws linux machine learning tools", "", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "", "", "must eighteen years age older must legally permitted work united states knowledge skills abilities typically acquired completion master degree program equivalent degree field study related job time spent sitting comfortable position frequent opportunity move rare occasions may need move lift light articles master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data science natural language processing previous work experience ecommerce conversational ai search science experience deep learning machine learning experience information retrieval dialogue systems coreference resolution ner nlu knowledge graph ability build scalable systems analyze huge data sets make actionable recommendations strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members", "", "", "", "", "bachelor' degree computer science computer engineering related field equivalent combination education related experience fundamental knowledge one following high performance computing scientific data analysis statistical analysis knowledge discovery computer security systems programming large scale data management big data technologies skilled aspects software project life cycle feasibility requirements design implementation integration test deployment fundamental experience developing software c c java python r matlab software applications linux unix windows environments data analysis algorithms data management approaches relational databases machine learning algorithms ability effectively handle concurrent technical tasks conflicting priorities approach difficult problems enthusiasm creativity change focus necessary work independently implement research concepts multi disciplinary team environment commitments deadlines important project success sufficient interpersonal skills necessary interact levels personnel included best places work glassdoor work premier innovative national laboratory comprehensive benefits package flexible schedules depending project needs", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "access opportunities expand skill set share knowledge others across organization", "many unique benefits including floating holidays lunch every wednesday paid volunteer time fuel efficient vehicle purchase assistance transit fare contribution first time homebuyer' payment assistance", "always interviews screening call conducted via video call", "", "eye great data visualization matplotlib plotly ggplot tableau", "", "years data science experience outside academia title commensurate experience advanced degree quantitative discipline applied mathematics statistics computer science physics related field leading academic institution expert python sql must experience writing production level code proficiency working spark process large data sets excellent communication skills demonstrated success presenting complex data analysis qualitative quantitative clear compelling manner inspires action strong understanding statistical analysis strong passion empirical research answering hard questions data passion problem solving comfort ambiguity creativity flexible analytic approach allows results varying levels precision quick learner ability initiate drive projects completion minimal guidance ability thrive dynamic fast paced environment drive change collaborate effectively variety individuals organizations", "", "interest politics educational policy", "", "years professional industry experience quantitative analysis role proficiency sql experience programming language like python r etc ability identify complex business problems provide sound analytical modeling solutions ability communicate clearly effectively cross functional partners varying technical levels ability define relevant metrics guide influence stakeholders appropriate accurate insights experience willingness learn tools create data pipelines using airflow ability build clear easy understand dashboards presentations years industry experience experience python r experience tableau ability model run experiments ability ramp data science manager role near future stock yearly employee travel coupon competitive salary paid time medical dental vision insurance life disability coverage 401k flexible spending accounts apple equipment daily breakfast lunch dinner", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "", "undergraduate graduate degree preferred major computer science mathematics statistics physics engineering related stem major least years relevant experience preferably digital domain experience productionising machine learning models programming experience either python r plus one general purpose programming language java c c sql relational database experience must experience nosql big data stack preferred strong foundation inferential statistics machine learning algorithms experience visualization tools tableau powerbi etc strong capacity communicate complex concepts easy understand terminology ability translate data insight value driving business plan experience shelf models aws gcp natural language processing desired", "", "least year experience working within retail ecommerce company", "", "", "", "", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "bachelor' degree computer science computer engineering related field equivalent combination education related experience fundamental knowledge one following high performance computing scientific data analysis statistical analysis knowledge discovery computer security systems programming large scale data management big data technologies skilled aspects software project life cycle feasibility requirements design implementation integration test deployment fundamental experience developing software c c java python r matlab software applications linux unix windows environments data analysis algorithms data management approaches relational databases machine learning algorithms ability effectively handle concurrent technical tasks conflicting priorities approach difficult problems enthusiasm creativity change focus necessary work independently implement research concepts multi disciplinary team environment commitments deadlines important project success sufficient interpersonal skills necessary interact levels personnel included best places work glassdoor work premier innovative national laboratory comprehensive benefits package flexible schedules depending project needs", "", "years experience data scientist preferably big data environment years programming experience java scala python hadoop stack hive pig hadoop streaming mapreduce hbase comparable nosql sql database experience experience google products google cloud storage google analytics google big query plus bachelor' degree quantitative related field design build predictive customer behavior models targeting personalization implement machine learning statistics based algorithms prediction optimization deliver production build maintain code populate hdfs hadoop log kafka data loaded sql production systems design build support algorithms data transformation conversion computation hadoop spark distributed big data systems", "", "parties night lan", "", "", "", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "university student final year stem related field ops research statistics applied math engineering business analytics expected graduation date december may currently completing first year non business year masters program less years work experience deep understanding statistical predictive modeling concepts machine learning approaches clustering classification techniques recommendation optimization algorithms experience one programming languages r python c etc ability easily understand complex algorithm logic process data experience working large volume data ability solve performance issues practitioner statistical data quality procedures test driven approach quality assurance basic business intuition clear expertise analyses ability describe analytic processes including specific approaches favored excellent communication presentation skills ability visualize report insights creatively variety formats various stakeholders ability deliver deadline driven environment team player passion coaching colleagues clients", "years recent experience data science data analyst role familiarity measuring ux customer engagement planning analyzing ab experiments comfortable well versed working predictive causal problems passion improving customer experience refining product excellent presentation communication social skills strong attention detail strong business mindset possessing ability condense complex analysis technical concepts clear concise takeaways business leaders ability operate comfortably effectively dynamic highly cross functional fast paced environment excellent time management skills ability manage work tight deadlines handle pressure product launches executive requests well versed sql languages experienced big data technologies hadoop spark familiarity python r data visualization tools tableau full stack data analysis insight synthesis presentation ability comprehensively understand data elements sources relationships business technical terms", "", "designing data architecture table dashboard", "", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "", "", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "", "years data science experience outside academia title commensurate experience advanced degree quantitative discipline applied mathematics statistics computer science physics related field leading academic institution expert python sql must experience writing production level code proficiency working spark process large data sets excellent communication skills demonstrated success presenting complex data analysis qualitative quantitative clear compelling manner inspires action strong understanding statistical analysis strong passion empirical research answering hard questions data passion problem solving comfort ambiguity creativity flexible analytic approach allows results varying levels precision quick learner ability initiate drive projects completion minimal guidance ability thrive dynamic fast paced environment drive change collaborate effectively variety individuals organizations", "", "", "", "minimum years relevant data science experience ph master' degree operations research applied statistics data mining machine learning physics related quantitative discipline preferred experience hive sql spark python scala deep understanding statistical predictive modeling concepts machine learning approaches clustering classification techniques recommendation optimization algorithms strong analytical problem solving skills", "collaborative team player values contribution others", "", "passionate data science disciplined work confident humble", "", "", "", "dog friendly office", "", "", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members", "", "", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems experience big data distributed computing platforms hadoop spark nosql databases strong knowledge standard machine learning techniques concepts years hand experience either industry academia scripting language python r general purpose programming language java scala ability build scalable systems analyze huge data sets make actionable recommendations strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "currently pursuing degree graduated within last months degree following field electrical engineering computer science data science statistics relevant fields phd aforementioned fields ms bs years' experience aforementioned fields strong background machine learning statistics years' experience probabilistic graphical models bayesian networks deep learning modeling paradigms experience python r scala similar research publications plus bs ms aforementioned fields experience developing c c java c scripting language experience database systems systems engineering experience designing developing high scale distributed systems plus knowledge lambda architectures plus knowledge machine learning data visualization ai plus bs ms aforementioned fields business management marketing communication similar experience project product program management customer design excellent storytelling team work written oral communication skills", "previous message jobs fwd inedinfo fwd job statistician fte next message jobs npd group", "", "", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "b cs statistics applied math physics quantitative discipline years experience role developing predictive explanatory models experimentation processes experience working data analytics experience moderate large scale data sets 100gb preferred core mathematical ability understand utilize innovate state art machine learning algorithms statistical modeling expertise least one production quality programming languages e g java python scala c exceptional communication skills expertise hadoop ecosystem especially spark plus", "", "", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "", "bachelor' computer science statistics quantitative social sciences related field minimum years experience technical projects database components r python programming proficiency proficiency data integration data quality development programming experience mysql shell programming django data visualization experience including r shiny python dash experience technologies like github amazon aws excellent problem solving skills proven track record experience communicating technical topics non technical audience interest educational applications", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "", "", "", "", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "masters phd computer science analytics relevant line study required least years' experience performing advanced quantitative analyses experience data visualization tools excellent understanding machine learning techniques algorithms k nn kmeans nlp naive bayes svm etc experience recommendation engines applied statistics skills distributions statistical testing regression etc experience common data science toolkits r python experience working aws environments experience relational databases proficiency query languages sql", "", "nordstrom stock purchase plan", "", "", "", "bs msc phd machine learning data science math physics computer science equivalent degree plus two four year experience game economy optimization knowledge iap plus experience reinforcement learning mobile video games exceptional understanding machine learning concepts data science programming excellent communication skills ability collaborate data scientists engineers product managers open mind motivation learn spirit excel", "", "", "", "", "", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "", "", "", "", "pursuing ph quantitative discipline graduating passion machine learning proficiency python r sql authorization work united states", "experience spark distributed computing frameworks", "", "open work environment everyone even new folks voice", "bs degree computer science related technical field equivalent practical experience years proven working experience software developer data engineer strong track record delivery using graph databases neo4j titandb orientdb fluency graph query languages gremlin cypher sparql experience designing creating maintaining recommendation engines experience java scala development knowledge scripting language like python ruby strong experience restful api interfaces microservice architectures knowledge statistics experience using statistical packages analyzing large datasets r excel spss sas etc experience microsoft azure amazon web services aws experience agile software development experience developing knowledge based systems different contexts information retrieval intelligent agents dialog systems recommendation systems experience scalability performance issues concerning large knowledge stores experience information extraction creation application layer experience developing rest json applications multi threaded applications strong background computer science algorithms data structures concurrency distributed systems strong oo programming oo design knowledge knowledge professional software engineering practices best practices full software development life cycle including coding standards code reviews source control management build processes testing operations technical expertise regarding data models database design development data mining segmentation techniques technical capabilities cloud services micro services patterns api management azure aws services strong communication verbal written collaboration abilities addition technical depth comfortable delivering within agile program", "", "", "strong data mining machine learning background experience big data environment years experience working real world noisy data phd computer science math related quantitative field experience recommender systems information retrieval graph analysis statistical modeling neural network natural language processing experience large scale data analysis building high performance computational software experience e commerce analyzing clickstream data proficiency python sql statistics experienced software engineering practices providing solutions development implementation scaling execution validation monitoring improvement data science solutions strong communication data presentation skills ability communicate data driven stories ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "self starter results orientated able work minimal guidance", "", "", "", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "", "", "", "bachelor degree data science analytics engineering mathematics industrial engineering computer science information technology economics finance least years related experience data science information technology working combination big data advanced data analytics machine learning programming data bases master' degree least years relevant experience acceptable experience complex business operations analysis including engineering design manufacturing logistics finance market forecasting ability generate effectively communicate analytical insights visualization reporting tools sas jmp ms powerbi hana qlik r markdown proficiency data science object oriented programming languages python r ruby proficiency database languages mssql oracle postgresql mysql neo4j hana hadoop familiarity operational business systems preferred sap salesforce enovia excellent communication customer interfacing skills verbal written ability work independently multi tasking deadline driven environment manages time prioritizes tasks effectively master' degree data science analytics engineering mathematics industrial engineering computer science information technology economics finance equivalent professional experience experience stochastic process modeling design experiments non linear regression simulation optimization methods competency natural language algorithms processing generation nlp nlg competency machine learning development knowledge aerospace industry certification data science analytics", "", "bachelor' degree computer science mathematics engineering statistics related field least year experience r python us citizenship advanced degree computer science data science business analytics similar analytics concentration business oriented experience working federal agencies clients big data experience hive spark pig mapreduce knowledge industry leading analytics big data technologies approaches tools experience working fast paced collaborative environments strong written oral presentation skills", "", "proven ability perform complex queries analyses using sas sql experience bigquery python r plus statistical modeling experience years experience marketing role decision support role high degree problem solving reasoning abilities exceptional organizational communication skills understanding efficient database data design principles excellent total compensation package enhanced k retirement package health care including domestic partner family coverage across medical dental vision flexible spending accounts health dependent care life insurance including domestic partner dependent coverage outdoor experience days addition paid time vacation holiday personal time discounts l l bean merchandise outdoor discovery school adventures employee store equipment loan program employee use room tuition reimbursement", "", "", "", "bachelor degree master degree years work experience completing undergraduate degree years advanced analytics experience deep familiarity analytics actuarial market landscape corresponding information needs knowledge insurance industry plus strong problem solving skills quantitative analytical thinking capabilities including experience familiarity multivariate predictive modeling analytics software sas spss r etc proven record leadership ability work collaboratively team environment ability work effectively people levels organization willingness travel time", "years experience leveraging data business impact deep understanding statistical analysis e g hypothesis testing experimentation regressions machine learning algorithms supervised unsupervised models demonstrated programming experience least one analytic tool r python scala etc ability write optimize complex sql queries experience building deploying cloud based data pipelines applications gcp aws plus ability communicate clearly effectively cross functional partners varying technical levels ability work independently proactively well collaborating team good sense humor always big plus", "strong experience microsoft suite e g excel word powerpoint experience facilitating meetings workshops quickly understand business processes related client business questions years implementing bi solutions working bi tool e obiee cognos business objects sap years relevant experience delivering visualizations analytic insight client facing role experience including one following use multi tier architectures session management web based web enabled applications public key infrastructure technology preferred", "strong data mining machine learning background experience big data environment phd computer science math related quantitative field experience recommender systems information retrieval graph analysis statistical modeling neural network natural language processing experience large scale data analysis building high performance computational software experience e commerce analyzing clickstream data experience big data environment including spark hadoop hive etc proficiency least one statistics data analysis package python r proficiency least one programming language java python etc solid coding practices including good design documentation unit testing integration testing experience working real world noisy data strong communication data presentation skills ability communicate data driven stories ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "", "", "bachelor' degree computer science computer engineering related field equivalent combination education related experience comprehensive knowledge one following high performance computing scientific data analysis statistical analysis knowledge discovery computer security systems programming large scale data management big data technologies skilled aspects software project life cycle feasibility requirements design implementation integration test deployment experience developing software c c java python r matlab software applications linux unix windows environments data analysis algorithms data management approaches relational databases machine learning algorithms ability effectively handle concurrent technical tasks conflicting priorities approach difficult problems enthusiasm creativity change focus necessary work independently implement research concepts multi disciplinary team environment commitments deadlines important project success effective interpersonal skills necessary interact levels personnel effective advanced analytical problem solving decision making skills develop creative solutions complex problems significant experience demonstrated expertise following technical languages concepts constructs one following advanced areas high performance computing scientific data analysis statistical analysis knowledge discovery computer security systems programming large scale data management big data technologies included best places work glassdoor work premier innovative national laboratory comprehensive benefits package flexible schedules depending project needs", "", "", "", "phd statistics machine learning related quantitative discipline strong research background passion solving real world problems significant experience r matlab hadoop scripting knowledge plus competitive salary bonus program entrepreneurial environment top notch health dental vision insurance stock options fast growing tech company 401k plan matching contribution generous paid time plan plus paid holidays frequent company sponsored lunches happy hours fun events plenty snacks drinks supplyframe equal opportunity employer", "data science years preferred master required united states required", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "", "", "", "", "", "", "bachelor' degree years equivalent work related experience strong working stakeholders gather requirements present results strong data mining data visualization ability handle multiple competing priorities fast paced environment experience various math statistics methodologies algorithms strong least code bases python healthcare population health knowledge preferred experience deploying maintaining machine learning models production environment plus", "", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "", "must basic knowledge statistical concepts regression time series mixed model bayesian methods clustering etc analyze data provide insights", "", "experience working structured unstructured clinical data solid understanding machine learning algorithms use cases experience building deploying supervised unsupervised learning models including clustering anomaly detection experience bioinformatics nlp plus experience working amazon aws microsoft azure services proficient jupyter python spark sql able explain technical concepts results non technical audience bsc ba computer science engineering relevant field graduate degree data science quantitative field preferred experience bio statistics research methodologies would additional asset", "", "leveraging educational background science mathematics statistics computer science data science related discipline along relevant professional work experience lead execute coordinate innovative data analytics initiatives establish sustainable solutions experience statistical toolkits programming languages e g r python valued asset drive diffusion ai within basf work creatively new applications ai familiarity different analytical areas expertise ai e g machine reinforcement learning statistics analytics enable explore technical possibilities create ideas together internal non ai expert business partners", "", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "programming languages python r sas java javascript php d3 js relational databases sql sql databases machine learning models including probability statistical models addition time series analysis modern ml techniques support vector machines categorical regression trees neural networks recommendation systems microsoft office suite tools windows os linux os command line tools grep regex big data technologies including hdfs hadoop hive hbase spark modern versioning systems git subversion", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "years experience working intelligence community teams working fields related data science statistical modeling information retrieval text analysis data mining machine learning intelligence analysis cyber threat analysis image analysis network security statistical modeling geo spatial analytics data munging cleaning bachelor degree ability work datasets different sizes formats across multiple databases knowledge experience specific techniques neural networks cluster analysis feature engineering extraction reduction web scraping decision trees cart collaborative filtering geo spatial analysis experience pcap data elastic search hadoop hdfs git spark mllib sql os experience windows linux windows ability compile results deliver presentations senior level leadership strong communication skills ability present material audiences differing technical aptitude r r shiny r studio python sci kit tensorflow intelligence community experience machine learning statistical modeling experience multi tb dataset manipulation cleaning querying modeling experience scikit learn tensorflow r caret experience curating datasets supervised unsupervised machine learning methods experience c java r javascript php matlab pig hive impala pyspark scala ruby pytorch", "degree statistics information systems mathematics finance preferred minimum years experience applied data science experience hands practical casework agency corporate side strong knowledge experience wide variety tools including sql sas r alteryx tableau proficient excel powerpoint presentation communication skills written oral hands experience manipulating deriving insight large datasets using advanced analytic techniques including time series regression cluster analysis decision trees etc comfort efficient data acquisition warehousing practices structured unstructured datasets", "", "data visualization skills plus", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "", "", "", "", "fun puzzle loving office sf financial district", "", "experience processing performing multi faceted analysis customer consumer behavior data experience developing enhancing models applying machine learning artificial intelligence algorithms statistical analysis natural language processing strong coding skills sql python r julia scala experience working distributed computing e hive apache spark experience working unix linux environment knowledge descriptive analytics data visualization exceptional standards quality strong attention detail experience full lifecycle agile application development supporting analytic requirement modern machine learning models expert level years required statistical algorithms years required expert usage r python years required unix years required master preferred dallas tx preferred authorized work us w sponsorship future required united states required", "", "", "familiarity marketing data email engagement metrics experience developing testing implementing customer segments fluent least one statistical computer language python r etc utilized statistical analysis machine learning gain insight large data sets experience querying databases sql created used leveraged supervised unsupervised machine learning algorithms components regression simulation scenario analysis modeling clustering decision trees neural networks etc visualized presented data key stakeholders accessible format values aligned simple energy mission actively understands need supports diversity openness different points view self aware positive proactive attitude outcome orientation driven help deliver critical company outcomes quantitative means thought process appetite problem solving curiosity knack structured thinking process creation ability spot unusual patterns emphasis product development growth mindset comfortable learning growing mistakes drive continuously learn master new technologies techniques great communicator communicates proactively across team external stakeholders excellent written verbal communication skills ability communicate results explain solutions technical non technical stakeholders", "", "", "", "years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems ability build scalable systems analyze huge data sets make actionable recommendations strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members", "", "", "", "", "", "experience working large data sets distributed computing tools hive redshift plus", "", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "sql experience", "", "years quantitative data analysis years sql hive experience years experience python r scripting languages experience designing analytic solutions open ended problems preferred qualifications experience designing data quality metrics implementing etl validations experience druid columnar data stores experience bi tools tableau looker", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "", "", "master' degree quantitative discipline e g statistics operations research economics computer science mathematics physics electrical engineering industrial engineering equivalent practical experience years experience working role focused statistical data analysis linear models multivariate analysis stochastic models sampling methods machine learning phd data science statistics similar technical quantitative field ability initiate drive multiple successful improvement initiatives inside across organization ability communicate clearly persuasively creating commitment drive success teams peers", "", "", "", "", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines", "passionate empirical research asking answering questions large datasets finding patterns insights within structured unstructured data strong communication interpersonal skills ability work team environment quick learner adapts well fast moving environment gets things done combines creativity problem solving skills attitude overcome obstacle understanding statistical predictive descriptive modeling concepts machine learning approaches clustering classification techniques recommendation optimization algorithms familiar data visualization using tools like python r experience web based visualization tool shiny d3 plotly knowledge machine learning algorithms using creating knowledge geospatial experience designing evaluating results complex controlled experiments experience computer programming using technologies languages like c java r python scala related knowledge relational database multi dimensional concepts ability perform complex queries sql server environment", "bachelor science arts degree higher computer science masters science phd candidate discipline requiring strong mathematics statistical methods years developing software java python c high level languages relevant experience described solid background machine learning statistical analysis software development solid background machine learning statistical analysis clustering algorithms strong software development skills knowledge experience predictive modeling including multivariate regression logistic regression combinatorial optimization stochastic processes complex analysis principal component analysis time series analysis experience matlab r weka experience using agile software development methodology develop deliver software support continuous integration continuous deployment process working knowledge linux strong communication presentation skills - must able explain present hypotheses analysis results wide audience clear concise manner", "", "data science dynamic evolving profession - ' looking people love learn find unique solutions without micro managed ' freedom try new things test solutions technologies tell us ' better path know way around cloud console handled pretty large volumes data time series plus familiar machine learning frameworks like tensorflow pytorch got experience deep learning maybe used enterprise products past high throughput data ingestion analysis", "", "phd computer science computer engineering ms years experience related field demonstrated history driving delivering analytics models solutions deep knowledge fundamentals machine learning data mining statistical predictive modeling extensive experience applying methods real world problems strong skills software prototyping engineering expertise applicable programming analytics languages python r c c various open source machine learning analytics packages generate deliverable modules prototype demonstrations work desired interdisciplinary skills include big data technologies etl statistics causal inference deep learning modeling simulation breadth skills experience machine learning - diverse types data diverse data sources different types learning models diverse learning settings ability inclination work multi disciplinary environments desire see ideas realized practice experience knowledge services domains business process outsourcing systems transportation systems healthcare systems financial services valued demonstrated ability propose novel solutions problems performing experiments show feasibility solutions working refine solutions real world context prior experience similar role required please include requirements appropriate must currently eligible work us employer without sponsorship", "", "", "", "", "", "", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "bachelor degree computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics gpa least scale experience real world data thesis research internships work experience creativity initiative integrity leadership abilities problem solving skills advanced degree data science equivalent field sub field experience working data rich problems research programs experience computer programming user experience user interface ability successfully complete projects large incomplete data provide solutions bachelor degree computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics gpa least scale resume cover letter specify qualifications position please address want work role differentiates applicants unofficial transcripts degrees writing sample five pages maximum single spaced technical analytic paper focuses current area expertise interest related interest positions cia excerpt longer papers", "bachelor science years data science experience master science years data science experience years experience sql relational databases example db2 oracle sql server years experience statistical programming languages example sas r bachelor degree statistics economics analytics mathematics years experience analytics related field certificate business analytics data mining statistical analysis doctoral degree statistics economics analytics mathematics year experience analytics related field demonstrates date expertise applies development execution improvement action plans develops analytical models drive analytics insights leads small participates large data analytics project teams models compliance company policies procedures supports company mission values standards ethics integrity participates continuous improvement data science analytics presents data insights recommendations key stakeholders", "commuter benefits flexible spendi", "", "bachelor' master' degree math statistics economics related analytical field solid sql skills ability build manage databases \"data labs\" knowledge least one scripting language - r python preferred - ability interest learn python demonstrable knowledge hypothesis testing distributions bayesian methods demonstrable knowledge healthcare data e g claims electronic health records experience relating data science approaches lay businesspeople expert data visualization package e g matplotlib seaborn ggplot tableau years experience working data science team focused healthcare problems proficient using microsoft word excel powerpoint strong interpersonal skills demonstrated ability influence motivate teams highly detail oriented ability coordinate initiatives little supervision strong oral written presentation skills levels organization ability apply independent thought judgment organize work priorities meet specific objectives tight project deadlines ability organize manage concurrent projects", "", "", "", "", "", "phd computer science statistics applied math quantitative field strong background machine learning data mining strong knowledge experiences machine learning statistical modeling e g neural networks decision trees clustering regression analysis required expertise data warehouse sql programming required expertise one statistical programing languages python r sas base sas stats sas enterprise miner required experience payment fraud detection prevention plus familiarity open source python based ml libraries plus", "graduate degree machine learning computer science artificial intelligence applied mathematics statistics physics related technical field python numpy pandas sklearn xgboost tensorflow etc java", "", "expert programming skills java python scala years relevant work experience experience building using large scale knowledge graphs including linked data ontologies rdf owl sparql strong command linear algebra statistics ability quickly translate ideas efficient elegant code development experience python java scala good command respective data pipelining matrix algebra statistics libraries experience nlp methods lsa lda semantic hashing word2vec lstm bidaf etc experience information retrieval tools elastic search lucene solr graph databases neo4j orientdb triple store tuning optimization sequential deep learning models ms computer science emphasis data science analytics machine learning phd preferred", "", "strong data mining machine learning background experience big data environment years experience working real world noisy data phd computer science math related quantitative field experience recommender systems information retrieval graph analysis statistical modeling neural network natural language processing experience large scale data analysis building high performance computational software experience e commerce analyzing clickstream data proficiency python sql statistics experienced software engineering practices providing solutions development implementation scaling execution validation monitoring improvement data science solutions strong communication data presentation skills ability communicate data driven stories ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "", "", "", "", "", "years' practical experience sas etl data processing database programming data analytics experience predictive modeling python sas alteryx angos r experience programming languages java python asset extensive background data mining statistical analysis experience analytics reporting tools tableau ms power bi tibco sisense qlik sap bo experience mining claims emr data preferably oncology related data able understand various data structures common methods data transformation", "", "education bachelor' degree engineering finance mathematics stats econometrics quantitative field intermediate advanced english communications skills years experience data analytics understanding applying structure schema source operational data plan track view share databases postgresql mysql must pentaho data integration experience must create maintain etl elt processes create maintain data warehouse data cubes domo knowledge experience plus", "", "", "", "", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "ph ms degree computer science applied mathematics related field", "years experience data science analytics fields experience programming languages r python scala experience processing analyzing large scale data volumes semi structured unstructured data near real time throughput experience applying right ml model solve business problems ability passion learn new techniques stay cutting edge experience working aws hadoop ecosystems strong database knowledge expertise sql good storytelling presentation skills e able present business side story data presents c team business leaders competitive health insurance benefits competitive salary annual target bonus commission parental leave weeks dependent eligibility paid vacation sick time employee stock purchase program free snacks beverages frequent company update talks leadership team free listing homeaway com electronic adjustable stand desk discounted metro rail pass", "ability explain complex analytical concepts people fields", "", "phd degree statistics biostatistics mathematics physics operations research econometrics related field exceptional interpersonal communication skills ability work independently drive projects years relevant experience proven track record leveraging massive amounts data drive product innovation strong statistical knowledge intuition ability tease incrementality vs correlations understanding predictive modeling time series probabilistic graphical models strong skills sql python r experience distributed analytic processing technologies hive pig presto spark data visualization skills convey information results clearly deep product sense", "phd computer science computer engineering ms years experience related field demonstrated history driving delivering analytics models solutions deep knowledge fundamentals machine learning data mining statistical predictive modeling extensive experience applying methods real world problems strong skills software prototyping engineering expertise applicable programming analytics languages python r c c various open source machine learning analytics packages generate deliverable modules prototype demonstrations work desired interdisciplinary skills include big data technologies etl statistics causal inference deep learning modeling simulation breadth skills experience machine learning - diverse types data diverse data sources different types learning models diverse learning settings ability inclination work multi disciplinary environments desire see ideas realized practice experience knowledge services domains business process outsourcing systems transportation systems healthcare systems financial services valued demonstrated ability propose novel solutions problems performing experiments show feasibility solutions working refine solutions real world context prior experience similar role required please include requirements appropriate must currently eligible work us employer without sponsorship", "", "bachelor' degree computer science information systems mathematics statistics related quantitative discipline applicants data related expertise professional background media also encouraged apply years professional experience media company preferred enthusiasm wall street journal understanding product must entrepreneurial attitude toward work sweat details well healthy skepticism status quo experience using analytics tools sql tableau excel interest advanced topics analytics artificial intelligence data engineering worked visualization machine learning libraries either r python experience building web applications agile development plus", "", "phd computer science computer engineering ms years experience related field demonstrated history driving delivering analytics models solutions deep knowledge fundamentals machine learning data mining statistical predictive modeling extensive experience applying methods real world problems strong skills software prototyping engineering expertise applicable programming analytics languages python r c c various open source machine learning analytics packages generate deliverable modules prototype demonstrations work desired interdisciplinary skills include big data technologies etl statistics causal inference deep learning modeling simulation breadth skills experience machine learning - diverse types data diverse data sources different types learning models diverse learning settings ability inclination work multi disciplinary environments desire see ideas realized practice experience knowledge services domains business process outsourcing systems transportation systems healthcare systems financial services valued demonstrated ability propose novel solutions problems performing experiments show feasibility solutions working refine solutions real world context prior experience similar role required please include requirements appropriate must currently eligible work us employer without sponsorship", "", "phd computer science computer engineering ms years experience related field demonstrated history driving delivering analytics models solutions deep knowledge fundamentals machine learning data mining statistical predictive modeling extensive experience applying methods real world problems strong skills software prototyping engineering expertise applicable programming analytics languages python r c c various open source machine learning analytics packages generate deliverable modules prototype demonstrations work desired interdisciplinary skills include big data technologies etl statistics causal inference deep learning modeling simulation breadth skills experience machine learning - diverse types data diverse data sources different types learning models diverse learning settings ability inclination work multi disciplinary environments desire see ideas realized practice experience knowledge services domains business process outsourcing systems transportation systems healthcare systems financial services valued demonstrated ability propose novel solutions problems performing experiments show feasibility solutions working refine solutions real world context prior experience similar role required please include requirements appropriate must currently eligible work us employer without sponsorship", "", "bring years industry experience working data science team hands experience nlp mining structured semi structured unstructured data rich history crafting new solutions evolving problems bring affinity apple media products digital content general enjoy paying attention details almost much big technical wins passionate working large scale data sets bring experience solr lucene cassandra related technologies experience machine learning tools libraries spark intuitive understanding machine learning algorithms supervised unsupervised modeling techniques highly technical detail oriented creative motivated focused achieving results excited reaching millions users across many platforms share obsession quality value strong interpersonal skills well experience driving decisions across diverse organizations love collaborating tight deadlines tackles problems imaginative elegant solutions creative problem solving skills utilized daily", "", "", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "", "", "", "", "", "expect would technical masters phd program", "education computer science statistics physics mathematics economics specialization certification", "", "", "strong programming skills ability explore large data sets excellent written verbal communication skills report research results methodologies publications top tier journals focusing topics plus preferred candidates pursuing masters computer science least years working experience within quantitative trading research role experience top tier quantitative investment firms aqr capital qms capital two sigma e shaw preferred development experience experience working startup fast paced development environment master", "", "bachelor degree math statistics operations research computer science engineering econometrics quantitative social science quantitative field equivalent combination education work related experience years industry working experience performing advanced quantitative analyses professional experience working python r sql professional experience big data manipulation proven ability apply advanced statistical methodologies multiple regression model mixed models time series models bayesian preferred neural networks cluster analysis text mining prior experience optimization simulation marketing mix multivariate testing ensemble modeling graph algorithms ability apply advanced optimization methodologies linear mixed integer optimization ability apply advanced simulation modeling methodologies techniques must relational database experience strong passion empirical research answering hard questions data curiosity humility empathy masters' degree ph digital experience professional experience software development practices", "bachelor' degree computer science computer engineering related field equivalent combination education related experience comprehensive knowledge one following high performance computing scientific data analysis statistical analysis knowledge discovery computer security systems programming large scale data management big data technologies skilled aspects software project life cycle feasibility requirements design implementation integration test deployment experience developing software c c java python r matlab software applications linux unix windows environments data analysis algorithms data management approaches relational databases machine learning algorithms ability effectively handle concurrent technical tasks conflicting priorities approach difficult problems enthusiasm creativity change focus necessary work independently implement research concepts multi disciplinary team environment commitments deadlines important project success effective interpersonal skills necessary interact levels personnel effective advanced analytical problem solving decision making skills develop creative solutions complex problems significant experience demonstrated expertise following technical languages concepts constructs one following advanced areas high performance computing scientific data analysis statistical analysis knowledge discovery computer security systems programming large scale data management big data technologies included best places work glassdoor work premier innovative national laboratory comprehensive benefits package flexible schedules depending project needs", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "nice experience working database storage systems like postgres druid aerospike elasticsearch", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "phd masters statistics mathematics computer science engineering related discipline least one year research experience applying quantitative research e g artificial intelligence operations research solving real world problems least one year experience python ability communicate complex ideas clear precise actionable manner ability execute propose execute analytics plan collaborating colleagues well outside team ability solve complicated problems decomposing problem gradually demonstrating progess demonstrated industrial experience applying artificial intelligence optimization statistics drive key decisions familiarity operations research cplex solving integer problems experience basic machine learning techniques statistics mixed models well r programing languages", "", "strong data mining machine learning background experience big data environment years experience working real world noisy data phd computer science math related quantitative field experience recommender systems information retrieval graph analysis statistical modeling neural network natural language processing experience large scale data analysis building high performance computational software experience e commerce analyzing clickstream data proficiency python sql statistics experienced software engineering practices providing solutions development implementation scaling execution validation monitoring improvement data science solutions ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "years experience working sql queries accessing data years consulting experience preferably big similar firm demonstrated expertise business analysis data analysis experience writing requirements specifications systems", "master' degree computer science computer engineering related field equivalent combination education related experience experience developing software python c c comprehensive experience implementing deep learning workflow using one following frameworks theano tensorflow pytorch keras fundamental knowledge experience applying algorithms one following machine learning areas anomaly detection one shot learning deep learning unsupervised feature learning ensemble methods probabilistic graphical models reinforcement learning broad knowledge network protocols dns https knowledge experience computer vulnerabilities buffer overflows code injection format string etc ability effectively manage concurrent technical tasks contending priorities well approaching difficult problems enthusiasm creativity change focus necessary lead multidisciplinary teams areas machine learning deep learning algorithms pursue program development opportunities co authoring proposals proposing ideas address sponsor needs identify program growth opportunities existing customers understanding customer space needs ph degree computer science computer engineering related field experience high performance computing parallel programing cloud computing experience modbus dnp3 iec iec protocols familiarity full stack software development included best places work glassdoor work premier innovative national laboratory comprehensive benefits package flexible schedules depending project needs", "", "", "", "", "preferred bachelors science computer science math scientific computing data analytics machine learning business analyst nanodegree equivalent experience requires years experience phd masters approved field minimum years relevant experience", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members", "post grad degree equivalent statistics mathematics data science analytics related area least years solid experience marketing analytics arena advanced knowledge r experience sas spss experience writing sql queries exposure big data technologies hadoop hive advanced knowledge excel experience handling integrating modelling digital behavioural data including segmentation predictive analytics retrieving information webpages product characteristics reviews etc ability transform scraped data usable format integrating scraped data data sources", "", "", "multiple years proven track record application ml nlp global business rely diversity culture thought deliver goals", "", "bachelor' degree economics math science finance engineering similar discipline least year experience comfortable developing statistical models using python sas r statistical packages master' degree economics math statistics finance engineering similar discipline year experience business application machine learning techniques statistical analysis experience python development highly desired strong data analysis communication skills", "", "", "", "", "", "", "", "years experience multi faceted software engineer strong back end orientation experience architecting schemas building databases experience scripting languages building maintaining data pipelines possess good organizational communication analytical technical writing skills experience working scientists r systems would beneficial background machine learning data science statistics exceptional multitasking skills attention detail ability work independently excellent communication presentation skills self motivated passionate comfortable working fast paced environment", "", "", "strong data mining machine learning background experience big data environment years experience working real world noisy data phd computer science math related quantitative field experience recommender systems information retrieval graph analysis statistical modeling neural network natural language processing experience large scale data analysis building high performance computational software experience e commerce analyzing clickstream data proficiency python sql statistics experienced software engineering practices providing solutions development implementation scaling execution validation monitoring improvement data science solutions ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "", "", "bs ms phd healthcare related field minimum years experience solving data science problems healthcare domain masters phd application data science solutions industrial projects", "", "love games", "", "", "", "bachelor' degree business economics statistics related field minimum years general management experience business marketing analytics field equivalent combination experience training provides required knowledge skills abilities master' degree preferred strong analytical skills curiosity attention detail data accuracy proven experience working large datasets relational database analytical tools alteryx tableau sas jmp similar highly skilled excel proficient powerpoint keynote advanced statistical skills plus multi variate analysis cluster analysis etc crm retail consumer loyalty analytics experience highly desired excellent project coordination management skills comfortable reaching business partners adept cross functional collaboration must able develop analytic plans manage multiple projects simultaneously adept translating data concise insightful business friendly executive summaries presentations strong written verbal skills travel air overnight required amount time lifting bending pounds amount weight", "", "", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "", "", "", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "", "", "", "", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "", "", "", "", "", "", "", "master' degree computer science engineering statistics quantitative fields theoretical knowledge machine learning algorithms statistics etc basic coding ability python r programming skills java scala plus full cycle machine learning engine implementation via internship school project", "bachelor degree minimum year experience predictive statistical modeling using sas r python proficient ms office applications excel proficiency pivots v lookups formulas bilingual spanish english master' degree experience performing data analysis experience extracting data using sql data exploration tools sas r experience data analytics design experience working large databases health care industry experience demonstrated ability effectively gather requirements probe deeper understanding translate deep technical concepts non technical well technical senior stakeholders marketing customers data scientists demonstrated ability manage people prioritize deliverables proven organizational skills ability flexible work ambiguity", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "", "find excited tell people building fetch wake truly excited know today directly impact young growing company", "", "", "", "external technology knowledge keeps abreast latest technological developments areas push technology roadmaps realization constantly looks opportunities incorporate new solution methods solve existing problems", "bonus points building analyzing web mobile applications business intelligence tools", "", "", "", "", "", "bachelor' degree higher studies data analytics statistics mathematics computer science related field completion mhs lean six sigma green belt curriculum required within years job placement years' experience data analysis operations improvement work computer science business intelligence work related experience must strong problem solving skills keen drive learn explore datasets understanding application interpretation statistical tools hypothesis testing non parametric tests analysis variation various forms regression factor analysis well various forecasting prediction techniques basic understanding structured query language sql code process etl techniques extracting data systems transforming data forms necessary analysis experience statistical computer packages minitab spss sas manipulate data draw insights large data sets basic understanding relational database rdb systems explore various data architectures using standard tools ex microsoft sql management studio oracle sql developer etc basic understanding data mining concepts", "", "like site cafeteria free parking access crown center fitness center", "bachelor master degree highly quantitative field cs machine learning mathematics statistics equivalent experience ms years bs years experience machine learning statistical modeling data mining analytics techniques experience python statistical machine learning software experience applying various machine learning techniques understanding key parameters affect performance experience developing experimental analytic plans data modeling processes use strong baselines ability accurately determine cause effect relationships experience one natural language processing topics tagging syntactic parsing word sense disambiguation topic modeling contextual text mining application deep learning nlp previous experience ml data scientist role large technology company fluency language english", "deep expertise search preferably e commerce excellent knowledge improving search incrementally well making step changes applying nlp text mining etc proven track record building robust search systems achieving strong results expertise personalization recommender systems able guide team engineers identify break necessary architecture services support search personalization services able guide work well team engineers implement machine learning algorithms models well implementing necessary software solid understanding search metrics implementing tracking measure performance solid understanding search engines utilizing features effectively familiarity elasticsearch plus hands experience developing implementing machine learning algorithms models background machine learning statistics information retrieval design implement test robust technical solutions high traffic site apps rely write clean code ' testable maintainable solves right problem well code proud phd masters equivalent experience quantitative field computer science physics mathematics bioinformatics etc plus means must experience programming functional languages scala golang haskell clojure etc plus must knowledge scripting languages like python r familiarity web frameworks plus experience java scala microservices plus understanding b testing able key influencer team' strategy contribute significantly team planning showing good judgement making technical trade offs team' short term long term business needs needs company whole strong team player superb communication skills thrives collaborative environment committed success team whole critical thinking ability track complex data engineering issues evaluate different algorithmic approaches analyze data solve problems creativity conceive new data driven products features technologies results prioritize focusing ideas features significant measurable impact planning estimation ability set meet project objectives milestones communicate results progress internally externally meetings presentations tech talks passion technology developers always evaluating new tools technologies make us better attracted interest lately", "years experience data scientist tools r python analytical methods regression modeling forecasting machine learning algorithms including ensemble models neural nets bayesian models model selection validation ability take digital marketing data translate valuable insights clients education master degree statistics quantitative analysis business analytics biostatistics related discipline love data enjoy taking new challenges afraid chasing answer lifelong learner enjoy reading new trends data science learning new tool trending connecting data junkies experience working clients enjoy helping business grow answering complicated business needs implementation data science project impeccable attention detail", "", "new york ny", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "", "", "", "", "", "", "master' degree quantitative technical field math statistics engineering economics physics computer science etc experience data exploration machine learning tools found r python demonstrated ability applying advanced statistical modeling techniques solve problems ability craft rigorous research evaluation design approaches based upon understanding clients research needs advanced knowledge experience querying languages sql etc understanding modern software development engineering practices including scrum agile git devops competitive compensation full health benefits medical dental vision 401k paid time tuition reimbursement full service gym game lounge area basketball court free healthy snacks refreshments subsidized public transit", "bs master degree computer science electrical engineering related degree equivalent experience years experience software engineering infrastructure design skills write well structured maintainable idiomatic code good documentation strong work ethic passion problem solving real world experience python make generous use tools like celery mongodb pandas scikit learn django experience data science fundamentals machine learning natural language processing development deployment software within linux environments", "years work experience years work experience applying scientific methods solve real world problems degree computer science applied statistics economics etc ability write structured efficient sql queries large data sets proven track record using analysis impact key business product decisions familiar data pipelines knowledge transform raw production external data user friendly tables fluent data science libraries python r comfortable amazon web services apache spark tensorflow etc ability organize clearly communicate insights stakeholders", "huge technical problems solve - constantly learning pushing boundaries working smartest people around", "", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "masters degree data science statistics operations research highly quantitative field e g computer science operations research systems engineering physics equivalent experience years industry experience predictive modeling data science analysis programming experience python r equivalent demonstrated experience data science data analysis desire pursue challenging questions extensive operational data analysis experience data analysis regression analysis demonstrated outstanding written verbal communication skills comfortable linux environment experience amazon web services aws e g dynamodb auroradb mysql s3 sqs sns ec2 interest experience experimental design background applied statistics machine learning", "", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "phd computer science computer engineering ms years experience related field demonstrated history driving delivering analytics models solutions deep knowledge fundamentals machine learning data mining statistical predictive modeling extensive experience applying methods real world problems strong skills software prototyping engineering expertise applicable programming analytics languages python r c c various open source machine learning analytics packages generate deliverable modules prototype demonstrations work desired interdisciplinary skills include big data technologies etl statistics causal inference deep learning modeling simulation breadth skills experience machine learning - diverse types data diverse data sources different types learning models diverse learning settings ability inclination work multi disciplinary environments desire see ideas realized practice experience knowledge services domains business process outsourcing systems transportation systems healthcare systems financial services valued demonstrated ability propose novel solutions problems performing experiments show feasibility solutions working refine solutions real world context prior experience similar role required please include requirements appropriate must currently eligible work us employer without sponsorship", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "", "three five years experience positions increasing responsibility working large datasets conducting statistical quantitative modeling melding analytics strong programming data mining clustering segmentation bachelor' degree combination years education work experience predictive modelling machine learning related quantitative field computer science physics mathematics statistics bioinformatics etc strong data statistical programming skills e g sql python r stata sas sql hadoop hive large data systems data scientist depth knowledge experience machine learning predictive analytics proficiency r python programming languages tools used data manipulation visualization experience working data warehouses depth knowledge relevant clinical informatics software systems highly complex concepts principles policies methodologies techniques best practices regulations standards practices involved patient care electronic medical data management uc health care system broadly encompassing highly depth knowledge machine learning deployment predictive analytics operational environments knowledge controlled terminology clinical workflows user interface optimization clinical decision support rules development data integration mining clinical ontologies adoption technology clinical domain knowledge preferred required may developed expanded hiring advanced organizational project management skills ability lead team prioritize tasks see projects inception completion schedule advanced interpersonal communications skills convey highly technical information instructions levels clinical users clear concise manner provide technical support develop deliver training materials needed ability apply advanced problem resolution skills highly complex issues quickly diagnose problems develop test implement appropriate effective solutions timely manner advanced analytical skills expertise documentation reporting ability apply metrics design run queries collect analyze performance data produce sophisticated reports analyses management use advanced ability serve technical leader information resource work collaboratively senior staff management across departments providing advice counsel analysis issues policy functionality system efficiency upgrades business analytics industry advances trends strong interest working health care data understanding challenges face complex health care delivery systems flexibility orient work ucsf medical center locations completion one two years undergraduate graduate level coursework statistics master' degree doctorate computer science related area background stata sas prior experience healthcare data particular epic derived healthcare data demonstrates service excellence following everyday pride guide ucsf medical center standards expectations communication behavior standards expectations convey specific behavior associated medical center' values professionalism respect integrity diversity excellence provide guidance communicate patients visitors faculty staff students virtually everyone every day every encounter standards include limited personal appearance acknowledging greeting patients families introductions using aidet managing service recovery managing delays expectations phone standards electronic communication team work cultural sensitivity competency uses effective communication skills patients staff demonstrates proper telephone techniques etiquette acts escort patient family member needing directions shows sensitivity differences culture demonstrates positive supportive manner patients families colleagues perceive interactions positive supportive exhibits team work skills positively acknowledge recognize colleagues uses personal experiences model teach living pride standards exhibits tact professionalism difficult situations according pride values practices demonstrates understanding adheres privacy confidentiality security policies procedures related protected health information phi sensitive personal information demonstrates understanding adheres safety infection control policies procedures assumes accountability improving quality metrics associated department unit meeting organizational departmental targets keeps working areas neat orderly clutter free including hallways adheres cleaning processes puts things back belong removes reports broken equipment furniture picks disposes litter found throughout entire facility posts flyers posters designated areas post walls doors windows knows environment care manual kept department corrects reports unsafe conditions appropriate departments protects physical environment equipment damage theft", "", "previous message jobs fwd omb seeking applications chief statistician next message jobs tenure track position university hawaii messages sorted date thread subject author", "", "", "", "", "", "years' experience applying machine learning techniques optimization statistics drive key decisions extensive hands experience development predictive models machine learning ai based solutions solid programming skills python r similar data science language advanced proficiency data visualization prefer financial services industry experience prefer experience crm financial analysis financial advisory bachelors master' degree computer science math data science statistics united states citizen", "fortune \"world' admired companies\" - corporate responsibility magazine \" best corporate citizens\" - informationweek \"elite \" - women' business enterprise national council \"america' top corporations women' business enterprises\" reputation institute \"world' reputable companies\" -", "", "strong data mining machine learning background experience big data environment years experience working real world noisy data phd computer science math related quantitative field experience recommender systems information retrieval graph analysis statistical modeling neural network natural language processing experience large scale data analysis building high performance computational software experience e commerce analyzing clickstream data proficiency python sql statistics experienced software engineering practices providing solutions development implementation scaling execution validation monitoring improvement data science solutions ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "", "", "", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "", "master degree mathematics physics statistics computer science engineering economics operations research bioinformatics computational biology similar quantitative field equivalent practical experience experience statistical software database languages e g sql r python matlab experience using applied statistics analyze data experience training deploying machine learning models phd degree scientific field leveraging statistics experience machine learning libraries e g tensorflow scikit learn keras theano torch experience google cloud platform ability draw conclusions data recommend actions ability break technical concepts simple terms present diverse technical non technical audiences effective written verbal communication skills", "", "", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "", "", "doctorate preferred", "", "master' degree bachelor' degree years experience engineering computer science statistics bioinformatics related field strong background statistical analysis machine learning data mining including regression analysis classification predictive modeling feature engineering hypothesis testing etc solid understanding probability theory statistics experience manipulating large data sets time series intermittent data strong communication organization skills proven ability contribute emerging cross disciplinary fields experience performing independent research experience working python development language emphasis data science experience python data analysis packages scikit learn pandas scipy comfortable working linux based environment experience working database types mongodb mysql postgres knowledge bayesian data analysis methods model comparison markov chain monte carlo sampling hierarchical modeling generalized linear models bayesian neural networks experience working clinical data", "ms phd degree computer science artificial intelligence machine learning related technical field prior experience numerical topic modeling technologies linux python", "", "", "years experience working intelligence community teams working fields related data science statistical modeling information retrieval text analysis data mining machine learning intelligence analysis cyber threat analysis image analysis network security statistical modeling geo spatial analytics data munging cleaning bachelor degree ability work datasets different sizes formats across multiple databases knowledge experience specific techniques neural networks cluster analysis feature engineering extraction reduction web scraping decision trees cart collaborative filtering geo spatial analysis experience pcap data elastic search hadoop hdfs git spark mllib sql os experience windows linux windows ability compile results deliver presentations senior level leadership strong communication skills ability present material audiences differing technical aptitude r r shiny r studio python sci kit tensorflow intelligence community experience machine learning statistical modeling experience multi tb dataset manipulation cleaning querying modeling experience scikit learn tensorflow r caret experience curating datasets supervised unsupervised machine learning methods experience c java r javascript php matlab pig hive impala pyspark scala ruby pytorch", "must master' degree foreign equivalent statistical science mathematics related quantitative field plus three years experience position offered vice president data scientist senior statistician related position must three years experience within financial industry extracting useful insights large messy data sets developing applying statistical methods solving complex problems performing quantitative research programming multiple languages including python java c c sql r writing well structured robust code research production utilizing large scale distributed computing technology including spark hadoop analyzing financial datasets utilizing natural language processing techniques", "", "' experience data mining machine learning statistical modeling underlying methods algorithms ' extensive experience analyzing large data sets using software r python power bi proficient working sql databases experience microsoft azure cloud computing platform plus proven track record using data provide actionable impactful business results prior experience within insurance financial services healthcare industry preferred effective communications skills instill confidence internal external audiences translate complex concepts non technical stakeholders help enable understanding drive informed business decisions low ego team oriented collaborative approach keeping corporate culture solutions oriented mind set able work effectively complex problems easily establish trusted partner ability build effective partnerships across areas organization colleagues levels ability influence variety stakeholders drive cultural organizational change intellectual curiosity passion data results orientation", "messages sorted date thread subject author", "", "", "", "", "", "least years progressive experience data science statistical analysis data modeling years experience statistical software insurance industry experience preferred professional experience building sophisticated models via regression segmentation decision tree time series design experiments multivariate analysis experience machine learning techniques algorithms svm random forests neural etc experience statistical packages one r sas spss statistica stata alteryx knime etc required python scikit computer language experience plus experience bi tools like tableau msbi etc plus", "", "", "advanced degree including mba preferred", "work engineers define manage data sources design implement new distributed machine learning methods implement data warehouses real time etl batch processing data support modeling needs build maintain internal data processing visualization tools years data science work academic experience deep experience python experience shipping products features early often experience working large datasets especially using dask kinesis background time series analysis hidden markov models gaussian processes variational inference familiarity node js front end back end javascript", "", "project portfolio github personal website model deployment experience data engineering web development business aspects entrepreneurial mindset translate business math tech apply analytics make better faster consistent business decisions measurable roi prescriptive analytics experience active learning causal inference cost sensitive classification design experiments interactive machine learning reinforcement learning hadoop experience typically requires bachelor' degree least years experience quantitative discipline like statistics math computer science engineering operations research master' degree experience normal office environment", "least years experience java spring mysql relational database python least years experience data scientist experience databases including nosql experience machine learning frameworks libraries supervised unsupervised learning machine learning concepts techniques regularization boosting random forests decision trees bayesian models neural networks support vector machines svm experience whole etl data cycle extract validate transform clean aggregate audit archive computer science mathematics physics degree excellent communication analytical skills willingness work hard hrs per week good english experience apache spark natural language processing tokenization tagging sentiment analysis entity recognition summarization r programming language modeling complex problems discovering insights identifying opportunities use statistical algorithmic mining visualization techniques participating areas architecture design implementation testing proposing innovative ways look problems using data mining approaches set information available designing experiments testing hypotheses building models conducting advanced data analysis designing highly complex algorithm applying advanced statistical predictive modeling techniques build maintain improve multiple real time decision systems competitive salary based prior experience qualifications potential stock options first year raise advancement opportunities based periodic evaluations visa sponsorship working outside us sponsorship granted months company based performance health benefits case working office washington dc position location requirement performed either remotely including outside u wallethub' offices downtown washington dc intending work outside us please aware position entails working least hour per week requires overlap est business hours 8am 7pm et including hour break", "", "", "experience querying databases using statistical computer languages r python slq etc experience creating using advanced machine learning algorithms statistics regression simulation scenario analysis modeling clustering decision trees neural networks etc experience analyzing data 3rd party providers google analytics adobe analytics experience distributed data computing tools map reduce hadoop hive spark experience visualizing presenting data", "", "", "bs computer science statistics applied mathematics physics engineering ms phd preferred excellent communication collaborative skills years professional experience applications machine learning data science analysis deep expertise least one ml frameworks like scikit learn keras tensorflow etc proficient least one programming language like python r c java", "", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems ability build scalable systems analyze huge data sets make actionable recommendations strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "united states preferred", "", "", "strong data mining machine learning background experience big data environment years experience working real world noisy data phd computer science math related quantitative field experience recommender systems information retrieval graph analysis statistical modeling neural network natural language processing experience large scale data analysis building high performance computational software experience e commerce analyzing clickstream data proficiency python sql statistics experienced software engineering practices providing solutions development implementation scaling execution validation monitoring improvement data science solutions strong communication data presentation skills ability communicate data driven stories ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "years experience working intelligence community teams working fields related data science statistical modeling information retrieval text analysis data mining machine learning intelligence analysis cyber threat analysis image analysis network security statistical modeling geo spatial analytics data munging cleaning bachelor degree ability work datasets different sizes formats across multiple databases knowledge experience specific techniques neural networks cluster analysis feature engineering extraction reduction web scraping decision trees cart collaborative filtering geo spatial analysis experience pcap data elastic search hadoop hdfs git spark mllib sql os experience windows linux windows ability compile results deliver presentations senior level leadership strong communication skills ability present material audiences differing technical aptitude r r shiny r studio python sci kit tensorflow intelligence community experience machine learning statistical modeling experience multi tb dataset manipulation cleaning querying modeling experience scikit learn tensorflow r caret experience curating datasets supervised unsupervised machine learning methods experience c java r javascript php matlab pig hive impala pyspark scala ruby pytorch", "experience distributed software packages full time year term appointment possibility extension conversion career appointment based upon satisfactory job performance continuing availability funds ongoing operational needs", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "", "", "", "masters level degree statistics biostatistics related fields experience sas spss years experience research setting using quantitative methods principles statistics", "least msc phd preferred degree math applied maths electrical engineering computer science related field experience leveraging software development machine learning years advanced analytics ai graph theory numerical analysis etc strong mathematical background strong software skills python numeric scientific computation libraries equivalent query languages like sql derivatives used datastores e g hiveql bigquery etc experience using complex data structures algorithms experience implementation optimization strategies experience applying machine learning computational math topics ecommerce sales marketing phd degree computer science computer engineering electrical engineering related field excellent communication management skill lead team towards execution product vision experience evaluating making decisions around use new existing tools project ready able coordinate cross disciplinary teams throughout phases development ability make right trade offs schedule resources scope order deliver project competitive base salary equity stake competitive benefits", "phd computer science computer engineering ms years experience related field demonstrated history driving delivering analytics models solutions deep knowledge fundamentals machine learning data mining statistical predictive modeling extensive experience applying methods real world problems strong skills software prototyping engineering expertise applicable programming analytics languages python r c c various open source machine learning analytics packages generate deliverable modules prototype demonstrations work desired interdisciplinary skills include big data technologies etl statistics causal inference deep learning modeling simulation breadth skills experience machine learning - diverse types data diverse data sources different types learning models diverse learning settings ability inclination work multi disciplinary environments desire see ideas realized practice experience knowledge services domains business process outsourcing systems transportation systems healthcare systems financial services valued demonstrated ability propose novel solutions problems performing experiments show feasibility solutions working refine solutions real world context prior experience similar role required please include requirements appropriate must currently eligible work us employer without sponsorship", "apply data science techniques support early warning conflict instability diagnostics related fragility relationship international development challenges assist usaid subject matter experts data science focused analytical thinking improved planning monitoring evaluation reporting programs conflict affected fragile environments design develop deliver data analysis visualization products support dcha cmm conflict fragility violence work emphasis early warning acquire process manage analyze data range sources improved decision making areas conflict fragility violence collaborate team interdisciplinary experts particularly usaid geocenter interagency partners regional bureaus develop manage interim early warning tool coordination technical staff dcha cmm serve technical advisor team responsible adapting evolving early warning tools provide consultation non technical audiences inside outside usaid develop implement guidance integrating data analysis work related conflict fragility violence prepare concept papers background analyses briefings build support use data analytics data science techniques early warning conflict instability dynamics related fragility participate discussions among key usaid stakeholders articulate vision plan integrating data science development work related conflict fragility violence provide training capacity building services non technical audiences accessing analyzing visualizing data advanced quantitative degree statistics physics math computer science economics engineering related technical field strong quantitative background experience data collection cleaning processing applied analysis visualization using statistical software programming languages python stata sas r spss matlab tableau powerbi ability produce compelling data visualization products using adobe applications illustrator photoshop indesign ability create interactive web based products using html css javascript curious self motivated individual excellent communications skills must able distill highly technical quantitative methods policy relevant snippets experience translating statistical regression visualization results briefing documents presentations senior leadership experience working data science pertains conflict fragility violence within foreign policy realm experience applying data science analytical techniques foreign policy programming ability apply data science analytical techniques problems data sets spanning diverse sectors agriculture democracy governance economic growth education environment health strategic project leadership experience leading facilitating projects concept iterative design development delivery ongoing support strong interpersonal skills experience working across different offices agencies", "bachelor master' degree engineering math statistics finance computer science logistics transportation related industry experience years hands experience statistical analysis applying various machine learning techniques predictive modeling data mining years experience data querying languages e g sql scripting languages e g python statistical mathematical software e g r sas matlab experience articulating business questions using quantitative techniques arrive solution using available data master' degree higher engineering math finance statistics computer science technical field accredited university ability develop experimental analytic plans data modeling processes use strong baselines ability accurately determine cause effect relations demonstrable track record dealing well ambiguity prioritizing needs delivering results dynamic environment excellent verbal written communication skills ability effectively advocate technical solutions research scientists engineering teams business audiences experience processing filtering presenting large quantities millions billions rows data", "", "", "", "", "interest causal inference", "phd computer science computer engineering ms years experience related field demonstrated history driving delivering analytics models solutions deep knowledge fundamentals machine learning data mining statistical predictive modeling extensive experience applying methods real world problems strong skills software prototyping engineering expertise applicable programming analytics languages python r c c various open source machine learning analytics packages generate deliverable modules prototype demonstrations work desired interdisciplinary skills include big data technologies etl statistics causal inference deep learning modeling simulation breadth skills experience machine learning - diverse types data diverse data sources different types learning models diverse learning settings ability inclination work multi disciplinary environments desire see ideas realized practice experience knowledge services domains business process outsourcing systems transportation systems healthcare systems financial services valued demonstrated ability propose novel solutions problems performing experiments show feasibility solutions working refine solutions real world context prior experience similar role required please include requirements appropriate must currently eligible work us employer without sponsorship", "bachelor' graduate degree computer science applied statistics equivalent quantitative field demonstrated knowledge multivariate statistical modelling techniques strong background r python sql experience using sas including sas enterprise guide manipulate data smart - well rounded emotional intelligence clear sme areas engaged - small business owner mentality curious - always learning questioning fun - good attitude stress pressure pleasant work team player - recognizes others helps possible looks contribute broader goal", "data science dynamic evolving profession - ' looking people love learn find unique solutions without micro managed ' freedom try new things test solutions technologies tell us ' better path know way around cloud console handled pretty large volumes data familiar machine learning frameworks like tensorflow pytorch maybe used enterprise products past high throughput data ingestion analysis", "", "master degree computer science mathematics years experiences software engineer data engineer data scientist proficiency r python spark proficiency sql hiveql knowledge data visualization experience advanced analytics strong interest gaming industry", "", "", "", "", "", "", "", "", "qualifications listed minimum acceptable considered position salary offers based candidates education level years experience relevant position also take account information provided hiring manager organization regarding work level position qualifications listed minimum acceptable considered position salary offers based candidates education level years experience relevant position also take account information provided hiring manager organization regarding work level position qualifications listed minimum acceptable considered position salary offers based candidates education level years experience relevant position also take account information provided hiring manager organization regarding work level position qualifications listed minimum acceptable considered position salary offers based candidates education level years experience relevant position also take account information provided hiring manager organization regarding work level position", "", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "", "", "", "", "", "bachelor degree minimum year experience predictive statistical modeling using sas r python proficient ms office applications excel proficiency pivots v lookups formulas bilingual spanish english master' degree experience performing data analysis experience extracting data using sql data exploration tools sas r experience data analytics design experience working large databases health care industry experience demonstrated ability effectively gather requirements probe deeper understanding translate deep technical concepts non technical well technical senior stakeholders marketing customers data scientists demonstrated ability manage people prioritize deliverables proven organizational skills ability flexible work ambiguity", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members", "", "advanced degree mathematics statistics economics computer science related fields", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "", "phd computer science computer engineering ms years experience related field demonstrated history driving delivering analytics models solutions deep knowledge fundamentals machine learning data mining statistical predictive modeling extensive experience applying methods real world problems strong skills software prototyping engineering expertise applicable programming analytics languages python r c c various open source machine learning analytics packages generate deliverable modules prototype demonstrations work desired interdisciplinary skills include big data technologies etl statistics causal inference deep learning modeling simulation breadth skills experience machine learning - diverse types data diverse data sources different types learning models diverse learning settings ability inclination work multi disciplinary environments desire see ideas realized practice experience knowledge services domains business process outsourcing systems transportation systems healthcare systems financial services valued demonstrated ability propose novel solutions problems performing experiments show feasibility solutions working refine solutions real world context prior experience similar role required please include requirements appropriate must currently eligible work us employer without sponsorship", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "", "", "experience working amazon web services aws", "quantitative field years professional experience data scientist nlp experience must experience unstructured text data performing tasks text mining sentiment analysis language modeling classification information retrieval tasks proficient r python", "process receiving graduate degree analytical area machine learning computer science physics mathematics statistics engineering similar experience analytics quantitative disciplines experience modeling analysis including machine learning statistical analysis operations research management science data mining strong interpersonal communication skills must able explain technical concepts analyses implications clearly wide audience able translate business objectives actionable analyses experience sql variations thereof python pyspark scala preferred though experience analytics software sas stata matlab mathematica r sparklyr acceptable familiarity aws solutions glue s3 redshift proven analytical quantitative skills use hard data metrics back assumptions develop business cases complete root cause analyses capable taking responsibility initiative working minimal direction self starter even assignments vague undefined process receiving phd quantitative field mathematics statistics analytics economics knowledge experience agile development practices experience demand planning forecasting supply chain inventory management plus demonstrated ability manage multiple competing priorities simultaneously drive projects completion", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members", "minimum years experience since obtaining bachelor degree experience data science projects experience signal processing digital communications familiarity one following programming languages python c c matlab within unix linux programming environments working knowledge machine learning libraries language strong written verbal communication skills position requires ability obtain maintain security clearance issued u government u citizenship required obtain security clearance advanced degree area digital communications computer science machine learning signal processing experience working linux development linux platform hands experience implementing data science solutions python familiarity tensorflow keras pytorch scikit learn familiarity gnu radio experience implementing data science algorithms gpus familiarity agile methodology current active secret special access clearances", "academic background technical quantitative field graduate degree plus equivalent experience working knowledge statistics pertains machine learning distributions statistical testing regression etc proficiency using sql several major dbms dw engines experience variety big data technologies distributed machine learning computing frameworks s3 spark hadoop elasticsearch tensorflow etc good scripting programming skills python unix shell data manipulation skills extract data relational non relational databases files multiple formats clean join slice dice organize analyze explain experience python data science ecosystem pandas numpy scipy scikit learn nltk gensim etc able hit ground running tools fast experience partitioning clustering techniques k means dbscan etc experience text mining parsing classification using state art techniques experience information retrieval natural language processing natural language understanding neural language modeling chat dialog modeling technologies strong background machine learning unsupervised supervised techniques particular excellent understanding machine learning techniques algorithms k nn naive bayes svm decision forests logistic regression mlps rnns etc ability evaluate quality ml models define right performance metrics models accordance requirements business", "bachelor higher degrees business mathematics computer science industrial engineering related fields least years' experience performing advanced quantitative analyses ability manipulate analyzes interprets terabytes data ability organize findings translate actionable insights using original innovative techniques style ability apply advanced statistical methodologies mathematical operations tasks cluster analytics sampling theory design experiments analysis variance correlation techniques factor analysis intermediate advanced experience statistical software e g r sas spss database applications working knowledge sas required must relational database experience", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "", "", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "excellent understanding machine learning techniques algorithms k nn naive bayes svm decision forests etc great communication skills experience data visualisation tools d3 js ggplot etc proficiency using query languages sql experience nosql databases good applied statistics skills distributions statistical testing regression etc good scripting programming skills python java b computer science software engineering information science mathematics statistics electrical engineering physics related fields", "excellent understanding machine learning techniques algorithms clustering decision trees neural networks etc proficiency using python numpy pandas scikit learn tensorflow keras etc r manipulate visualize data build machine learning pipelines experience sql nosql databases elasticsearch excellent understanding math especially probability theory statistics good understanding general computer science algorithms sorting hashing etc good communication presentation skills ability explain technical concepts non technical audience ability work minimum supervision experience cloud technologies hadoop spark etc would plus knowledge java scala would plus minimum bachelor' degree statistics mathematics computer science mis related degree seven years relevant experience combination education training experience master' degree ph statistic mathematics computer science ten years experience highly preferred equivalent combination education experience training analysis identify understand issues problems opportunities compare data different sources draw conclusions communication clearly convey information ideas variety media individuals groups manner engages audience helps understand retain message exercise judgment decision making use effective approaches choosing course action developing appropriate solutions recommend take action consistent available facts constraints probable consequences technical professional knowledge demonstrate satisfactory level technical professional skill knowledge position related areas remains current developments trends areas expertise building effective relationships develop use collaborative relationships facilitate accomplishment work goals client focus make internal external clients needs primary focus actions develop sustain productive client relationships opportunity work bleeding edge projects work highly motivated dedicated team competitive salary flexible schedule medical insurance benefits program corporate social events professional development opportunities", "", "bachelor degree computer science management information systems statistics healthcare administration related field years hands experience machine learning command principles machine learning statistical analysis data mining algorithms mathematical segmentation modeling demonstrated ability use knowledge current techniques develop new methodologies years experience healthcare industry proven ability experience design development solutions increasing yield proven analytical skills experience handling large volume data experience dealing imperfections data experience implementing data visualization solutions working knowledge statistical programming languages 'r' python", "", "proud say work stitch fix know work brings joy clients every day", "must deep hands experience think strategically love getting hands dirty bachelor' degree mathematics statistics engineering computer science technical discipline experience etl development patterns tooling experience real time incremental batch data ingestion expertise schema design developing data models complex data sets expert knowledge mathematical programming language r python preferred strong sql experience ability develop tune debug complex sql applications experience powerbi equivalent bi tool create impactful reports visualizations interactive dashboards ability communicate complex findings clear precise actionable manner able translate high level ideas well defined problems", "bachelor' degree statistics actuarial science related field study years professional experience python r sas including academic experience internships experience working large data sets experience data wrangling cleansing statistical modeling programming experience tableau similar visualization tool travel master degree statistics actuarial science related field study experience data mining predictive modeling experience extensive knowledge tools data mining statistics experience hr analytics strong knowledge ms office products solid statistical understanding good oral written communication skills ability effectively work team environment individual strong work ethic desire help clients improve businesses strong desire success business analytics experience one following industries insurance consumer products packaged goods human resources", "", "", "ability work cross functionally measurement teams product teams members wider analytical teams", "", "extensive experience software development expertise architecting delivering new technologies product features scale highly reliable cloud services experience developing scalable saas monitoring automation logging solutions highly reliable service offerings prior technical paper publications public speaking engagements extensive software development experience one following c java c python experience across windows linux plus strong algorithmic problem solving skills distributed systems experience ability see present big picture offer solutions make better extraordinarily intelligent rigorous thinker operate successfully among bright charismatic people strong customer facing relationship building skills effective working independently team setting ability uncover business challenges develop custom solutions solve challenges years work experience technology industry", "", "", "years revenue forecasting experience needed preferably within retailing domain excellent statistical skills applied regression spatial time series modeling deep understanding design experiments principles proven working knowledge sql proven working knowledge sas r experience scripting automation data extraction transformation modeling outputs experience data visualization tableau apple equal opportunity employer committed inclusion diversity also take affirmative action offer employment advancement opportunities applicants including minorities women protected veterans individuals disabilities apple discriminate retaliate applicants inquire disclose discuss compensation applicants", "prior experience finance", "", "", "solid understanding ad networks media campaigns work", "", "proud say work stitch fix know work brings joy clients every day", "", "ph degree computer science engineering applied mathematics expert knowledge scientific computing language r python sql data science prototyping experience physical systems experience distributed storage compute tools e g hive spark experience bringing prototypes production hadoop spark platforms containerized services experience natural language processing deep learning using tensorflow cnn rnn lstm gans streaming analytics e spark streaming experience data visualization tools tableau plotly bokeh etc experience working remote global teams results driven positive attitude", "", "", "strong data mining machine learning background experience big data environment years experience working real world noisy data phd computer science math related quantitative field experience recommender systems information retrieval graph analysis statistical modeling neural network natural language processing experience large scale data analysis building high performance computational software experience e commerce analyzing clickstream data proficiency python sql statistics experienced software engineering practices providing solutions development implementation scaling execution validation monitoring improvement data science solutions ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "", "", "", "", "bs statistics computer science engineering operational research marketing business economics applied mathematics another quantitative field study experience google analytics google tag management experience performing advanced data transformations sql excel excited learn google bigquery advanced knowledge least one scripting language experience b testing collecting leveraging user event data analysis experience analyzing visualizing presenting data excellent oral written communication skills must able interact cross functionally technical non technical people ability handle large workload efficiently prioritize forward thinking dynamic innovative years applicable experience experience digital media experience using bi visualization software familiarity unix linux environment automating processes shell scripting familiarity r sas spss statistical modeling clustering classification machine learning data text mining", "ph math statistics operations research computer science econometrics quantitative field must least years actual working experience performing advanced quantitative analyses advanced hands working knowledge python sql required ability apply advanced statistical methodologies mixed model random fixed effects simultaneous equations arima neural networks multinomial discrete choice ability apply mathematical operations tasks cluster analytics sampling theory design experiments analysis variance correlation techniques factor analysis ability apply advanced optimization methodologies linear mixed integer optimization ability apply advanced simulation modeling methodologies techniques utilize complex computer operations intermediate programming 3rd 4th generation languages relational databases operating systems advanced features software packages word processing spreadsheet graphics etc experience aws data machine learning services plus must relational database experience strong passion empirical research answering hard questions data working knowledge sas r preferred working knowledge big data manipulation plus experience deploying models highly scalable production environment preferred experience aws data machine learning services plus", "bachelors degree quantitative discipline years analytics experience masters degree quantitative discipline years analytics experience hands experience applied machine learning either python r years using data mining methods clustering anomaly detection understand data patterns select appropriate predictive techniques proficient understanding relational sql e g oracle sql server postgresql nosql mongo neo4j databases data structures excellent communication skills able interact directly non technical client stakeholders act business technical translation role experience working onsite client technical consulting environment preferred experience working within agile scrum framework self motivated self managing proficient creating reasonable accurate time estimates assigned tasks masters degree phd experience modern natural language processing techniques embeddings deep learning nlp experience advanced deep learning methods experience deploying machine learning models aws", "", "ba bs computer science statistics healthcare informatics similar degree years applicable analytics data science experience deep knowledge machine learning techniques statistical modeling predictive modeling natural language processing demonstrated ability apply machine learning solve complex business problems proficiency python r programming extensive experience exploratory data analysis feature engineering data visualization experience sql relational databases ability communicate collaborate effectively technical non technical audiences high degree personal initiative strong problem solving skills masters degree computer science statistics healthcare informatics similar degree familiarity industry experience working healthcare data especially oncology population health experience model explanation interpretation methodologies lime shapley values strong data visualization skills", "strong data mining machine learning background experience big data environment years experience working real world noisy data phd computer science math related quantitative field experience recommender systems information retrieval graph analysis statistical modeling neural network natural language processing experience large scale data analysis building high performance computational software experience e commerce analyzing clickstream data proficiency python sql statistics experienced software engineering practices providing solutions development implementation scaling execution validation monitoring improvement data science solutions strong communication data presentation skills ability communicate data driven stories ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "li bsteward", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "pursuant san francisco fair chance ordinance consider employment qualified applicants arrest conviction records e verify company", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members", "", "", "", "fluent english", "", "development languages including python r javascript machine learning frameworks tensorflow keras data science algorithms decision trees linear regression clustering word embeddings cloud ml resources like google cloud platform net experience plus ocr experience plus experience implementing successful machine learning systems", "", "year experience developing machine learning models using deep learning methods classification regression experience python r scala java c c related languages comfort sql nosql db mapreduce apache spark kafka large scale data processing tools performing various tasks related machine learning data science activities including data cleanup data transformation data mashing algorithm parallelization experience caffe torch tensorflow theano matlab similar deep learning toolkit machine learning year python years aws year linux various flavors", "", "bachelor' degree concentration mathematics statistics computer science equivalent work experience master' plus must minimum years experience data scientist data geek - ' looking people love data comfortable working numbers patterns like solve puzzles free time ' right track detail oriented process driven- ' looking folks see data patterns quickly help create new process efficiencies improved knowledge variety machine learning statistical modeling techniques business setting ability choose best technique given problem even solution ' involve ml proficiency python r scripting languages well toolkits like pandas numpy etc experience writing production ready code plus experience gcp cloud platforms plus mindset research lead actionable results excited tell us want work kinds challenges looking talk intelligently passionately interesting challenges projects presented sense humor perspective preference given local candidates mass relocation offered position", "master computer science math related quantitative field bs ba computer science math related quantitative field years data mining experience work experience hadoop sas hbase cassandra similar development platforms expert knowledge large scale information retrieval statistical analysis years experience data mining statistical analysis previous work experience ecommerce experience large scale data analysis demonstrated ability identify key insights data solve business problems experience big data distributed computing platforms hadoop spark nosql databases strong knowledge standard machine learning techniques concepts years hand experience either industry academia scripting language python r general purpose programming language java scala ability build scalable systems analyze huge data sets make actionable recommendations strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members equal opportunity employer discriminate employee applicant employment race color sex age national origin religion sexual orientation gender identity status veteran basis disability federal state local protected class", "experience applying wide variety unsupervised semi supervised supervised machine learning techniques ability turn big data actionable intelligence familiarity network endpoint security concepts technologies ability analyze retrain improve machine learning models ability work part remote team ability provide receive scientific critiques work towards data driven solutions significant development experience python matlab r scala ability document explain technical details clearly concisely peer reviewed publications preferred minimum years experience data science data analytics required b computer science equivalent experience ph preferred strong written verbal communication skills experience sklearn pandas numpy similar packages familiarity malware host forensics network traffic analysis concepts experience linux command line bash scripting experience reverse engineering malware experience aws infrastructure experience deep learning frameworks tensorflow theano mxnet experience gpu accelerated computing hardware e g nvidia dgx experience using hadoop spark experience using relational non relational databases experience web frameworks visualize large datasets", "", "", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "", "provide consultation data science machine learning strong interests machine learning fast learner bachelor' degree necessary ' using rich customer insights advanced technology data science build cloud native insurtech solution person work closely growing inter disciplinary team physicians research scientists software developers challenge status quo . . . experience using statistics propensity model building algorithms manipulate data draw insights large data sets exciting experience data science' analytics' hottest topics hp' analytics team covering wide range exciting fast growing business. . . report directly triplebytes head machine learning work alongside team machine learning engineers data scientists use data understand business patterns trends analyze data understand business market trends order increase company revenue . . . experience data mining advanced data manipulation machine learning statistical analysis identifies meaningful insights large data metadata. . . data scientist advanced analytics responsible leveraging data analytics visualization advanced analytics data science help driving. . . possess end end data expertise including data sourcing consolidation integration manipulation visualization fluent highly adept sql python r years industry experience proven ability apply scientific methods solve real world problems web scale data prototype new ideas build existing systems collaborating data scientists product managers front end developers dedicated. . . past experience clinical science ai assisted diagnostics big plus years proven track record successfully applying ml predictive modeling . . . skilled data scientist machine learning image analysis statistical analysis experience machine learning years required", "", "", "work home wednesdays", "bs ba quantitative field math statistics computer sciences related field years professional experience data science advanced analytics experience business environment large scale complex datasets proficient sql experience efficient processing large data sets ability write sophisticated optimized queries large databases familiarity columnar databases like redshift understand database optimization experience math stats software - r sas python tableau - build clear actionable dashboards experience conveying key insights complex analysis summarized business terms graduate degree ms mba phd etc quantitative field highly valued required experience big data solutions well aws solutions business acumen - understand business drivers framework driven process analyzing business problems developing solutions communication - share insights way easy grasp actionable build relationships help drive adoption data insights driven decision making", "minimum years experience solving data science problems masters phd application data science solutions industrial projects domain expertise energy healthcare finance logistics", "masters phd degree quantitative discipline statistics computer science engineering math economics evidence exceptional ability related fields year plus experience quantitative analysis strong passion curiosity data data driven decision making solve complex business problems solid communication skill acute attention detail proficiency sql r python experience java tableau hive spark mongodb plus deep knowledge applied statistics including predictive modeling bayesian statistics time series analysis machine learning experience interest data visualization techniques ability convey complex analyses efficient intuitive visual methods", "", "experience aws cloud computing technologies e g emr etc", "proven ability solve business problems developing implementing machine learning algorithms statistical models must previous predictive analytics segmentation experience experience building web mobile interfaces displaying interactive visual insights demonstrated ability listen quickly learn innovate drive insights disparate data sets fluid environment must able work independently identify customer opportunities trends patterns ability prototype transform ideas actionable insights quickly curiosity ability work pressure creativity positive \" \" attitude must proven ability manage multiple priorities keeping \"team first\" perspective demonstrated ability assimilating real life experiences objective data accelerates learning behavio responsible ensuring security availability confidentiality privacy policies andcontrols adhered masters degree equivalent experience maths statistics computer science - years experience creating analyzing interpreting presenting complex data extensive experience d3 another front end js data visualization library experience analyzing data using python r another programming language strong skillsin machine learning data text mining advanced statistical analysis model evolution validation testing experience relational sql nosql databases salesforce ticketing systems plus experience working agile environment", "phd computational quantitative discipline e g statistics computer science biomedical informatics genetics physics epidemiology health economics master' degree similar field study deep understanding ml including strong knowledge mathematical underpinnings behind various methods e g regression techniques neural networks decision trees clustering pattern recognition dimensionality reduction proven experience applied statistics ml business setting deep understanding tools trade including variety modern programming languages r python javascript open source technologies linux tensorflow hadoop spark experience effective data visualization approaches keen eye detail visual communication findings comfort working communicating non technical teams translate business questions analytically actionable questions strong desire build meaningful solutions life sciences business task oriented ability set goals complete deliverables domain knowledge clinical data real world data life sciences related research data expertise data science related tools e g sql tableau d3", "bachelor' degree quantitative field statistics computer science economics mathematics equivalent work experience years experience business intelligence statistical modeling data collection aggregation analysis year experience machine learning experience designing implementing machine learning solutions plus demonstrated experience following technologies r python programming sql server mysql postgresql self driven passion finding collaboratively solving problems", "bachelor degree year work experience advanced mathematical statistical engineering physics related quantitative field actuarial credential master degree advanced mathematical statistical engineering physics related quantitative field without experience years experience advanced mathematical statistical engineering physics related quantitative field learning growth mindset customer focused interpersonal verbal written communication skills experience least three following six areas data analysis relational style query languages machine learning statistical modeling data visualization high level programming language distributed computing understanding healthcare masters ph quantitative field bachelors degree significant healthcare experience", "", "phd computer science computer engineering ms years experience related field demonstrated history driving delivering analytics models solutions deep knowledge fundamentals machine learning data mining statistical predictive modeling extensive experience applying methods real world problems strong skills software prototyping engineering expertise applicable programming analytics languages python r c c various open source machine learning analytics packages generate deliverable modules prototype demonstrations work desired interdisciplinary skills include big data technologies etl statistics causal inference deep learning modeling simulation breadth skills experience machine learning - diverse types data diverse data sources different types learning models diverse learning settings ability inclination work multi disciplinary environments desire see ideas realized practice experience knowledge services domains business process outsourcing systems transportation systems healthcare systems financial services valued demonstrated ability propose novel solutions problems performing experiments show feasibility solutions working refine solutions real world context prior experience similar role required please include requirements appropriate must currently eligible work us employer without sponsorship", "bachelor degree enrolled graduate program study full time student gpa least point scale expertise working data using combination mathematics computation visualization interaction experience thesis research internships work experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research physics quantitative finance statistics availability work least one day tour prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment thorough medical psychological exam polygraph interview comprehensive background investigation", "", "qualifications listed minimum acceptable considered position salary offers based candidates education level years experience relevant position also take account information provided hiring manager organization regarding work level position qualifications listed minimum acceptable considered position salary offers based candidates education level years experience relevant position also take account information provided hiring manager organization regarding work level position qualifications listed minimum acceptable considered position salary offers based candidates education level years experience relevant position also take account information provided hiring manager organization regarding work level position qualifications listed minimum acceptable considered position salary offers based candidates education level years experience relevant position also take account information provided hiring manager organization regarding work level position", "", "full time student pursuing bachelor degree technical field gpa least point scale availability work two day tours prior graduation attending school full time basis following internship creativity initiative integrity leadership abilities problem solving skills ability work diverse team environment interest experience science technology engineering mathematics stem related field computational social science computer science data analytics economics engineering geospatial analysis mathematics operations research quantitative finance statistics thorough medical psychological exam polygraph interview comprehensive background investigation", "", "", "", "strong programming skills e g python r javascript proficiency writing sql queries ability desire present complex findings simple approachable way non technical audiences e g writing reporting tools person presentations experience cleaning structuring transforming data via etl processes ability design deploy machine learning algorithms models", "", "", "us citizenship required offer sponsorships years experience data scientist machine learning engineer similar role understanding structure theory common machine learning models familiarity common machine learning libraries implementation sklearn weka tensorflow torch . . . proficiency one python scala java r julia matlab expertise subfield machine learning computer vision natural language processing ability understand implement new models literature project team leadership experience presenting team progress results technical writing experience including reports proposals experience distributed analytics processing spark hadoop . . . experience geospatial data analytics active ts security clearance familiarity agile development", "experience google cloud platform bigquery compute engine data flow experience relational sql nosql databases experience developing products deployed production experience developing search recommendation visual language applications design develop algorithms models use large datasets create business insights establish scalable efficient processes large scale data analyses model development model implementation present analysis resulting recommendations senior management leverage data present compelling business case optimize investments operations communicate educate technical non technical employees analytics data driven decision making position reports director data science sr manager data science position direct reports located comfortable indoor area unpleasant conditions would infrequent objectionable typically requires overnight travel less time must eighteen years age older must legally permitted work united states knowledge skills abilities typically acquired completion master degree program equivalent degree field study related job time spent sitting comfortable position frequent opportunity move rare occasions may need move lift light articles ability build scalable systems analyze huge data sets make actionable recommendations strong communication data presentation skills ability quickly adapt new technologies tools techniques flexible responsive able perform fast paced dynamic work environment meet aggressive deadlines ability work technical non technical team members", "", "", "", "extensive experience solving analytical problems using quantitative approaches comfort manipulating analyzing complex high volume high dimensionality data varying sources strong passion empirical research answering hard questions data flexible analytic approach allows results varying levels precision ability communicate complex quantitative analysis clear precise actionable manner familiarity relational databases sql expert knowledge analysis tool r matlab sas strong working knowledge financials acquired skills would include data warehousing etl bi data mining machine learning strong written verbal skills - able explain work plain language", "", "phd computer science computer engineering ms years experience related field demonstrated history driving delivering analytics models solutions deep knowledge fundamentals machine learning data mining statistical predictive modeling extensive experience applying methods real world problems strong skills software prototyping engineering expertise applicable programming analytics languages python r c c various open source machine learning analytics packages generate deliverable modules prototype demonstrations work desired interdisciplinary skills include big data technologies etl statistics causal inference deep learning modeling simulation breadth skills experience machine learning - diverse types data diverse data sources different types learning models diverse learning settings ability inclination work multi disciplinary environments desire see ideas realized practice experience knowledge services domains business process outsourcing systems transportation systems healthcare systems financial services valued demonstrated ability propose novel solutions problems performing experiments show feasibility solutions working refine solutions real world context prior experience similar role required please include requirements appropriate must currently eligible work us employer without sponsorship", "", "", "", "", "", "", "", "", "", "", "ping pong scooters foosball beautiful trails nearby name", "", "", "masters degree computer science related field experience machine learning text analysis nlp experience algorithm design modeling experience designing working ontologies programming skills java python similar language", "", "", "", "", "experience end end etl work using python sql strong systems design knowledge know architect data pipelines storage compute fits together prior experience making large datasets accessible familiarity hive presto parquet plus experience compute frameworks job orchestration systems experience google cloud platform plus years industry experience health insurance premium covered dependent children flexible vacation paid time weeks paid family leave equity plan employees retirement benefits employer match fertility adoption benefits free lunch snacks offices education reimbursement dog friendly workplace new york office commuter benefit form reduced tax ireland pretax us", "", "", "bachelor' degree computer science software engineering related fields years experience data engineering years experience working python nodejs years experience working kafka based pipeline development strong knowledge sql required year hands experience apache airflow experience distributed computing using hadoop ecosystem spark presto knowledge java scala knowledge kafka connect ecosystem", "", "must minimum years cyber security engineering experience include application cyber security methodologies enterprise environment must experience following security frameworks nist 171r2 candidates must ability obtain maintain dod secret level security clearance condition continued employment preferred qualifications certifications degrees ideal candidate master degree stem related discipline years experience cyber security engineering professional technical certifications cissp cism ccsp security plus aws certified security specialty aws certified solutions architect azure security engineer azure solutions architect dod iat iam level ii iii certificationsecurity skills senior level cyber security engineering architecture experience experience interpreting implementing security compliance standards guidance including governance risk compliance grc policies procedures nist security control framework experience areas system security network application security knowledge current emerging cyber security threats vulnerabilities controls contributor architectural industry changes area cyber securitytechnical skills experience evaluating designing configuring implementing cloud services models saas paas iaas experience linux windows operating systems experience operating agile devops environment experience scripting knowledge application program interface api ability manipulate api integrate different toolsets advanced knowledge cybersecurity principles networking architecture servers systems design virtual hosts configuration management identity access management encryption intrusion detection systems ids intrusion prevention systems ips experience supporting deployment configuring managing maintaining technologies directory services centralized authentication active directory red hat identity manager vulnerability scanning management databases operating systems web applications ids ips anti malware tools technologiesprocess skills experience agile scrum application lifecycle management alm experience operating agile devops environmentsoft skills exceptional verbal written communications quickly learn adapt new changing business technical concepts requirements skills tools goal oriented team player committed quality detail proven track record driving decisions collaboratively resolving conflicts ensuring follow innovative strategic thinker positive proactive readily embraces change", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "experience data warehouses lakes big query redshift snowflake nosql cassandra redis stream processing engines dataflow flink workflow management tools airflow pachyderm big data solutions highly appreciated", "", "", "", "", "", "great perks vary location include employee discounts transportation reimbursements subsidized cafes fitness facilities conveniences dry cleaning car washes recycling programs", "knowledge relevant engineering best practices data management fundamentals data storage principles current recent advances distributed systems pertains data storage computing years experience designing building maintaining data architecture infrastructure relational non relational years maintaining data warehouse systems working large scale data transformation using sql hadoop hive big data technologies experience etl tools plus years data modeling experience able use data models improve performance software services experience cloud based solution aws redshift gcp big query programming language python java plus experience communicating colleagues engineering analytics business backgrounds degree engineering math statistics computer science related discipline equivalent experience plus able legally work europe holder eu passport holder eu residency permit holder schengen work visa", "education b sc computer science related fields knowledge experience years deployment integration server management well experience writing scripts using python bash advanced knowledge sql non sql databases strong analytic skills related working unstructured datasets strong knowledge manipulating processing extracting value large disconnected datasets experience big data tools hadoop spark kafka etc experience data pipeline workflow management tools azkaban luigi airflow etc experience stream processing systems storm spark streaming etc experience object oriented object function scripting languages python java c scala etc experience aws cloud services ec2 emr rds redshift", "lead implementation teams concept completion leveraging best practices big data participate analysis architecture design data hub builds robust big data solution systems eye long term maintenance support application work part team design develop code scripts data pipelines leverage structured unstructured data integrated multiple sources looks leverage reusable code modules solve problems across team including data preparation transformation data export synchronization design develop automated test cases verify solution feasibility interoperability include performance assessments act big data delivery liaison infrastructure security application development testing team helps drive cross team design development via technical leadership mentoring keep current latest big data technologies products including hands evaluations depth research consult advise solution architects overall enterprise wide analytics solutions include data hub component works project manager perform detailed planning risks issues escalation requirements big data engineer healthcare analytics senior years experience working batch processing tools hadoop tech stack e g mapreduce yarn pig hive hdfs oozie years experience working tools stream processing tech stack e g spark storm samza kafka avro experience developing applications work nosql stores e g elasticsearch hbase cassandra mongodb couchdb experience developing tb level data stores 10gbps ingest speeds high capacity data ingest hadoop spark highly desired hands experience least one major hadoop distribution cloudera horton works mapr ibm big insights system usage optimization tools splunk plus least years experience delivering enterprise solutions solutions architect years experience sql least two major rdbms years systems integrator linux systems shell scripting years data related benchmarking performance analysis tuning years java experience solid programming experience preference towards java python dba data modeling experience experience operational business level metadata management bachelor degree computer science information systems information technology related field years software development dw bi experience health care experience plus excellent verbal written communication skills hands experience cloudera higher horton works higher mapr higher experience map reduce solution design development etl solution experience preferable hadoop experience industry leading business intelligence qualifications", "offer ability work newest tech stack talented engineers iot industry", "", "fluency english written spoken", "", "", "", "roughly years industry experience data machine learning team proficiency modern programming languages go python java scala etc sql practical experience probability statistical modeling machine learning backend developer experience optimizing data access layer mature web applications experience building working real time compute streaming infrastructures kafka kinesis flink storm beam etc experience writing debugging etl jobs using distributed data framework deep abiding appreciation agile software processes data driven development reliability responsible experimentation collaborative attitude helpful personality health dental vision life insurance 401k matching program commuter benefits catered lunch unlimited snacks unlimited reimbursement work related books", "", "aware challenges associated building ml datasets computer vision models definition coverage target data distribution bias creativity good sense product enable intuition model expected work eg face detector model could potentially fail eg low light pale dark skin etc data used test failure patterns strong python coding skills enable manipulate data scale run ml models come data driven solutions facing situations trade offs decision uncertainty", "experience search technologies solr elasticsearch lucene", "", "", "", "bs computer science systems engineering similar technical field relevant work experience understanding data model design database schemas optimizing database applications breadth understanding database technologies including relational non relational solutions experience manipulating large data sets time series intermittent data experience using version control software experience designing developing database access layers schemas contain multiple databases containing unique data types access requirements experience working python primary development language emphasis data management processing experience mysql mongodb experience producing software clinical setting utilizes clinical patient data e g labs physiologic signals administrative data understanding clinical data driven research data aggregation methodologies standpoint study design subject protections statistical analysis experience agile software development methodologies continuous integration delivery", "minimum four years experience planning design implementation security solutions including minimum two years experience leading design implementation troubleshooting operation security technologies minimum two years technical leadership role without direct reports bachelor degree computer science cis related field minimum eight years experience operations environment technical experience distributed technologies systems development networking additional equivalent work experience may substituted degree requirement preferred qualifications three years experience building technology solutions meet corporate industry regulatory requirements three years experience design implementation complex data infrastructure solutions three years experience infrastructure consulting primary location california pleasanton pleasanton tech cntr building f owens dr scheduled weekly hours shift day workdays mon tue wed thu fri working hours start working hours end pm job schedule full time job type standard employee status regular employee group union affiliation salaried non union exempt job level individual contributor job category information technology specialty eng infrastructure department dcss a2o travel yes time", "", "gym membership compensation", "bs ms degree computer science math statistics technical field years applied software engineering experience especially startups big data python years team lead managerial role python expertise classes inheritance generators decorators docstrings pylint pytest etc numpy pandas experience preferred sql hive expertise clauses joins group bys windowing functions exploding spark expertise sparksql pyspark caching checkpointing dataframes rdds expertise building monitoring maintaining reliable etl pipelines ability write well abstracted extensible object oriented code components enjoy working fast paced highly collaborative ambitious startup work environment basic understanding probability statistics experience evaluating data quality scale experience amazon web services rds s3 ec2 emr data pipeline pycharm github jira equivalents experience open source search platforms solr elasticsearch like experience unix osx cli bash background data wrangling various structured unstructured data sets consuming apis e g rate limiting exponential back offs like knowledge graph storage computation frameworks e g graphx titandb neo4j familiarity scala java apache spark internals job optimization significant interest background big data politics advertising finance technology experience working directly data scientists basic knowledge common machine learning techniques experience agile development similar methodologies continuous development product technology", "", "developer tools git sdlc oop etc experience required", "experience advanced courses data science machine learning", "strong communication skills including ability identify communicate data driven insight", "experience business intelligence analytics equivalent analyst position experience sql additional object oriented programming language e g python java high level expertise data modeling effective problem solving analytical skills ability manage multiple projects report simultaneously across different stakeholders structured thinking ability easily break ambiguous problems propose impactful data modeling designs attention detail effective verbal written communication skills bachelor' degree engineering computer science statistics economics mathematics finance related quantitative field equivalent practical experience years experience consulting business intelligence analytics equivalent analyst position experience sql python", "gym membership compensation", "", "", "knowledge digital adtech landscape", "", "", "", "creative analytic problem solver diligent attention detail", "", "demonstrated fluency modern programming languages data science covering wide gamut data storage engineering frameworks machine learning libraries", "", "", "experience cloudera", "", "experience one following languages functional programming general scala haskell java javascript experience one following technologies distributed logging systems kafka pulsar kinesis etc stream processing flink spark storm beam etc batch processing spark hadoop . . . idl avro protobuf thrift mpp databases redshift vertica . . . query execution columnar storage push downs hive presto parquet workflow management airflow oozie azkaban cloud storage s3 gcs distributed logging systems kafka pulsar kinesis etc stream processing flink spark storm beam etc batch processing spark hadoop . . . idl avro protobuf thrift mpp databases redshift vertica . . . query execution columnar storage push downs hive presto parquet workflow management airflow oozie azkaban cloud storage s3 gcs understanding distributed systems concepts principles consistency availability liveness safety durability reliability fault tolerance consensus algorithms eager learn new things passionate technology experience contributing open source software experience following cassandra dynamodb rocksdb leveldb graphite statsd collectd wework wework technology bridging gap physical digital platforms providing delightful flawless powerful experience members employees build software hardware enables members connect space around like never augment community culture teams tools build believe ' macro shift toward new way working--one focused movement towards meaning purpose wework technology proud shaping movement team passionate fearless collaborative problem solvers distributed globally one goal mind humanize technology across world equal opportunity employer value diversity company discriminate basis race religion color national origin gender sexual orientation age marital status veteran status disability status", "", "computer science engineering bioinformatics master level plus years relevant experience excellent programming skills python c r experience designing implementing restful apis webservices ability interact various data sources structured unstructured e g hdfs sql nosql experience working across multiple scientific compute environments create data workflows pipelines e g hpc cloud unix linux systems expertise biological health data experience modelling data information graph network representation experience working metadata models controlled vocabularies ontologies ability understand map integrate document complex data relationship business rules familiarity data quality cleaning masking techniques modern frameworks concepts scalable distributed computation containerization orchestration e g k8s specialized frameworks spark hadoop experience image processing computer graphics experience cloud computing", "", "bachelor degree experience designing developing medium large data environments associated services e g data pipelines data cataloging enterprise applications analytics projects proven experience fusing mining preparing data sets using sql oracle elastic technologies experience performing machine learning ml functions data preparation automation data services e g data quality measures utilization mapping etc familiarity hybrid cloud infrastructures experience python java relevant scripting languages", "", "", "always interviews screening call conducted via video call", "", "", "", "bachelor' degree math statistics computer science finance equivalent experience years experience data engineer bi engineer business financial analyst systems analyst sql writing experience experience etl expert understanding best practices handle extremely large volume data ability create extensible scalable data schema lay foundation downstream analysis clear passion learning new bi skills techniques independently continuously ability prioritize multiple concurrent projects still delivering timely accurate results experience working lean successful start new product team continuous innovation desired ambiguity norm experience mentoring others sql modeling forecasting use large datasets proficiency scripting languages unix systems python perl bash etc experience following plus looker tableau microstrategy", "", "", "", "willingness travel", "", "python django flask", "gym membership compensation", "", "gym membership compensation", "", "responsible staying current enterprise standards industry standards technologies methodologies best practices", "", "", "", "data engineer previous experience business intelligence data warehousing know work high volume heterogeneous data preferably distributed systems kafka spark mpp databases snowflake comfortable building deploying applications public clouds particular aws cloud know write distributed high volume services golang scala java hands experience large number technologies programming languages allows choose right tool job afraid contributing systems across whole findhotel organisation knowledgeable data modeling data access data storage techniques appreciate agile software processes data driven development reliability responsible experimentation strive excellence clarity transparency shipping business value early possible building incrementally afterwards remote candidates based time zone utc utc experience e commerce clickstream data event tracking experience interest data analysis continue hire grow look survived corona crisis also thrived year helping 1m customers around world find better hotel deals using data transparency industry leading features fast growth mode growing bookings year year past years plan continue coming years beginnings organisation excelled user acquisition grown engineering product driven organisation independence maturity achieved terms user acquisition engineering product enables us disruptive build customer features nobody else industry plenty chances learn grow - surrounded brightest minds city part culture values sharing knowledge every day budget attend conferences develop profitable company fast growth great scale opportunity competitive compensation package perks benefits including stock appreciation rights flexible time take many holidays need chance work remotely measure results time spent office part highly international team fun work environment value good food offer catered lunches various cuisines great coffee ice cream fridge occasional bbq garden survive refocus thrive travel startup corona times hiring process", "", "final meeting engineering leadership via video person hour", "", "", "", "", "gym membership compensation", "", "", "", "", "", "competitive compensation visa relocation support needed", "offer ability work newest tech stack talented engineers iot industry", "", "willingness travel", "due business requirements successful candidate must based ireland candidates based eu willing relocate ireland also considered python experience years years software development experience good command linux front end development experience required creating supporting internal tools back end development experience required creating supporting internal tools python web frameworks like twisted aiohttp django flask databases understanding web technologies javascript html css http strong analytics skills related working unstructured datasets excellent written english strong web crawling web scraping skills scrapy knowledge browser automation experience splash experience plus experience handling mid size large datasets organizing parallel processing good spoken english strong record open source activity", "", "", "", "", "", "", "pension plans employee benefits", "", "", "weekly catered lunches stocked kitchen full fruit energy bars popcorn coconut water healthy snacks", "gym membership compensation", "", "significant competence sql", "understanding data visualization tools", "", "", "", "", "minimum years experience data engineering extensive knowledge different programming scripting languages java c python php ruby perl scala bash etc proficient sql solid understanding relational nosql database technologies experience designing building maintaining end end data pipelines etl infrastructure using tools hadoop spark kafka samza amazon kinesis etc experience aws strong familiarity working linux environment excellent critical thinking problem solving analytical skills must authorized work united states gaming industry experience experience redshift machine learning experience", "", "strong understanding security including threat propagation malware analysis", "", "", "advanced working sql knowledge experience working relational databases query authoring sql well working familiarity variety databases experience building optimizing 'big data' data pipelines architectures data sets experience performing root cause analysis internal external data processes answer specific business questions identify opportunities improvement strong analytic skills related working unstructured datasets build processes supporting data transformation data structures metadata dependency workload management successful history manipulating processing extracting value large disconnected datasets working knowledge message queuing stream processing highly scalable 'big data' data stores strong project management organizational skills experience supporting working cross functional teams dynamic environment experience big data tools hadoop spark kafka etc experience relational sql nosql databases including postgres cassandra experience data pipeline workflow management tools azkaban luigi airflow etc experience aws cloud services ec2 emr rds redshift experience stream processing systems storm spark streaming etc", "", "", "", "knowledge commonly used third party analytics tools like periscope tableau segment heap", "", "design develop applications data processing one languages python java scala create chains data ingestion relational dbs nosql storage utilize unix linux os write data preprocessing scripts develop data processing systems hadoop spark storm impala etc", "", "bachelor degree computer science engineering technical science years experience programming building large scale data analytics solutions operating production environments minimum years expertise designing implementing large scale data pipelines data curation feature engineering machine learning using spark combination pyspark java scala python either premise cloud aws google azure minimum year designing building performant data tiers refactoring existing ones supports scaled ai analytics using different cloud native data stores aws azure google redshift s3 big query sqldw etc well using nosql graph stores minimum year designing building streaming data ingestion analysis processing pipelines using kafka kafka streams spark streaming similar cloud native technologies minimum year designing building secured governed big data etl pipelines using talend informatica technologies data curation analysis large production deployed solutions experience implementing smart data preparation tools palate trifacta tamr enhancing analytics solutions minimum year building business data catalogs data marketplaces powering business analytics using technologies alation collibra informatica custom solutions", "", "sense humor must ash mustache. . . get mustache", "", "", "", "curiosity determination understand improve data flows", "python sql relational databases e g postgres mysql distributed computation e g spark hive athena cloud infrastructure use aws general application e g web api development git collaboration", "", "", "", "", "", "", "", "", "", "", "", "", "pet friendly office environment", "", "", "", "", "", "", "fixed term contract option perm", "", "components frameworks including hadoop hdfs spark storm hbase pig hive scala kafka pyscripts unix shell scripts", "", "", "experience agile methodologies scrum kanban", "", "strong preference someone experienced python rather java", "", "", "", "gym membership compensation", "", "bachelor' degree computer science information technology quantitative field years relevant experience ideally scientific computing environment ability design informative accessible technical training materials proven deep technical background including linux administration ability work demanding environment minimal supervision strong track record implementing cloud based processing techniques administration experience azure aws google cloud one major iaas providers google cloud platform preferred passion data science including knowledge least one high level programming language python r preferred familiarity database usage basic administration tasks postgresql preferred experience leveraging established distributed computing frameworks apache spark hadoop experience handling large volumes spatiotemporal data", "", "", "", "", "", "master' degree areas like mathematics physics computer science engineering econometrics business analytics information management minimum years hands relevant big data technologies implementations experienced distributed computing distributed storage containerisation spark pyspark kafka sql nosql e g elastic hive bigquery kafka ktables aws s3 docker kubernetes experienced programming python scala familiar micro services development rest api' iot fluent english fluent dutch strong preference mandatory good business communication skills able transform business requirements use cases flexible transport arrangement suits personal situation electric car bicycle flexible budget including ns business card", "", "", "", "minimum years experience programming language scala r python java experience writing reusable efficient code automate analyses data processes minimum years experience processing large amounts structured unstructured data cluster computing environment similar experience academia interested candidates must submit resume cv www nbcunicareers com considered must willing work new york ny experience formulating opinions constructing data processing systems good knowledge principles systems scale using big data technologies like spark hive impala hadoop databricks good understanding aws azure cloud technologies including aws services athena glue s3 lambda experience open source enterprise software experience building maintaining production data pipelines familiarity relational databases sql team oriented collaborative approach demonstrated aptitude willingness learn new methods tools ability communicate insights findings data visualization tools tableau domo shiny ability work effectively across functions disciplines levels experience media entertainment industry plus experience television ratings digital measurement tools nielsen rentrak comscore omniture etc familiarity nosql graph databases experience large scale video assets experience computer vision metadata generation video master' degree specialization computer science engineering physics quantitative field equivalent", "", "", "extensive experience software development expertise architecting delivering new technologies product features scale highly reliable cloud services experience developing scalable saas monitoring automation logging solutions highly reliable service offerings prior technical paper publications public speaking engagements extensive software development experience one following c java c python experience across windows linux plus strong algorithmic problem solving skills distributed systems experience ability see present big picture offer solutions make better extraordinarily intelligent rigorous thinker operate successfully among bright charismatic people strong customer facing relationship building skills effective working independently team setting ability uncover business challenges develop custom solutions solve challenges years work experience technology industry", "experience large scale data query optimization techniques experience etl data warehouse systems experience aws cloud services ec2 rds redshift auroraexpert sql nosql rdbms knowledge multiple scripting languages e g python knowledge cloud distributed systems stream processing systems passionate learning new technologies solving hard problems fast paced environment computer science degree years experience enterprise saas environments student game thrives new challenges enjoys learning teammates afraid teach others time sees glass half full new industry space vision could make difference wants make lasting impact lifelong connections another paycheck", "ci cd drone gitlab ci", "", "", "", "tuition reimbursement learning development programs", "professional development program", "", "knowledge tableau qliksense similar technologies", "strong communication skills ability present deep technical findings business audience", "", "", "bachelor degree equivalent experience", "roughly years industry experience data machine learning team proficiency modern programming languages go python java scala etc sql practical experience probability statistical modeling machine learning backend developer experience optimizing data access layer mature web applications experience building working real time compute streaming infrastructures kafka kinesis flink storm beam etc experience writing debugging etl jobs using distributed data framework deep abiding appreciation agile software processes data driven development reliability responsible experimentation collaborative attitude helpful personality health dental vision life insurance 401k matching program commuter benefits catered lunch unlimited snacks unlimited reimbursement work related books", "aws docker kubernetes terraform vault", "high quality swag", "", "", "", "", "gleaming new square foot headquarters complete foot climbing wall showers lockers bike parking", "", "proficient english read write proficiently speak conversational level english", "", "", "", "", "", "gym membership compensation", "", "", "", "", "relocation simplified -- paid accommodation well experienced \"relocation buddies \" guide visa application", "always improve value personal progress want look back proudly ' done", "", "", "transparency - regular team meetings stay know ' going areas business", "", "", "bachelor degree computer science related field experience relational databases sql map reduce languages pig hive understanding different data storage engines work limitations sql nosql key value stores knowledge java c c go node js experience druid time series based data storage solution deep knowledge building high performance high availability distributed systems experience kafka spark cassandra extensive experience working big data designing etl pipelines end end expert one rdms familiarity postgresql redshift knowledge ad serving platforms online advertising systems experience game development", "", "excellent medical dental vision benefits", "total years preferably dw etl bi projects experience gathering end user requirements writing technical documentation time management multitasking skills effectively meet deadlines basic understanding data management concepts 3nf dimensional specific applications sql pl sql experience analyzing transforming integrating data preferably one database technologies oracle mssql db2 teradata basic understanding dwh related terms example procedures functions triggers views indexes etc ability analyze data various sources find meaningful insights well identify gaps inconsistencies client facing team player skills good command written spoken english willingness develop competences area data analytics availability mobility work projects poland across europe previous experience etl tools example odi sas di ibm datastage informatica ms ssis experience agile development methodologies access leading edge cloud technologies career competence support including ongoing mentoring financing certificates different technologies tools full work comfort private healthcare additionally life insurance sport packages opportunity work global top clients innovative large international projects european level", "", "", "", "knowledge experience financial markets banking exchanges", "", "", "http www showtimeanytime com", "", "http www showtimeanytime com", "", "", "exceptionally detail oriented", "", "deep understanding distributed computing frameworks spark particularly sparkml sparksql tune optimize debug spark jobs hadoop flink experience big data aws particular using emr s3 experience docker container orchestration like kubernetes swarm similar experience pipeline management tools like airflow luigi nifi experience programming languages python go scala good knowledge sql rdbms experience command line shell scripting version control git excellent communication skills english oral written german nice preferably experience automatic configuration management like terraform puppet preferably experience modern agile software development practices like microservices test driven development pair programming ci cd etc training opportunities individual management specialist career provision necessary equipment need deliver top performances employees self reliant free schedule work day onboarding program including welcome day trainings individual incorporation chance work together goals also working create something new motivated colleagues cool office loft trendy district kreuzberg", "", "", "", "", "", "", "strong programming skills c java scala python deep understanding basic data structures algorithms experience scaling data platforms hundreds terabytes petabytes using spark hadoop familiarity modern machine learning techniques creative collaborative product focused", "", "", "", "", "", "minimum years development experience experience data analytics business intelligence data science proficient sql strong programming skills experience designing building restful api services strong experience etl must able interact various data sources connecting data warehouse must technically proficient able problem solve data extracted transformed loaded warehouse role work collaboratively business departments supports must able understand requirements data analytics group requirements technologically executed bachelor degree related field study experience amazon aws azure powerbi preferred required big data tools - mapreduce hadoop spark communication protocols json xml amazon redshift snowflakebottom form", "little travel", "offering annual salary range usd", "", "", "", "roughly years industry experience data machine learning team proficiency modern programming languages go python java scala etc sql practical experience probability statistical modeling machine learning backend developer experience optimizing data access layer mature web applications experience building working real time compute streaming infrastructures kafka kinesis flink storm beam etc experience writing debugging etl jobs using distributed data framework deep abiding appreciation agile software processes data driven development reliability responsible experimentation collaborative attitude helpful personality health dental vision life insurance 401k matching program commuter benefits catered lunch unlimited snacks unlimited reimbursement work related books", "", "", "experience micro services kubernetes", "strong clevertech community", "", "experience streaming data", "provide report predicting future data based existing information u sql new big data query language azure data lake analytics service experience providing technical leadership mentor engineers best practices data engineering space able use data frame solve problems comfortable algorithms data structures dynamic array linked list stack queue binary. . . experience ingesting external data sources existing platform create error reporting install metrics add data quality measures follow best practices security standards data including personally identifiable data data engineer builds robust fault tolerant data. . . advanced knowledge application data infrastructure architecture disciplines bs ba degree equivalent experience data engineer designs builds platforms tools solutions help bank manage secure generate value data designs data solutions data distributions verification data modeling data mining automation data data platforms tools improve data usability data quality data warehouse experience dimensional data modeling schema design database data warehouse around years experience working data engineer data warhousing projects managing petabytes data data engineering years preferred data vault data architecture identify anomalies inconsistencies data large scale data flows kafka strong written verbal communication skills location de position would remote start sit onsite delaware post pandemic experience building optimizing big data data pipelines architectures data sets optimize maintain data pipelines architecture data. . . work data engineers data scientists drive efficient solutions platform help define data story enable data driven solutions . . .", "", "gym membership compensation", "", "", "company events happy hours bowling bocce league etc", "", "bs ms degree computer science engineering related field experience building processes extract process add value data sets multiple source systems experiencing data modeling tuning relational well nosql datastores oracle red shift impala hdfs hive athena etc experience working distributed computing tools spark hive etc experience aws cloud services ec2 emr rds redshift s3 lambda experience data pipeline workflow management tools airflow etc experience one general purpose programming languages including limited java scala c c c swift objective c python javascript experience working agile development methodologies sprint scrum", "", "expert level sql skills years experience database technologies e postgres mysql sql server oracle redshift etc minimum years experience data warehousing working knowledge dimensional modeling techniques working knowledge data quality approaches techniques experience redshift highly desired experience aws tools s3 redshift dynamodb iam highly desired experience working standard etl tool e informatica ssis talend pentaho etc architectural insight store data modeling experience recommend structured make accessible performant resilient change entrepreneurial spirit drive ship quickly familiarity agile software development practices ability deal ambiguity communicate well partner teams - technical non technical strong empathy customer experience experience working linux plus programming language experience python java etc plus api development experience plus working agile scrum development process plus", "masters degree years experience aws", "work awesome companies around world partner great software companies world constantly get interact people great companies", "", "", "working knowledge data integration data movement global database administration traffic shadowing test driven development software development life cycle big data dimensional design leveraging star snowflake schema sql server integration services ssis sql server reporting services ssrs sql server analysis services ssas reporting services python r tableau programming languages analysis tools preferred required knowledge agile methodology frameworks like scrum kanban xp build execute big data strategies systems platforms addressing capture data data storage data analysis search sharing transfer visualization querying updating information privacy data source cluster enterprise relational document oriented databases restful services traffic mirroring integrate database strategies data larger 1tb using relational databases especially ms sql server database administrator database administrator", "", "years field experience implementing etl data integration enterprise environment bachelor degree computer science data sciences information sciences software engineering similar field required commitment full time hours week travel basic expectation travel within north america approx candidates required pass background screen including criminal history reference check bankruptcy drug screen", "independent worker able manage project timelines priorities advanced analytical thinking problem solving skills strong communication skills ability compile present information good interpersonal skills demonstrated ability work within team environment experience working international organisation multicultural environment added advantage understanding wfp' core mission values language skills fluent english spoken written", "bachelor degree computer science software engineering mis equivalent combination education experience development experience building etl graphs using ab initio gde eme co operating system strong sql development skills development experience least two different programming languages python java scala etc development experience unix tools shell scripts development experience least two different database platforms teradata oracle mysql ms sql etc minimum years experience designing developing testing software aligned defined requirements experience tuning sql queries ensure performance reliability competitive salary performance based bonus opportunities single family health insurance plans including dental coverage short term long term disability matching k competitive paid time training certification opportunities eligible expense reimbursement team building social activities", "", "", "years technologies experience tech lead experience plus bachelor computer science related field equivalent experience years experience spark scala java python plus years experience working hadoop sql sql platforms mongodb plus years experience various data types avro json parquet plus experience development management manipulation large complex datasets demonstrated knowledge data management competencies implementation", "", "fan working agile scrum environments", "bs ms degree experience ph degree years experience drug development setting years biologics downstream process development experience using fplc system e g akta expertise method protocol development scale biologics purification including limited affinity cation exchange cex anion exchange aex viral filtration tangential flow filtration tff required familiarity hplc based analytical methods sec rp hplc ion exchange hydrophobic interaction chromatography etc prior experience ind regulatory requirements gmp environments quality documentation highly desired demonstrated ability work independently minimal supervision familiarity design experiment doe comfortable working analytical tools create high quality unbiased analyses prior experience successful technology process transfers third party vendors cro cdmo cmo highly desired experience mentoring coaching training junior staff members preferred excellent interpersonal skills including clear succinct timely communication proven ability foster strong relationships team members stakeholders highly self motivated detail oriented proven ability work dynamic fast moving team comfortable working data driven team collaborating research scientists data scientists platform engineers familiarity experience scripting statistical analysis data", "", "", "", "", "bachelor master degree life sciences computer science engineering experience software engineering development strong learning agility ability pick new technologies used support commercialization data analysis needs experience planisware strong experience working agile methodology devops jenkins jira github frameworks successful experience working collaborative team environment experience working container technologies e g docker developing microservices proficiency software development languages including limited java c experience cloud aws databricks platforms hpc high performance computing environments experience developing supporting web applications including familiarity web technologies frameworks extjs d3 js react js data analysis reporting experience using analytics visualization database technologies oracle pl sql spotfire tableau python numpy scipy pandas expert r scripting r development r shiny experience processing analyzing large ngs data expertise translating business requirements technical requirements recommend solutions working leading agile development methodologies sprint scrum knowledge experience life physical computational sciences strong written oral communication skills", "bachelor degree required least years employment data engineer professional setting relevant development experience expert python expert working cloud platforms aws google cloud etc experience airflow workflow management software ability define data model data storage strategies including knowledge distributed data systems ability manage multiple competing priorities make right tradeoffs timely delivery features experience familiarity geography geometry gis systems experience working satellite remote imagery relevant education coding bootcamp bachelors computer science equivalent experience part well funded early stage start market competitive comp equity incentives give stake future medical vision dental dependents pre tax commuter parking benefits flexible time upbeat collaborative work culture company sponsored outings", "experience informatica tools powercenter big data management master data management cloudera cdh ecosystem tools solr spark impala hive hue etc marklogic sas analytics python r amazon web services preferred", "bs ms degree computer science engineering related field years experience designing complex inter dependent data models analytic machine learning use cases years experience architecting building processes extract process add value data sets multiple source systems experiencing data modeling tuning relational well nosql datastores oracle red shift impala hdfs hive athena etc experience working distributed computing tools spark hive etc experience aws cloud services ec2 emr rds redshift s3 lambda experience data pipeline workflow management tools airflow etc years experience one general purpose programming languages including limited java scala c c c swift objective c python javascript years experience working leading agile development methodologies sprint scrum experience software engineering best practices including limited version control git tfs subversion etc ci cd jenkins maven gradle etc automated unit testing dev ops experience semantic technologies approaches plus biotech pharma experience plus full stack development using infrastructure cloud services aws preferred cloud native tools design patterns containers serverless docker etc plus", "", "advanced working sql knowledge experience working relational databases query authoring sql well working familiarity variety databases experience building optimizing big data data pipelines architectures data sets strong analytic skills related working unstructured datasets build processes supporting data transformation data structures metadata dependency workload management looking candidate years experience data engineer role attained degree computer science statistics informatics information systems relevant experience list type tech use though expect experience big data tools hadoop storm cassandra etc relational sql nosql databases including postgres mysql mssql data pipeline workflow management tools google cloud dataflow cloud services aws redshift google bigquery etc stream processing systems kafka storm spark streaming etc object oriented object function scripting languages python java c scala etc big data tools hadoop storm cassandra etc relational sql nosql databases including postgres mysql mssql data pipeline workflow management tools google cloud dataflow cloud services aws redshift google bigquery etc stream processing systems kafka storm spark streaming etc object oriented object function scripting languages python java c scala etc", "", "", "", "", "clearance ts sci ts sci current ci scope polygraph one year currency minimum willing undergo ci scope polygraph based specific analytical position plus education bachelors degree years direct relevant experience plus experience years analytical experience years identity intelligence functional regional source analysis experience operational strategic level within dod equivalent government agencies strong briefing skills include ability clearly articulate information senior members intelligence community ability gather analyze collate fuse available intelligence products produce iirs reports briefings including ability clearly articulate information education masters degree related field certification counter terrorism counter insurgency global regional issues humint ci pol mil geopolitical analysis senior intelligence analysis familiarity icd competed os301 fundamentals course completed os302 osint analytic tools course completed basic social media analysis course complete advanced social media analysis course competed os301 fundamentals course completed os302 osint analytic tools course completed basic social media analysis course complete advanced social media analysis course experience years experience related specific labor category least portion experience within last years equivalency chief warrant officer field grade officer o4 o5 jise ace director deputy director", "", "", "connections recruiters industry experts online live devex events", "", "", "years application development support experience deep knowledge distributed data architecture commonly used bi tools approaches packages deployed machine learning build experience creating production software systems proven track record identifying resolving performance bottlenecks production systems experience machine learning algorithm design feature engineering validation prediction recommendation measurement experience complex high volume multi dimensional data well machine learning models based unstructured structured streaming datasets good understanding payments banking industry including aspects consumer credit consumer debit prepaid small business commercial co branded merchant experience planning organising managing multiple large projects diverse cross functional teams demonstrated ability incorporate new techniques solve business problems post graduate degree information technology qualification computer science engineering ideal certification hadoop cloudera hortonworks apache spark working knowledge hadoop ecosystem associated technologies e g apache spark mllib graphx ipython sci kit pandas advanced experience writing optimizing efficient sql queries python scripts scala c experience ideal deliver results within committed scope timeline budget strong people project management skills experience ability travel within cemea short notice results oriented strong problem solving skills demonstrated intellectual analytical rigor good business acumen track record solving business problems data driven quantitative methodologies experience payment retail banking retail merchant industries preferred team oriented collaborative diplomatic flexible style detailed oriented expected ensure highest level quality rigor reports data analysis proven skills translating analytics output actionable recommendations delivery experience presenting ideas analysis stakeholders whilst tailoring data driven results various audience levels exhibits intellectual curiosity desire continuous learning exhibits intellectual curiosity desire continuous learning demonstrates integrity maturity constructive approach business challenges role model organization implementing core visa values respect individuals levels workplace strive excellence extraordinary results use sound insights judgments make informed decisions line business strategy needs leadership skills include ability allocate tasks resources across multiple lines businesses geographies leadership extends ability influence senior management within outside analytics groups ability successfully persuade influence internal stakeholders building best class solutions", "", "wary google hangout skype interviews publicly listed numbers used verify legitimacy interviewer", "", "", "", "connections recruiters industry experts online live devex events", "", "bs ms degree computer science engineering related field years experience architecting building processes extract process add value data sets multiple source systems experiencing data modeling tuning relational well nosql datastores oracle red shift impala hdfs hive athena etc experience working distributed computing tools spark hive etc experience aws cloud services ec2 emr rds redshift s3 lambda experience data pipeline workflow management tools airflow etc years experience one general purpose programming languages including limited java scala c c c swift objective c python javascript years experience working leading agile development methodologies sprint scrum experience software engineering best practices including limited version control git tfs subversion etc ci cd jenkins maven gradle etc automated unit testing dev ops experience semantic technologies approaches plus biotech pharma experience plus", "", "", "minimum years professional software development experience hands data centric role data engineering architecture streaming warehousing required experience least one modern server side language python java similar required experience least one relational database technology mysql postgres ms sql etc required proficient sql required familiarity cloud services infrastructure preferably aws familiarity columnar store databases vertica redshift snowflake etc experience building data ingest pipelines experience building working within extract load transform data lake architectures comfortable working mix structured unstructured data variety sources preferred experience schema design experience spark dataframes pandas big plus experience transforming partially fully unstructured data easily queryable formats experience tuning databases performance experience working agile environment scrum kanban knowledge machine learning tools concepts plus fun collaborative environment optional work home wednesdays competitive compensation benefits package including medical dental vision insurance weeks pto paid holidays total weeks 401k stock options commuter benefits stocked kitchens coffee soda snacks regular team activities including mariners games ping pong tournaments movies etc", "", "", "", "competitive salary", "", "", "", "bs computer science computer engineering data analytics data science physics mathematics information systems engineering quantitative disciplines experience extracting processing curating integrating analyzing data using python spark sql hands experience aws services - kinesis s3 glue lambda cloudformation rds ec2 emr hdfs hadoop yarn hbase hive pig hands experience elt etl dimensional data modeling proficiency python least one sql language sql pl sql excellent knowledge relational non relational database systems ability think creatively solve problems ability work highly collaborative dynamic work environment effective written oral communication skills experience building production data pipelines using python sql spark aws environment kinesis s3 glue lambda cloudformation rds hdfs hadoop yarn hbase hive pig strong programming experience python spark scala experience etl elt dimensional data modeling experience working relational non relational databases advanced sql nosql scripting familiarity data pipeline workflow management tools airflow aws step functions nifi", "understand business use data support work processes strategic business objectives leverage data analytics data science techniques create business value solution delivery self service enablement identifies acquires cleanses prepares data including data architecture aligned defined architecture patterns collaborates data scientists identify build integrate advanced analytics models solutions enables data scientists develop advanced analytics models uses agile approach develop analytic solutions technical components platform data ingestion data preparation analytics processing visualization deploys solution including model documentation training integration b computer science related field years industry experience developing data products analytics solutions business intelligence data warehousing data science. . . data architecture modeling integration experience analytics data product solution architecture hadoop microsoft azure analytics experience data extract load transformation technics tools hadoop ssis microsoft azure data factory understanding relational dimensional data models experience analytics data product solution architecture hadoop microsoft azure analytics experience data extract load transformation technics tools hadoop ssis microsoft azure data factory understanding relational dimensional data models data visualization analytics experience analytics solutions microsoft azure analytics paas saas experience data visualization spotfire powerbi experience analytics solutions microsoft azure analytics paas saas experience data visualization spotfire powerbi software engineering experience software engineering agile devops methodologies tools jira jenkins vsts experience programming languages scripting tools java python r c experience information security management experience software engineering agile devops methodologies tools jira jenkins vsts experience programming languages scripting tools java python r c experience information security management learning agility work productively uncertain fast changing environments finds opportunities ambiguity embraces adapts change adept learning work productively uncertain fast changing environments finds opportunities ambiguity embraces adapts change adept learning analytical thinking problem solving knowledge techniques tools promote effective analysis determine root cause problems create alternative solutions solve best interest business knowledge techniques tools promote effective analysis determine root cause problems create alternative solutions solve best interest business", "years relevant experience bs ms phd computer science related field experience java scala python expert knowledge machine learning algorithms operationalization data science pipelines demonstrable experience etl elt tools expert knowledge distributed data processing hadoop ecosystem spark strong knowledge sql eg mysql linux comfortable data security concepts sdlc familiarity leading cloud vendors gcp azure aws related tools", "", "bachelors degree least years experience using database management tools sql least year experience creating data visualizations using tableau least year experience using relational database systems snowflake postgresql mysql years coding experience years experience creating tableau visualizations years experience relational database systems including snowflake postgresql mysql years experience building data pipelines using etl tools years experience creating automated solutions", "", "", "", "experience demonstrated proficiency database development management experience extracting cleaning transforming data analysis required experience preferred higher education data analysis reporting setting institutional research enrollment analysis etc higher education setting advanced knowledge sql required advanced knowledge relational databases required experience preferred sql server oracle advanced practical experience sql based data query tools brio hyperion toad toad intelligence central strongly preferred practical experience data management tools alteryx preferred familiarity dimensional data modeling concepts e g star schema preferred familiarity python open source data engineering tools plus strong orientation detail strong organization skills excellent interpersonal communication skills including strong customer service orientation ability manage concurrent projects activities high degree quality data accuracy ability meet deadlines deliver projects timely manner bachelor' degree required master' degree preferred", "", "", "", "bachelor degree computer science engineering related field equivalent training fellowship work experience", "bs ms computational sciences computational biology bioinformatics related fields computer science applied math statistics experience scientific programming experience data analysis especially pertains protein sequence structure next generation dna sequence analysis excellent problem solving skills knowledge biochemistry bioinformatics protein structures molecular biology ngs data analysis ability execute automate standard statistical analyses using tools languages including unix python r sql etc knowledge cloud computing aws docker etc experience scientific software administration tool development strong communication skills", "b degree engineering related discipline years experience pharmaceutical biotech years experience master' degree", "bachelor' degree equivalent experience none database development knowledge experience e sql programming skills e python r java computer proficiency oracle unix linux intermediate analytic data sourcing data management skills ability extract data various data sources solid experience time task management ability learn new technologies strong attention detail intermediate written verbal communication skills including ability effectively collaborate multi disciplinary groups organizational levels", "", "", "years working agile devops teams scrum experience aws cloud services ec2 emr rds redshift s3 lambda experience data pipeline workflow management tools mulesoft informatica cloud etc familiarity various application technology stacks bi stacks technology domains data analytics working experience data analytic tools e g tableau splunk spotfire hadoop clear understanding relational dimensional database modeling skilled programmer sufficient experience high level programming languages c c java python visual basic experience programming compiled c c interpreted languages python ruby etc devops experience building deploying infrastructure cloud deployment build test automation technologies like ansible chef puppet docker jenkins etc experience big data advanced analytic techniques real time data full stack development using infrastructure cloud services aws preferred cloud native tools design patterns containers serverless docker etc demonstrated ability adopt new cloud technologies paradigms emerging big data technologies hadoop hive etc exceptional teaming skills encompassing cross functional teams peer relationships informing understanding appreciating differences proven experience member leader high performing team strong ability convey influence complex technical issues manner easily understood actionable experience applying change management methodologies large global corporate environments involving multiple businesses certified information security manager cism comptia security certified information systems security professional cissp", "demonstrated knowledge sql relational databases postgresql professional experience complex data systems strongly preferred experience public sector data data systems preferred programming experience either python r experience preferred familiarity version control systems git preferred ability balance many competing demands well demonstrate good decision making initiative excellent written verbal communication skills including discussing technical concepts non technical users ability work independently high degree initiative required ability manage multiple concurrent tasks required demonstrated judgment discretion handling sensitive information required must able remain stationary position extended periods time must able operate computer extensively four hours per day", "", "bachelor' master' ph degree computer science electrical engineering relevant field proven significant experience building managing data pipelines ability build manage data pipelines python scala java knowledge software engineering best practices code reviews testing frameworks maintainability readability commercial client facing project experience helpful including working close knit teams ability work across structured semi structured unstructured data extract information identify linkages across disparate data sets meaningful experience multiple database technologies hadoop ms sql server oracle mysql teradata confirmed ability clearly communicating complex solutions deep understanding information security principles ensure compliant handling management process data experience interest cloud platforms aws azure google platform familiarity data warehousing deploying etl processes python extraordinary attention detail strong organizational interpersonal skills get things done way optimizes results strengthens internal external relationships consideration resources knowledge cgmp' health authority regulations quality systems work environment physical demands safety considerations ability work international global environment travel anticipated may work clean room environment requires gowning form hospital scrubs ability sit stand move within work space extended periods", "wary google hangout skype interviews publicly listed numbers used verify legitimacy interviewer", "", "understand desire best place work trust foundation", "", "proven attention detail proactive communication proficient one following coding languages python java scala demonstrable experience writing sql using rdbms redshift postgres mysql teradata oracle etc experience schema design dimensional data modeling experience aws services like ec2 s3 redshift spectrum glue athena rds lambda api gateway experience software devops ci cd tools git jenkins linux shell script experience spark hive kafka kinesis spark streaming airflow hands experience using databricks jupyter similar notebook environment experience building data warehouses etl pipelines plus culture makes amgen special place work powerful shared purpose around mission serve patients respect one another recognize contributions embedded collaboration trust empowerment inclusion equip staff members live well rounded healthy lives recently amgen added benefits transgender employees continues pride industry leading family friendly offerings families compositions amgen focuses areas high unmet medical need uses expertise strive solutions improve health outcomes dramatically improve people lives biotechnology pioneer since amgen grown one world leading independent biotechnology companies reached millions patients around world developing pipeline medicines breakaway potential amgen equal opportunity employer consider without regard race color religion sex sexual orientation gender identity national origin protected veteran status disability status", "", "", "", "embed billing team create billing pipelines enable granular bills help better users understand costs", "", "", "bs ms degree computer science engineering related field years experience architecting building processes extract process add value data sets multiple source systems experiencing data modeling tuning relational well nosql datastores oracle red shift impala hdfs hive athena etc experience working distributed computing tools spark hive etc experience aws cloud services ec2 emr rds redshift s3 lambda experience data pipeline workflow management tools airflow etc years experience one general purpose programming languages including limited java scala c c c swift objective c python javascript years experience working leading agile development methodologies sprint scrum experience software engineering best practices including limited version control git tfs subversion etc ci cd jenkins maven gradle etc automated unit testing dev ops experience semantic technologies approaches plus biotech pharma experience plus full stack development using infrastructure cloud services aws preferred cloud native tools design patterns containers serverless docker etc plus", "existing affinity data science machine learning cloud computing networking security encryption restful interfaces devops", "computer science computer engineering mathematics degree strong analytical problem solving skills coding proficiency least one modern programming language python ruby java etc experience data modeling etl development data warehousing data warehousing experience example oracle redshift teradata etc experience building data products incrementally integrating managing datasets multiple sources knowledge mathematical statistics experience big data technologies hadoop hive hbase pig spark etc plus good english free glovo credits opportunity change world see everyone uses product build work international dynamic passionate environment great company culture consider variants relocation barcelona spain cover visa relocation costs", "various experience levels considered junior candidates must strong background coursework academic projects around data engineering machine learning scale appropriate industry experience contributing projects senior candidates must demonstrate track record successful technical leadership execution large scale data projects software engineering - level appropriate experience software engineering sdlc stack may include python golang scala must consider code readability reuse extensibility priority developing solutions big data engineering experience building scalable data pipelines involving machine learning optimization prediction big data devops experience performing operations automation various big data ecosystems production environments aws related cloud service ability thrive fast paced cross regional diverse dynamic work environment experience aws data stack - redshift athena emr kinesis documentdb dynamodb experience establishing well organized data lakes experience setting optimizing data warehouses background data modeling performance tuning relational sql databases experience data practices security data management governance experience operations research machine learning optimization", "", "graduate degree computer science information systems equivalent quantitative field years experience similar data engineer role experience working extracting value large disconnected unstructured datasets demonstrated ability build processes support data transformation data structures metadata dependency workload management advanced working sql knowledge experience working relational databases query authoring sql well working familiarity variety databases experience building optimizing 'big data' data pipelines architectures data sets experience performing root cause analysis internal external data processes answer specific business questions identify opportunities improvement experience developing frameworks support business applications", "", "connections recruiters industry experts online live devex events", "", "years relevant experience bs ms phd computer science related field experience java scala python expert knowledge machine learning algorithms operationalization data science pipelines demonstrable experience etl elt tools expert knowledge distributed data processing hadoop ecosystem spark strong knowledge sql eg mysql linux comfortable data security concepts sdlc familiarity leading cloud vendors gcp azure aws related tools", "", "", "", "", "", "", "years relevant experience data tools techniques manipulation required education college degree stem related field technical knowledge advanced knowledge data tools techniques manipulation preferred examples limited data science engineering big data cloud platforms programming languages sas sql spark python r h2o knime hive aws development visualization platforms python notebook ides github qlikview tableau microstrategy qlik sense experience communication skills ability communicate thoughts designs ideas unambiguous manner adjusts communication based audience exhibits active effective communication skills team members including active listening effective written verbal communication skills effectively contributes communicates immediate team able present complex technical concepts audiences varying size level business knowledge partnership able develop business partnerships influence business priorities solution identification aligned business objectives goals able communicate business terms describe data capabilities concepts ways business understand problem solving decision making able proficiently diagnose root causes solve complex issues able evaluate alternative solutions assess risk taking action ability reach sound decisions quickly escalates appropriately demonstrates ability optimize use available resources team orientation able maintain enhance partnerships across organization achieve objectives practices objectivity openness others views able recognize support team priorities leadership accountable set technical goals priorities self team members exhibits team leadership collaborates partners planning project management demonstrates ability identify critical project tasks establish clear priorities keeping bigger picture mind able effectively collaborate project manager utilize sound project management practices able manage time competing priorities financial awareness", "", "", "experience hadoop eco system hdfs spark", "years work experience including years experience data engineering processing extracting value large disconnected datasets continuous integration systems jenkins travis drone ci", "years work experience including years experience data engineering processing extracting value large disconnected datasets continuous integration systems jenkins travis drone ci", "", "", "", "", "competitive salary", "", "work cool people impact millions daily players", "", "", "", "least five years proficiency sql microsoft stack banking financial sector experience beneficial fove years net language experience visualisation storyboarding experience stats experience assist driving data services strategy across multiple platforms building operation frameworks processes take data services next level excellence building big data platform consolidating group data provisioning via required channels operational analytical data provisioning insights data landscape understanding optimising data flow patterns order rationalise data inputs shaping unstructured data acquisition realtime data integration patterns engagement shaping data initiatives ensure strategic alignment group architecture", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "career categories", "", "experience designing implementing supporting highly scalable data systems services java scala experience hadoop ecosystem technologies particular mapreduce spark spark sql spark streaming hive yarn mr2 experience building running large scale data pipelines including distributed messaging kafka data ingest multiple sources feed batch near realtime streaming compute components experience data modeling data architecture optimized big data patterns ie warehousing concepts efficient storage query hdfs data security privacy techniques knowledgable distributed storage network resources level hosts clusters dcs troubleshoot prevent performance issues", "minimum years experience analytics data engineering role bs ms degree quantitative field equivalent practical experience mastery relational databases sql mysql ability translate business processes data analytics solutions strong foundation data modeling including least years relevant business people analytics experience commitment data governance demonstrated ability manage technical non technical stakeholders drive collaboration experience python scripting language plus experience tableau plus desired attributes outstanding written verbal visual communication capabilities ability pivot widely divergent tasks subject matter short notice rapidly adapt varied audiences record adding value work outcomes innovation orientation toward continuous refinement improvement highly thorough detail oriented", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "years experience software engineer excellent programming skills e g python go java excellent problem solving analytic skills solid computer science systems foundations ability quickly learn new domains proven system development skills unix type os e g linux mac os experience working large data sets pipelines ideally using apache software stack e g spark hbase experience continuous integration continuous development solutions e g jenkins etc experience cloud native deployment e g kubernetes good communication skills teamwork passion building great products curiosity desire learn following experience nice required data modeling experience working search engines machine learning natural language processing", "", "work clients model data landscape obtain data extracts define secure data exchange approaches plan execute secure good practice data integration strategies approaches acquire ingest process data multiple sources systems big data platforms working across different environments technologies learning nuances -amazon web services sql nosql big data hadoop platforms designing setting running etl transformations using tools including informatica powercenter data quality informatica cloud create manage data environments cloud focus working financial services clients collaborate data scientists map data fields hypotheses curate wrangle prepare data use advanced analytical models keep date information security principles ensure compliant handling management client data involved end end data management cutting edge advanced analytics data science graduate relevant subject management consulting experience leading client facing projects including working close knit teams experience working projects within cloud preferably aws house experience within large financial institution preferred topics aml proven ability clearly communicating complex solutions strong development background experience least one scripting object oriented functional programming language etc sql python java scala c r data warehousing experience building operational etl data pipelines across number sources constructing relational dimensional data models distributed systems experience good experience least one database technology traditional rdbms ms sql server oracle mysql postgresql mpp aws redshift oracle exadata teradata ibm netezza distributed processing spark hadoop emr nosql mongodb dynamodb cassandra vertica neo4j titan experience least one etl tool e g informatica sap bods ability work across structured semi structured unstructured data extracting information identifying linkages across disparate data sets excellent interpersonal skills interacting clients verbally email written clear timely professional manner deep personal motivation always produce outstanding work clients colleagues excel team collaboration working others diverse skill sets backgrounds", "completed bsc computer science degree similar software engineering data science etc experience software two five years' experience data engineering hadoop spark data processing products big query redshift spectrum s3 athena kafka spark storm flink beam presto hive etl processes transformations cloud experience ideally google cloud platform devops stack development experience apache airflow data pipeline tools exposure scala java context data processing experience proficiency python experience design implementation data flows support sophisticated predictive data products maintaining data science production environments cloud based python ensuring outputs data science models available integrated system integrate coordinate maintain data flows various sources data manage maintain cloud service integrations perform key data functions working towards replacing third party elements data pipeline using open source tools make decisions around infrastructure layout processes data warehouse including working engineering team best track record data following data inconsistencies ensure corrected", "", "401k", "", "", "", "years hands experience business intelligence management role corporate consulting setting strong development background experience least two scripting object oriented functional programming languages sql python java scala c r client stakeholder engagement management experience leading work stream managing small teams agile projects data warehousing experience building operational etl elt pipelines comprised several sources architecting data models layers analytics ability work across structured semi structured unstructured data extracting information identifying linkages across disparate data sets experience multiple database technologies traditional rdbms ms sql server oracle mysql postgresql mpp aws redshift oracle exadata teradata ibm netezza distributed processing spark hadoop emr nosql mongodb dynamodb cassandra vertica neo4j titan experience developing solutions cloud platforms amazon web services microsoft azure google cloud platform experience generating insights form reports kpis dashboards ad hoc queries experience tableau bonus ability thrive lively project consulting setting often working different multiple projects time excellent interpersonal skills interacting clients verbally email written clear timely professional manner problem solving brainstorming solutions data integration analytics challenges passion developing knowledge skills technical aspects data technology industry personal professional development work life", "", "career categories", "", "career categories", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "", "", "", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "", "career categories", "identify data sources add value decision making work source system owners analysts understand source data e g data profiling definition mapping design implement efficient data loads using traditional structured data etl techniques design implement real time near real time data load solutions using technologies like data streaming design implement unstructured data loads e g text speech images video design implement load monitoring tools procedures perform continuous monitoring optimising loads work analysts architect design implement effective efficient data models using appropriate modelling techniques design implement data warehouse data models design implement data pipelines ad hoc unstructured data models design implement appropriate aggregation data structures enhance usability data e g multi dimensional olap structures summary tables etc design implement maintain appropriate indexing tables enhance speed access design implement data models support automated decision making analytics continuously search data elements sources enhance existing data objects supplement enhance context design implement interfaces data access e g batch exports real time decision api' etc design implement interface monitoring management solutions ensure availability accuracy design implement data monitoring solutions procedures continuously monitor maintain integrity existing environment troubleshoot technical data issues make appropriate changes required design implement meta data solutions assist understanding managing data work together business owners analysts maintain good data governance work together business owners analysts manage changes data organisation provide technical data related support source system teams external parties exchange data manage data growth usage implementing effective strategies e g archiving indexing manage systems technology tools enable data management analytics liaise infrastructure operations regarding system infrastructure management take ownership work delivering high quality work time show initiative pre active finding opportunities improve data processes take ownership career development continuously improving skills knowledge application thereof designing implementing solutions positive engagement team activities actively contribute ideas improve team dynamics performance complex solution service design implementation cross functional data team knowledge gathering sharing responsible team activities team dynamics performance manage project task delivery team multiple stakeholder management internal external assist development others e g mentoring knowledge share quality control ' work degree information technology engineering mathematics statistics actuarial related discipline least years' experience working data business intelligence analytics environment sql data analysis data visualisation data modelling microsoft business intelligence data technologies ssis ssas sql server data warehouse concepts best practices information gathering problem analysis applying professional specialist technical expertise creating innovating quality detail orientation planning organizing presenting communicating information analysing leadership", "", "' looking individuals proven big data experience either implementation data science prospective expert knowledge least one big data technology spark hadoop elasticsearch strong coding background either java python scala desire learn code scala experience working agile environment experience building data processing pipelines use production handsoff batch systems including either traditional etl pipelines analytics pipelines competitive salary company bonus private healthcare life insurance income protection pension scheme company contribution contribute days annual leave option buy sell days amazing working environment employee referral scheme etl scala", "career categories", "", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "virtual company sponsored social events", "team player excellent communication skills", "", "development experience least two different database platforms teradata oracle ms sql minimum years experience designing developing testing etl interfaces aligned defined requirements exposure business intelligence tools business objects informatica ssrs cognos microstrategy tableau qlikview spotfire etc experience tuning etl processes ensure performance reliability mdm experience competitive salary performance based bonus opportunities single family health insurance plans including dental coverage short term long term disability matching k competitive paid time training certification opportunities eligible expense reimbursement team building social activities", "experience spark google big query redis amazon aurora dynamo db kinesis riak", "completed bsc computer science degree similar software engineering data science etc experience software two five years' experience data engineering hadoop spark data processing products big query redshift spectrum s3 athena kafka spark storm flink beam presto hive etl processes transformations cloud experience ideally google cloud platform devops stack development experience apache airflow data pipeline tools exposure scala java context data processing experience proficiency python experience design implementation data flows support sophisticated predictive data products maintaining data science production environments cloud based python ensuring outputs data science models available integrated system integrate coordinate maintain data flows various sources data manage maintain cloud service integrations perform key data functions working towards replacing third party elements data pipeline using open source tools make decisions around infrastructure layout processes data warehouse including working engineering team best track record data following data inconsistencies ensure corrected", "machine learning statistical analysis", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "", "", "career categories", "", "years experience building traditional data warehouse solutions knowledgeable data modeling data access data storage techniques understand standard methodologies etl proficient debugging optimizing pipelines easily transition one etl tool set e g informatica kettle programmatic approaches python sql spark scala extensive experience sql understanding nosql solutions significant coding experience java python want apply skills processing big data years experience object oriented design coding testing patterns well experience engineering open source software platforms large scale data infrastructures understanding distributed systems nosql solutions redshift bigquery familiar google cloud cloud provider products servers able work teams collaborate others clarify requirements bachelor' degree master' information technology relevant discipline experience automation build tools release engineering bonus experience spark scala distributed data systems mpp databases", "excellent written verbal communication skills tenacious relentless determined curious always learning new technologies rapidly synthesizing new information understanding \" \" \" \" self directed capable operating amid ambiguity poised display excellent judgment prioritizing across difficult tradeoffs pragmatic letting \" perfect\" enemy \" good \"", "ba bs degree computer science engineering discipline statistics information systems another quantitative field", "", "", "", "", "", "", "", "career categories", "", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "career categories", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "", "experience areas data driven statistical modeling discriminative methods feature extraction analysis supervised learning", "", "", "possess bachelor degree computer science applied mathematics engineering technology related field", "", "work cool people impact millions daily players", "", "", "", "", "", "", "proficient designing accessing maintaining data stores data feeds data processing tools rdbms nosql apis kafka apache spark elk stack experience aws services ec2 s3 lambda glue athena redshift etc proven experience deploying machine learning algorithms production data engineer machine learning engineer similar role proficiency using github docker luigi airflow jenkins terraform proficient python shell scripting java plus familiar bi tools e g power bi aws quicksight tableu etc proficient writing high quality scalable code integrating version control systems knowledge statistical data mining techniques including model evaluation validation enthusiasm big data translating data actionable insights demonstrable experience big data technologies structured unstructured data proficient building robust data pipelines delivering reliable data services stakeholders ability work complex fast paced environment maintaining high degree accuracy professionalism excellent interpersonal verbal written communication skills agile project development experience plus experience leading successful data engineering projects operationalizing machine learning algorithms bs required years experience master degree preferred years experience computer science applied mathematics engineering operations research related field", "", "advanced degree systems electrical engineering equivalent experience systems engineering", "advanced sql knowledge years data extraction experience proficient writing advanced sql tuning sql code semantic layer esl development experience relational databases oracle teradata vertica hadoop desired experience data induction validation source systems experience working capital projects plus including writing requirements executing uat expert normalizing data reporting strong analytical skills ability evaluate analyze present data answer business questions experience data visualization tools e g tableau plus familiarity finance operations retail contact center data desired desire end end ownership work flexibility balance directional changes ability support multiple deadline specific projects maintaining day day business support ability deal ambiguity proactive driven individual comfortable working global matrixed fast paced environment", "excellent written verbal communication skills tenacious relentless determined curious always learning new technologies rapidly synthesizing new information understanding \" \" \" \" self directed capable operating amid ambiguity poised display excellent judgment prioritizing across difficult tradeoffs pragmatic letting \" perfect\" enemy \" good \"", "", "", "bachelor master degree computer science related technical field equivalent professional experience greater years experience safety quality first valuing ethics integrity diversity passion serving customers globally dedication servant leadership creating value shareholders customers employees consistently delivering commitments competitive salary comprehensive health wellness income protection benefits k savings plan company match paid vacations holidays opportunities flexible work arrangements educational reimbursement program employee referral program", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "python", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "excellent written verbal communication skills tenacious relentless determined curious always learning new technologies rapidly synthesizing new information understanding \" \" \" \" self directed capable operating amid ambiguity poised display excellent judgment prioritizing across difficult tradeoffs pragmatic letting \" perfect\" enemy \" good \"", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "bsc msc computer science related field equivalent experience strong analytical learning problem solving skills personal interest subjects math statistics machine learning ai analytics solid knowledge data structures algorithms unix linux proficient scala java sql strong experience apache spark experience working agile environment using tdd continuous integration experience refactoring code scale production mind familiar git python javascript html css proficient understanding distributed computing principles proficiency hadoop v2 mapreduce hdfs good knowledge big data querying tools pig hive impala experience integration data multiple data sources experience nosql databases hbase cassandra mongodb knowledge various etl techniques frameworks experience big data ml toolkits mahout sparkml h2o experience cloudera mapr hortonworks management hadoop cluster included services experience building stream processing systems using solutions storm spark streaming good understanding lambda architecture along advantages drawbacks experience various messaging systems kafka rabbitmq", "career categories", "", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "", "identify data sources add value decision making work source system owners analysts understand source data e g data profiling definition mapping design implement efficient data loads using traditional structured data etl techniques design implement real time near real time data load solutions using technologies like data streaming design implement unstructured data loads e g text speech images video design implement load monitoring tools procedures perform continuous monitoring optimising loads work analysts architect design implement effective efficient data models using appropriate modelling techniques design implement data warehouse data models design implement data pipelines ad hoc unstructured data models design implement appropriate aggregation data structures enhance usability data e g multi dimensional olap structures summary tables etc design implement maintain appropriate indexing tables enhance speed access design implement data models support automated decision making analytics continuously search data elements sources enhance existing data objects supplement enhance context design implement interfaces data access e g batch exports real time decision api' etc design implement interface monitoring management solutions ensure availability accuracy monitor maintain integrity existing environment troubleshoot technical data issues make appropriate changes required implement meta data solutions assist understanding managing data work together business owners analysts maintain good data governance provide technical data related support source system teams external parties exchange data manage data growth usage designing implementing effective strategies e g archiving indexing manage systems technology tools enable data management analytics take ownership work delivering high quality work time show initiative pre active finding opportunities improve data processes take ownership career development continuously improving skills knowledge application thereof designing implementing solutions positive engagement team activities actively contribute ideas improve team dynamics performance stakeholder management internal external assist development others e g mentoring knowledge share quality control ' work degree information technology mathematics engineering actuarial science related discipline least years technical data role preferably formal data data warehouse business intelligence environment sql data analysis data visualisation data modelling microsoft business intelligence data technologies ssis ssas sql server data warehouse concepts best practices financial services knowledge specifically personal unsecured loans business process monitoring optimising microsoft business intelligence visualisation technologies ssrs power bi infrastructure e g storage networking servers security unstructured data experience information gathering problem analysis applying professional specialist technical expertise creating innovating quality detail orientation analysing", "", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "knowledge best practices operations always always available service experience knowledge agile software development methodologies excellent problem solving troubleshooting skills process oriented great documentation skills excellent oral written communication skills keen sense customer service bs ms degree computer science related technical field years python java development experience years sql experience sql experience plus years experience schema design dimensional data modeling ability managing communicating data warehouse plans internal clients experience designing building maintaining data processing systems experience working either map reduce mpp system size scale", "", "bachelor master degree computer science related technical field equivalent professional experience greater years experience safety quality first valuing ethics integrity diversity passion serving customers globally dedication servant leadership creating value shareholders customers employees consistently delivering commitments competitive salary comprehensive health wellness income protection benefits k savings plan company match paid vacations holidays opportunities flexible work arrangements educational reimbursement program employee referral program", "architect build data pipelines architect implement data warehouse structure table schemas develop data models enable end users effectively analyze data optimize tune data warehouse query performance analytical workloads identify troubleshoot resolve data quality issues write complex sql queries data analysis design maintain robust data reporting visualization tools based requirements develop integrations bi tools third party productivity applications years engineering experience expert sql preferably across number dialects commonly write snowflake redshift postgresql mysql sql server experience developing software code one programming languages python java scala ruby experience managing database data warehouse technologies bonus redshift snowflake experience implementing etl tools bonus stitch fivetran matillion understanding data analytics ecosystem experience one relevant tools spark kafka aws glue amazon kinesis sqoop flume flink experience implementing business intelligence tools bonus looker experience developing data pipelines scratch", "", "", "", "", "workflow scheduling orchestration airflow oozie big data warehousing rdb mpp db oracle teradata postgress hive strong python programming skills understanding data analytics linear algebra ml libraries numpy scipy experience query apis using json protocolbuffers xml", "strong software development skills proficiency object oriented functional python required demonstrated skill experience using pyspark sparksql familiarity cloud based distributed systems blob storage elastic compute virtual instances familiarity software development life cycles tools methodologies support git continuous integration issue tracking code reviews quality assurance processes scheduling supporting data collection curation data provenance working machine learning models exposure signal processing low level data collection experience statistical inference methods scientific experimentation experience scala ensure individuals disabilities provided reasonable accommodation participate job application interview process perform essential job functions receive benefits privileges employment please contact us request accommodation apple' important resource soul people apple benefits help well employees families meaningful ways matter work apple take advantage health wellness resources time away programmes ' proud provide stock grants employees levels company also give employees option buy apple stock discount -- offer everyone apple chance share company' success ' discover many benefits working apple programmes match charitable contributions reimburse continuing education give special employee pricing apple products apple benefits programmes vary country subject eligibility requirements apple committed working providing reasonable accommodation applicants physical mental disabilities apple drug free workplace", "", "bachelor degree computer science information systems related field years data engineering aws environment developing etl tools years practical hands work experience data systems years practical demonstrable hands work experience sql reading writing database management skills experience one following python java scala r sharp attention detail ability effectively prioritize execute multiple tasks excellent problem solving skills ability communicate effectively stakeholders peers experience working relational nosql columnar data stores familiarity aws aws lambda s3 redshift mongodb graph databases azure cloud based technologies contribute development newly established data warehouse ecosystem communicate effectively define requirements wide range different teams translate business requests database design execution work closely domain experts understand source systems data build robust scalable interfaces etl programs data pipelines develop maintain data dictionary published data sources develop improve data release testing processes create manage data sources integrate numerous apis ability identify resolve performance issues operate open source cloud based environment includes aws redshift python r hadoop spark technologies work structured unstructured data competitive wages including performance bonuses medical dental life disability insurance paid time 401k employer match employer funded retirement plan health savings account medical dependent care flexible spending accounts wellness program membership tpc sawgrass", "proficient sql programming python preferred experience mpp databases preferred years experience data engineering etl pipeline development years spark development years experience big data technologies hadoop mapreduce hive etc. . . spark experience preferred", "", "data engineer five seven years' experience big data tools hadoop spark kafka five seven years' experience sql knowledge experience working relational databases query authoring sql well working familiarity variety databases five seven years strong analytic skills related working unstructured datasets experience aws cloud services ec2 emr rds redshift full sql stack ssis ssas ssrs degree information technology", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "seeks challenge gets things done math stats machine learning background data modeling sql nosql database experience", "", "position requires computer science bioinformatics related degree years experience data movement data wrangling delivery data analytics pipelines experience implementing maintaining data analytic pipelines experience big data technologies cloud based offerings microsoft azure gcp aws etc corresponding tools experience data movement management pharmaceutical industry related scientific fields experience core components hadoop stack including hdfs apache spark ideally cloudera based stack background experience lims systems next generation sequencing ngs workflows cloud computing hpc systems understanding diverse 'omic data types including rna seq dna seq chip seq wes wgs atac seq microbiome proteomic metabolomic data etc different sources familiarity data mining machine learning artificial intelligence techniques proven ability contribute development projects operating pace agile decision making using evidence applying judgement balance pace rigour risk committed delivering high quality results overcoming challenges focusing matters execution continuously looking opportunities learn build skills share learning sustaining energy well building strong relationships collaboration honest open conversations", "", "identify data sources add value decision making work source system owners analysts understand source data e g data profiling definition mapping design implement efficient data loads using traditional structured data etl techniques design implement real time near real time data load solutions using technologies like data streaming design implement unstructured data loads e g text speech images video design implement load monitoring tools procedures perform continuous monitoring optimising loads work analysts architect design implement effective efficient data models using appropriate modelling techniques design implement data warehouse data models design implement data pipelines ad hoc unstructured data models design implement appropriate aggregation data structures enhance usability data e g multi dimensional olap structures summary tables etc design implement maintain appropriate indexing tables enhance speed access design implement data models support automated decision making analytics continuously search data elements sources enhance existing data objects supplement enhance context design implement interfaces data access e g batch exports real time decision api' etc design implement interface monitoring management solutions ensure availability accuracy design implement data monitoring solutions procedures continuously monitor maintain integrity existing environment troubleshoot technical data issues make appropriate changes required design implement meta data solutions assist understanding managing data work together business owners analysts maintain good data governance work together business owners analysts manage changes data organisation provide technical data related support source system teams external parties exchange data manage data growth usage implementing effective strategies e g archiving indexing manage systems technology tools enable data management analytics liaise infrastructure operations regarding system infrastructure management take ownership work delivering high quality work time show initiative pre active finding opportunities improve data processes take ownership career development continuously improving skills knowledge application thereof designing implementing solutions positive engagement team activities actively contribute ideas improve team dynamics performance complex solution service design implementation cross functional data team knowledge gathering sharing responsible team activities team dynamics performance manage project task delivery team multiple stakeholder management internal external assist development others e g mentoring knowledge share quality control ' work degree information technology engineering mathematics statistics actuarial related discipline least years' experience working data business intelligence analytics environment sql data analysis data visualisation data modelling microsoft business intelligence data technologies ssis ssas sql server data warehouse concepts best practices information gathering problem analysis applying professional specialist technical expertise creating innovating quality detail orientation planning organizing presenting communicating information analysing leadership", "agile engineering kanban lean hybrid agile experience big plus", "", "degree educated data science similar strong experience data preparation techniques including exploration visualisation experience statistical models times series analysis multiple machine learning techniques clustering regression classification strong skills using python sql elastic visualise data surfacing tool experience developing machine learning systems experience amazon quicksite advantage", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "splunk hadoop similar", "", "", "expertise python programming functional object oriented strong foundation statistical analysis able develop optimized pipelines data acquisition pruning preprocessing insightful data performance visualizations iterating algorithm variants", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "subject expected comply applicable university policies procedures including limited personnel policies policies found university' administrative guide http adminguide stanford edu", "", "", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "contact us unsolicited services offers", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "career categories", "", "", "", "excellent programming skills c c python java prior experience developing production software years minimum experience linux system administration command line tools strong analytical thinking self motivated able work independently", "years experience developing etl jobs analyzing processing high volume data apache hadoop ecosystem especially spark expert knowledge one object oriented programming languages scala preferred proficient schema design data modeling experience data tools jupyter notebooks zeppelin excellent problem solving analytic skills ability program several scripting languages python perl bash experience workflow management tools oozie airflow azkaban etc experience batch streaming data processing ability learn research new technologies rapidly passion customer privacy strong interpersonal skills experience working cross functional projects", "identify data sources add value decision making work source system owners analysts understand source data e g data profiling definition mapping design implement efficient data loads using traditional structured data etl techniques design implement real time near real time data load solutions using technologies like data streaming design implement unstructured data loads e g text speech images video design implement load monitoring tools procedures perform continuous monitoring optimising loads work analysts architect design implement effective efficient data models using appropriate modelling techniques design implement data warehouse data models design implement data pipelines ad hoc unstructured data models design implement appropriate aggregation data structures enhance usability data e g multi dimensional olap structures summary tables etc design implement maintain appropriate indexing tables enhance speed access design implement data models support automated decision making analytics continuously search data elements sources enhance existing data objects supplement enhance context design implement interfaces data access e g batch exports real time decision api' etc design implement interface monitoring management solutions ensure availability accuracy monitor maintain integrity existing environment troubleshoot technical data issues make appropriate changes required implement meta data solutions assist understanding managing data work together business owners analysts maintain good data governance provide technical data related support source system teams external parties exchange data manage data growth usage designing implementing effective strategies e g archiving indexing manage systems technology tools enable data management analytics take ownership work delivering high quality work time show initiative pre active finding opportunities improve data processes take ownership career development continuously improving skills knowledge application thereof designing implementing solutions positive engagement team activities actively contribute ideas improve team dynamics performance stakeholder management internal external assist development others e g mentoring knowledge share quality control ' work degree information technology mathematics engineering actuarial science related discipline least years technical data role preferably formal data data warehouse business intelligence environment sql data analysis data visualisation data modelling microsoft business intelligence data technologies ssis ssas sql server data warehouse concepts best practices financial services knowledge specifically personal unsecured loans business process monitoring optimising microsoft business intelligence visualisation technologies ssrs power bi infrastructure e g storage networking servers security unstructured data experience information gathering problem analysis applying professional specialist technical expertise creating innovating quality detail orientation analysing", "possess bachelor degree computer science applied mathematics engineering technology related field", "", "nosql databases like mongodb cassandra neo4j", "", "precise thought code serious interest sql data sql dev skills strong - stored procs etc knowledge ssis - standard etl ' drives ssas would useful", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "", "", "", "", "expert designing implementing supporting highly scalable data systems services java scala extensive experience hadoop ecosystem technologies particular mapreduce spark spark sql spark streaming hive yarn mr2 expertise building running large scale data pipelines including distributed messaging kafka data ingest multiple sources feed batch near realtime streaming compute components experience data modeling data architecture optimized big data patterns ie warehousing concepts efficient storage query hdfs data security privacy techniques knowledgable distributed storage network resources level hosts clusters dcs troubleshoot prevent performance issues", "", "", "years software engineering experience track record making difference past projects naturally accountable responsible self motivated self sufficient experience designing distributed systems services scale experience working cassandra solr spark hadoop kafka similar technologies production contexts scale preference experience scala python apple' important resource soul people apple benefits help well employees families meaningful ways matter work apple take advantage health wellness resources time away programmes ' proud provide stock grants employees levels company also give employees option buy apple stock discount -- offer everyone apple chance share company' success ' discover many benefits working apple programmes match charitable contributions reimburse continuing education give special employee pricing apple products apple benefits programmes vary country subject eligibility requirements", "high level understanding well hands experience implementing data pipelines proficient scripting functional programming languages python scala bash groovy ruby experience database message queueing data streaming solutions instance postgresql aws rds apache kafka aws kinesis rabbitmq redis apache spark similar tools experience regression testing data pipelines experience devops application monitoring tools plus instance docker kubernetes terraform cloudformation ansible chef puppet salt splunk elastic elk stack sentry datadog similar tools knowledge machine learning computer vision plus", "bachelor' degree higher quantitative technical field computer science statistics engineering minimum two years work experience related field required working knowledge data design architecture warehousing understanding data management fundamentals data storage principles knowledge distributed systems pertains data storage cloud computing understanding administration aws docker linux based systems experience custom etl design implementation maintenance experience large scale data processing using traditional distributed systems like hadoop spark dataflow airflow knowledge practical experience machine learning ai fundamentals experience implementing machine learning solutions scale experience working batch real time data processing systems ability work communicate effectively stakeholders opportunity work one fastest growing financial startups country competitive salary equity 401k company match gym public transportation subsidy student loan assistance relocation assistance unlimited pto", "", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "", "bachelor' degree higher quantitative technical field computer science statistics engineering working knowledge data design architecture warehousing understanding data management fundamentals data storage principles knowledge distributed systems pertains data storage cloud computing understanding administration aws docker linux based systems experience custom etl design implementation maintenance experience large scale data processing using traditional distributed systems like hadoop spark dataflow airflow knowledge practical experience machine learning ai fundamentals experience implementing machine learning solutions scale experience working batch real time data processing systems ability work communicate effectively stakeholders opportunity work one fastest growing financial startups competitive salary equity health dental vision insurance 401k gym public transportation subsidy relocation assistance", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "", "", "experience sql databases graph databases largest growth sql neo4j currently data transformation experience scripting python java perl pl sql platforms spark knime exposure aws data services experience architect design implement updates enhancements data repositories indices support data ingest enrichment analysis visualization dissemination", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "relevant degree equivalent applied knowledge database architecture concepts within traditional rdbms environments also memory nosql virtualised federated approaches columnar streaming databases strong knowledge concepts oltp olap star snowflake schemas dimensional modelling normalisation extensive track record performing data engineering domain including extracting data enterprise systems sap security management data modelling etl development using tools data integrator services experience experience scrum agile project management annual bonus plan discretionary cash award group personal pension plan enhanced company contribution medical travel health life insurances holiday days annual leave option buy additional days per year sabbatical paid days every four year service volunteering one paid working day year teamarm varies location cycle work free car parking gym site team social events", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "", "", "experience areas data driven statistical modeling discriminative methods feature extraction analysis supervised learning", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "career categories", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "", "experience building data pipelines analytics systems distributed data systems aws using spark experience scala python experience building batch pipelines data event data streams nosql apis competitive health insurance benefits competitive salary annual target bonus commission paid vacation sick time vacation rental yearly basis taxable benefit employee stock purchase program free snacks beverages frequent company update talks leadership team free listing homeaway com electronic adjustable stand desk discounted metro rail pass", "", "identify data sources add value decision making work source system owners analysts understand source data e g data profiling definition mapping design implement efficient data loads using traditional structured data etl techniques design implement real time near real time data load solutions using technologies like data streaming design implement unstructured data loads e g text speech images video design implement load monitoring tools procedures perform continuous monitoring optimising loads work analysts architect design implement effective efficient data models using appropriate modelling techniques design implement data warehouse data models design implement data pipelines ad hoc unstructured data models design implement appropriate aggregation data structures enhance usability data e g multi dimensional olap structures summary tables etc design implement maintain appropriate indexing tables enhance speed access design implement data models support automated decision making analytics continuously search data elements sources enhance existing data objects supplement enhance context design implement interfaces data access e g batch exports real time decision api' etc design implement interface monitoring management solutions ensure availability accuracy monitor maintain integrity existing environment troubleshoot technical data issues make appropriate changes required implement meta data solutions assist understanding managing data work together business owners analysts maintain good data governance provide technical data related support source system teams external parties exchange data manage data growth usage designing implementing effective strategies e g archiving indexing manage systems technology tools enable data management analytics take ownership work delivering high quality work time show initiative pre active finding opportunities improve data processes take ownership career development continuously improving skills knowledge application thereof designing implementing solutions positive engagement team activities actively contribute ideas improve team dynamics performance stakeholder management internal external assist development others e g mentoring knowledge share quality control ' work degree information technology mathematics engineering actuarial science related discipline least years technical data role preferably formal data data warehouse business intelligence environment sql data analysis data visualisation data modelling microsoft business intelligence data technologies ssis ssas sql server data warehouse concepts best practices financial services knowledge specifically personal unsecured loans business process monitoring optimising microsoft business intelligence visualisation technologies ssrs power bi infrastructure e g storage networking servers security unstructured data experience information gathering problem analysis applying professional specialist technical expertise creating innovating quality detail orientation analysing", "bs ms equivalent computer science related technical field years software engineering oop languages years architectural experience developing scalable data pipelines data processing frameworks cloud native distributed computing environment years serving lead data pipeline architect large scale cloud based high volume data processing organization business division demonstrated experience writing architectural requirements systems design documents experience designing governing scalable cloud native backend compute capabilities rest apis microservices distributed computing frameworks geospatial processing indexing messaging frameworks paradigms data quality management excellent written verbal communication including presentation complex engineering designs concepts solutions clear concise manner technical non technical audiences alike expertise processing aggregating high volume geospatially oriented iot data expertise imagery processing feature extraction satellite aerial sources depth knowledge data operations data quality management space experience layered geospatial data structures data representations expertise designing implementing highly scalable data intensive distributed computing solutions using modern cloud native processing frameworks experience amazon web services ec2 s3 rds sqs etc strongly preferred experience compiled jvm language including scala plus superb medical dental vision life disability benefits 401k matching program stocked kitchen large assortment snacks drinks get day encouragement get office field agents farmers see first hand products used inspire one another innovate leave mark world find possible impossible", "", "", "extensive experience relational database development particularly oracle ms sql server experience writing optimising stored procedures views transform deliver data experience using performance monitoring alerting tools experience scripting automation language shell python familiar etl processes interest nosql database bigdata concepts systems knowledge scrum agile methodologies experience data architecture dimensional modelling would plus experience markit edm would plus knowledge investment management including investment risk would definite plus good communicator collaborator keen work closely across teams business take ownership show willingness tackle difficult issues embrace effectively manage change get hands providing technical direction problem solving support team ensure third party development conforms organisation best practice quality standards demonstrate exceptional problem solving skills intellectual curiosity show passion innovation continuous improvement initiate efforts implement alternative solutions", "", "", "years data extraction front end report building php similar proficient data access preparation methods using teradata sql relational databases proficient scripting glue languages like php python web technologies like html css javascript strong interpersonal skills verbal written ability work multiple assignments excellent attention detail flexibility handle directional changes ability support multiple deadline specific projects maintaining day day business support driven self motivated individual experienced working global matrixed fast paced environment ability comprehensively understand data elements sources relationships ability establish manage relationships cross functional team environment apple equal opportunity employer committed inclusion diversity also take affirmative action offer employment advancement opportunities applicants including minorities women protected veterans individuals disabilities apple discriminate retaliate applicants inquire disclose discuss compensation applicants", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "minimum five years data analytics programming database administration data management experience", "", "degree educated data science similar strong experience data preparation techniques including exploration visualisation experience statistical models times series analysis multiple machine learning techniques clustering regression classification strong skills using python sql elastic visualise data surfacing tool experience developing machine learning systems experience amazon quicksite advantage", "stated experience level guide preclude applications candidates less experience provided requisite skills demonstrated", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "", "bachelors degree computer science computer engineering applied math statistics related field certified data management professional cdmp plus years experience designing building maintaining data management systems broad experience planning architecting delivering mission critical enterprise grade systems solutions experience cloud architecture data repositories data design experience experience data cleansing optimization data consumption experience source control tools git tfs plus experience net framework web services soap rest xml plus experience azure service fabric microservices plus working familiarity front end web framework using c javascript angular plus experience using csv json xml data formats experience developing lob line business applications well consumer websites experience consumer facing ecommerce mobile systems plus free day resort stays cruises anniversary medical health insurance onsite wellness clinic long term disability life insurance dental vision coverage 401k plan pet care insurance legal insurance flexible spending accounts fsa employee assistance program discounted employee services dry cleaning nail services massage personal training etc dedicated employee enrichment recognition programs", "career categories", "career categories", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "", "excellent written verbal communication skills tenacious relentless determined curious always learning new technologies rapidly synthesizing new information understanding \" \" \" \" self directed capable operating amid ambiguity poised display excellent judgment prioritizing across difficult tradeoffs pragmatic letting \" perfect\" enemy \" good \"", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "work cool people impact millions daily players", "", "", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "experience high level programming languages java scala python proficiency databases sql required proficiency data processing using technologies like spark streaming spark sql map reduce expertise hadoop related technologies hdfs azkaban oozie impala hive pig expertise developing big data pipelines using technologies like kafka flume storm experience large scale data warehousing mining analytic systems ability work analysts gather requirements translate data engineering tasks", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "years experience data warehouse space years experience working either mapreduce mpp system years experience writing complex sql etl processes years experience object oriented programming languages bs ba technical field computer science mathematics knowledge python java experience analyzing data identify deliverables gaps inconsistencies actively mentored team members careers experience effectively collaborating communicating complex technical concepts broad variety audiences", "", "four years development experience working python data skills sql document stores large scale etl apache beam apache spark high scale restful services cloud experience google cloud platform azure aws bsc b tech computer science preferred", "experience azure data factory data bricks data lake", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "years experience data engineering expertise various etl technologies familiar etl tools engineered metrics statistical information massive complex datasets e g hive spark mllib druid solr kafka proficient least one programming language e g python scala comfortable developing code within team environment e g git testing code reviews built robust data analytic pipelines keen eye automate e g oozie airflow solid understanding relational nosql database technologies experience visualization data mining statistical tools", "", "", "", "years experience deep understanding data engineering concepts database designs advanced sql knowledge working relational data postgres programming experience java python working unstructured data apis experience etl integration tooling dms stitch experience data analysis visualization tools mode working knowledge message queuing sns sqs rabbitmq stream processing kinesis strong communication skills positive attitude empathy self awareness desire continually improve desire write maintain clean well tested code base avoiding tech debt experience business operations tools salesforce zuora hubspot etc experience apache spark general understanding data science machine learning technology landscape experience aws services including lambda dynamodb etc competitive salary employee stock option plan generous health commuter benefits dog friendly office", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "commercial client facing project experience including working close knit teams proven ability clearly communicating complex solutions data security governance expertise strong experience traditional data warehousing etl tools informatica talend pentaho datastage experience interest big data technologies hadoop spark nosql dbs cloud aws azure strong sql experience optional python scala java r exceptional attention detail", "", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "assist gathering new data sources scale including api calls web scraping process unstructured structured data use analysis proficient one common scripting languages python preferred experience designing data architecture ground experience processing large amounts structured unstructured data extensive aws experience", "", "", "", "years experience working data systems years experience data warehousing processing pipelines infrastructure query patterns years experience spark hadoop years experience open source etl frameworks airflow luigi similar years experience python sql experience systems data processing spark flink hadoop airflow storage s3 kafka elasticsearch dynamo mysql postgres experience elk experience reading optimizing data schema queries content performance", "years engineering experience years focused architecting data platforms working experience large scale data warehouse platform hands experience working either mapreduce spark presto mpp system skilled dimensional data modeling schema design data warehousing bachelor degree computer science engineering mathematics related fields equivalent experience expert sql knowledge experience optimizing large datasets masters degree computer science engineering mathematics related fields equivalent experience", "", "", "career categories", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "", "career categories", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "career categories", "curious mind obsession quality background data science data mining multivariate statistics computer vision machine learning experience working large scale data sets solid programming skills including python c c experience data visualization presentation familiar data analysis tools tableau", "", "", "identify data sources add value decision making work source system owners analysts understand source data e g data profiling definition mapping design implement efficient data loads using traditional structured data etl techniques design implement real time near real time data load solutions using technologies like data streaming design implement unstructured data loads e g text speech images video design implement load monitoring tools procedures perform continuous monitoring optimising loads work analysts architect design implement effective efficient data models using appropriate modelling techniques design implement data warehouse data models design implement data pipelines ad hoc unstructured data models design implement appropriate aggregation data structures enhance usability data e g multi dimensional olap structures summary tables etc design implement maintain appropriate indexing tables enhance speed access design implement data models support automated decision making analytics continuously search data elements sources enhance existing data objects supplement enhance context design implement interfaces data access e g batch exports real time decision api' etc design implement interface monitoring management solutions ensure availability accuracy monitor maintain integrity existing environment troubleshoot technical data issues make appropriate changes required implement meta data solutions assist understanding managing data work together business owners analysts maintain good data governance provide technical data related support source system teams external parties exchange data manage data growth usage designing implementing effective strategies e g archiving indexing manage systems technology tools enable data management analytics take ownership work delivering high quality work time show initiative pre active finding opportunities improve data processes take ownership career development continuously improving skills knowledge application thereof designing implementing solutions positive engagement team activities actively contribute ideas improve team dynamics performance stakeholder management internal external assist development others e g mentoring knowledge share quality control ' work degree information technology mathematics engineering actuarial science related discipline least years technical data role preferably formal data data warehouse business intelligence environment sql data analysis data visualisation data modelling microsoft business intelligence data technologies ssis ssas sql server data warehouse concepts best practices financial services knowledge specifically personal unsecured loans business process monitoring optimising microsoft business intelligence visualisation technologies ssrs power bi infrastructure e g storage networking servers security unstructured data experience information gathering problem analysis applying professional specialist technical expertise creating innovating quality detail orientation analysing", "", "architect build data pipelines architect implement data warehouse structure table schemas develop data models enable end users effectively analyze data optimize tune data warehouse query performance analytical workloads identify troubleshoot resolve data quality issues write complex sql queries data analysis design maintain robust data reporting visualization tools based requirements develop integrations bi tools third party productivity applications years engineering experience expert sql preferably across number dialects commonly write snowflake redshift postgresql mysql sql server experience developing software code one programming languages python java scala ruby experience managing database data warehouse technologies bonus redshift snowflake experience implementing etl tools bonus stitch fivetran matillion understanding data analytics ecosystem experience one relevant tools spark kafka aws glue amazon kinesis sqoop flume flink experience implementing business intelligence tools bonus looker experience developing data pipelines scratch", "career categories", "", "", "", "", "", "", "", "", "", "career categories", "identify data sources add value decision making work source system owners analysts understand source data e g data profiling definition mapping design implement efficient data loads using traditional structured data etl techniques design implement real time near real time data load solutions using technologies like data streaming design implement unstructured data loads e g text speech images video design implement load monitoring tools procedures perform continuous monitoring optimising loads work analysts architect design implement effective efficient data models using appropriate modelling techniques design implement data warehouse data models design implement data pipelines ad hoc unstructured data models design implement appropriate aggregation data structures enhance usability data e g multi dimensional olap structures summary tables etc design implement maintain appropriate indexing tables enhance speed access design implement data models support automated decision making analytics continuously search data elements sources enhance existing data objects supplement enhance context design implement interfaces data access e g batch exports real time decision api' etc design implement interface monitoring management solutions ensure availability accuracy monitor maintain integrity existing environment troubleshoot technical data issues make appropriate changes required implement meta data solutions assist understanding managing data work together business owners analysts maintain good data governance provide technical data related support source system teams external parties exchange data manage data growth usage designing implementing effective strategies e g archiving indexing manage systems technology tools enable data management analytics take ownership work delivering high quality work time show initiative pre active finding opportunities improve data processes take ownership career development continuously improving skills knowledge application thereof designing implementing solutions positive engagement team activities actively contribute ideas improve team dynamics performance stakeholder management internal external assist development others e g mentoring knowledge share quality control ' work degree information technology mathematics engineering actuarial science related discipline least years technical data role preferably formal data data warehouse business intelligence environment sql data analysis data visualisation data modelling microsoft business intelligence data technologies ssis ssas sql server data warehouse concepts best practices financial services knowledge specifically personal unsecured loans business process monitoring optimising microsoft business intelligence visualisation technologies ssrs power bi infrastructure e g storage networking servers security unstructured data experience information gathering problem analysis applying professional specialist technical expertise creating innovating quality detail orientation analysing", "architect build data pipelines architect implement data warehouse structure table schemas develop data models enable end users effectively analyze data optimize tune data warehouse query performance analytical workloads identify troubleshoot resolve data quality issues write complex sql queries data analysis design maintain robust data reporting visualization tools based requirements develop integrations bi tools third party productivity applications years engineering experience expert sql preferably across number dialects commonly write snowflake redshift postgresql mysql sql server experience developing software code one programming languages python java scala ruby experience managing database data warehouse technologies bonus redshift snowflake experience implementing etl tools bonus stitch fivetran matillion understanding data analytics ecosystem experience one relevant tools spark kafka aws glue amazon kinesis sqoop flume flink experience implementing business intelligence tools bonus looker experience developing data pipelines scratch", "", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "", "love hack things come creative ways approach problems passionate social consumer applications travel detail oriented communicates clearly writing conversation years experience integrating variety consumer apis comfortable working packer terraform update dependencies experience nlp named entity recognition machine learning php rust react swift experience building browser plugins scratch experience mechanical turk similar marketplaces competitive salary equity healthcare flexible hours unlimited vacation time", "", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark flink kafka etc building efficient extraction transformation pipelines scale proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth attitude extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "excellent written verbal communication skills curious excellent analytical problem solving skills excited digging massive petabyte scale semi structured datasets years industry experience working distributed data technologies e g hadoop mapreduce spark etc proficiency least one high level programming language python go java scala equivalent experience large complex highly dimensional data sets hands experience sql pragmatic letting \" perfect\" enemy \" good\" self directed capable operating amidst ambiguity humble continually growing self awareness possessing growth mindset extras ' excited experience building stream processing applications using apache flink spark streaming apache storm kafka streams others", "", "career categories", "agile engineering kanban lean hybrid agile experience big plus", "", "", "", "highly expert battle tested lead core contributor data processing projects consistent record designing implementing scalable performant data pipelines data services data products hands position expect write code programming experience building high quality software skills java python scala preferred proficiency hadoop kafka spark mpp sql databases like vertica redshift snowflake large scale environment strong aptitude learning new technologies related data management data science demonstrated ability work well independently within fast paced team oriented environment work noisy dirty unstructured data data cleansing scraping unstructured data converting structured data evaluate benchmark improve scalability robustness efficiency performance big data platform applications experience building reports using tableau microstrategy knowledge engineering machine learning feature engineering systems plus", "", "design build launch extremely efficient reliable data pipelines move data across number platforms including data warehouse online caches real time systems ba bs degree computer science engineering discipline statistics information systems another quantitative field action oriented", "", "", "strong java experience mainly core java strong knowledge oop sql hands google cloud platform dataflow bigquery pub sub hadoop echosystem experience hbase hive spark commercial experience linux messagedriven architecture experience using activemq kafka similar track record developing technology enable large scale data transformation etl data load process development experience dealing large complex data sets development testing best practices maven git familiar nosql technologies embrace agile software engineering practices tools tdd continuous integration etc understanding software development lifecycle agile software design principles build processes excellent organisation communication interpersonal skills ability work collaboratively limited supervision deliver results within set deadlines bachelor java years", "travel", "", "", "", "identify data sources add value decision making work source system owners analysts understand source data e g data profiling definition mapping design implement efficient data loads using traditional structured data etl techniques design implement real time near real time data load solutions using technologies like data streaming design implement unstructured data loads e g text speech images video design implement load monitoring tools procedures perform continuous monitoring optimising loads work analysts architect design implement effective efficient data models using appropriate modelling techniques design implement data warehouse data models design implement data pipelines ad hoc unstructured data models design implement appropriate aggregation data structures enhance usability data e g multi dimensional olap structures summary tables etc design implement maintain appropriate indexing tables enhance speed access design implement data models support automated decision making analytics continuously search data elements sources enhance existing data objects supplement enhance context design implement interfaces data access e g batch exports real time decision api' etc design implement interface monitoring management solutions ensure availability accuracy design implement data monitoring solutions procedures continuously monitor maintain integrity existing environment troubleshoot technical data issues make appropriate changes required design implement meta data solutions assist understanding managing data work together business owners analysts maintain good data governance work together business owners analysts manage changes data organisation provide technical data related support source system teams external parties exchange data manage data growth usage implementing effective strategies e g archiving indexing manage systems technology tools enable data management analytics liaise infrastructure operations regarding system infrastructure management take ownership work delivering high quality work time show initiative pre active finding opportunities improve data processes take ownership career development continuously improving skills knowledge application thereof designing implementing solutions positive engagement team activities actively contribute ideas improve team dynamics performance complex solution service design implementation cross functional data team knowledge gathering sharing responsible team activities team dynamics performance manage project task delivery team multiple stakeholder management internal external assist development others e g mentoring knowledge share quality control ' work degree information technology engineering mathematics statistics actuarial related discipline least years' experience working data business intelligence analytics environment sql data analysis data visualisation data modelling microsoft business intelligence data technologies ssis ssas sql server data warehouse concepts best practices information gathering problem analysis applying professional specialist technical expertise creating innovating quality detail orientation planning organizing presenting communicating information analysing leadership", "", "", "open office environment ideas flow among marketers developers product managers support reps sit shoulder shoulder collaborating challenging encouraging", "", "", "advanced working sql knowledge experience working relational databases query authoring sql well working familiarity variety databases several years experience building optimizing data pipelines architectures data sets experience performing root cause analysis internal external data processes answer specific business questions identify opportunities improvement strong analytic skills related working unstructured datasets build processes supporting data transformation data structures metadata dependency workload management working knowledge message queuing stream processing highly scalable data stores experience leading cross functional teams dynamic environment experience using following software tools years experience big data tools hadoop spark kafka etc years experience relational sql nosql databases including postgres experience data pipeline workflow management tools years experience aws cloud services ec2 emr rds redshift experience stream processing systems comprehensive medical dental vision package provided empire blue cross blue shield life disability insurance also included enrollment 401k plans enrolled plan change contributions rate opt time stock option program eight company paid holidays calendar year paid time pto uncapped accordance company' policy outlined new hire orientation", "", "robust perks - generous pto 401k contributions tuition assistance entertainment discounts", "", "", "", "vibrancy wellness program yoga fitness classes onsite massage volunteer opportunities company happy hours product demos outings", "", "", "years experience data engineer role preferred bachelor graduate degree computer science statistics informatics information systems another quantitative field commensurate experience experience using following software tools intermediate development experience one following java python scala experience relational sql nosql databases strong knowledge database systems general foreign keys indexes basic dba tasks experience big data tools intermediate advanced knowledge python emr spark lambda plus working knowledge scripting language bash python powershell familiarity linux unix environment experience data pipeline workflow management tools plus experience stream processing systems plus willingness roll sleeves analyst work purposes data exploration assessment demonstrated experience measuring handling large data sets relational databases communication collaboration project management skills ability distill complex subjects wider audience creativity flexibility entrepreneurial mindset solution business issues sound business judgment ability meet exceed deadlines demonstrated sense urgency clear concise ability communicate complex concepts english ownership--performance unit grants peer recognition awards employee referral bonus program 401k company match free access office health club oakbrook chicago employees discounted gym memberships bluecross blueshield illinois medical prescription drug insurance monthly premiums paid employee coverage dental insurance monthly premiums paid employee coverage vision insurance monthly premiums paid employee coverage provided cost employees group term life insurance long term disability accidental death personal loss insurance flexible spending accounts health care dependent care commuter benefits identity protection insurance voluntary term life insurance group universal life insurance accident critical illness insurance paid time vacation illness maternity paternity leave corporate sponsored activities events year round", "", "", "", "", "work phenomenal team individuals constantly pushing market boundaries offer healthy food utmost convenience", "previously healthcare experience highly desirable", "", "", "", "", "distributed computing principles legacy modern database architectures hadoop based technologies e g mapreduce hive pig sql based technologies e g postgresql mysql nosql technologies e g cassandra mongodb stream processing systems e g storm spark streaming etl tools apis optimizing data storage retrieval specific use cases testing validating accuracy data transformations cloud computing architectures preferably specific expertise microsoft azure aws programming multiple programming languages including python java creative problem solving sensitive available time resource constraints effective listening communication project managing", "", "sql server sql ssis stored procedures user defined functions table functions managing design risk software development lifecycles unit test techniques debugging analytical techniques help career growth joining industry leader continuing advance echo web based technologies working organization defined market goals products customers revenue development teams experienced mentors learn adopt new practices ability introduce views takes product offerings work wide variety data management ability constantly enhance improve applications clearly defined career growth track enough flexibility pave way", "opportunity work smart people challenging problems", "", "bachelor' degree computer science related field years experience data platform administration engineering", "strong knowledge sql python bash expert database design experience git general knowledege linux server administration bs ms computer science engineering experience google cloud platform experience spark hadoop stack interest data science machine learning career rapidly growing official employment according labor code russian federation \"white salary\" flexible working day start working days mn fr comfortable cosy office friendly employees corporate education trainings seminars conferences fitness compensation medical insurance", "", "", "extensive experience sql ruby web services big data skills appreciated experience testing multi tier consumer facing web applications ui level experience testing batch streaming etl processes using spark beam etc parsing analysis free form fixed form data sets good problem solving debugging skills comfortable working agile development environment experience executing api tests broad experience designing maintaining automated tests whitebox blackbox testing experience unit testing frameworks rspec preferred minitest test unit similar experience libraries used implement browser automation watir preferred selenium capybara etc knowledge best practices software development life cycle sldc working knowledge jira issues project tracking software experience git distributed revision control source code management systems background payments desirable required experience platforms non functional requirements operating systems experience performance testing security testing", "excellent understanding manipulation analysis large complex data sets", "", "", "", "modern tech tools hi tech equipment", "", "employee referral bonuses ' get opportunity work friends get extra cash pocket", "commitment open inclusive diverse work culture", "upper intermediate knowledge english proficiency r java python numpy scipy matplotlib pandas sklearn hands experience kafka activemq rabbitmq kestrel message brokers cassandra understanding hadoop mapreduce mpp systems apache storm spark amazon aws knowledge one several newsql solutions drawntoscale voltdb splicemachine sqlfire impala redshift clustrix nuodb hadapt skills analysis multi dimensional datasets production performance experience predictive analytics statistical analysis solutions development data mining knowledge machine learning k nn naive bayes svm gbm etc artificial intelligence ai experience common data science toolkits r weka scikit learn pandas proficiency nosql strongly desired proficiency data lakes enterprise data models hands experience digital twins production processes proficient linux relational database design methods efficiently retrieving data degree applied mathematics computer science physics comparable fields solid experience custom etl design implementation maintenance great business sense understanding business metrics communication skills experience relevant positions strong plus knowledge scrum agile waterfall methodologies ability handle situations numerous unknowns set questions tasks little guidance willingness dive deep unstructured material find answer yet unknown question ability clearly communicate findings orally written visually come suggestions advices able work fast paced environment grafik raboty ofitsial'noe trudoustroistvo cotspaket usloviia po zarabotnoi plate budut ustanovleny po rezul'tatam sobesedovaniia", "", "", "", "bachelors degree computer engineering computer science related discipline masters degree preferred years etl design development performance tuning microsoft ssis sql server preferable multi dimensional data warehousing environment years ssas design development maintenance performance tuning microsoft sql server preferable expert mdx dax skills years advanced sql programming pl sql sql u sql years enterprise data analytics solution architecture years power bi experience including mobile solutions years strong extensive hands experience azure preferably data heavy analytics applications leveraging relational nosql databases data warehouse big data experience azure data lake azure sql data warehouse data catalog azure analysis services data bricks storage account gen2 azure sql database azure dns virtual network documentdb azure app service data factory experience big data technologies hadoop sqoop hive kafka spark pyspark python scala pig experience big data management bdm relational non relational data formats like json xml avro parquet copybook etc experience setting operating data pipelines using python sql strong analytical abilities strong intellectual curiosity", "", "ability work large amounts data experience map reduce frameworks hive hadoop spark elastic map reduce would plus strong grasp statistics data modelling designing algorithms cloud service credits aws google cloud azure digital ocean public transit allowance mobile data allowance flex days tea coffee bar office snacks", "hackathons", "", "", "", "analysing problem domains identify entities data flows designing implementing efficient scalable data models automating provisioning management data platforms familiarity rdbms nosql distributed data technologies end end involvement software delivery working production systems ownership product set features within product range software delivery tools source control agile tools ci etc implementing following best practices effective prioritisation tasks personal time management producing estimates self others demonstrating initiative coaching mentoring junior team members interacting clients product owners degree computer science related discipline delivering highly collaborative agile environment understanding blockchain technologies", "", "", "", "bachelor' degree field computer science mathematics data architecture equivalent experience skills strong development knowledge experience key languages environments python scala sql nosql databases advanced data sourcing content management skills experience time task management strong attention detail good written verbal communication skills including ability collaborate across teams experience mentoring junior team members master' degree computer science technical field strong development knowledge experience hadoop big data stack e g hdfs hive spark etc demonstrated capability learn new technologies follow industry trends", "", "", "", "", "", "", "", "work data science teams deliver metrics consumers", "", "", "phd related discipline least years experience master' degree years experience bachelor' degree experience biomedical data management data engineering quality assurance assay development specimen data management related discipline demonstrated proficiency molecular biology concepts ability support develop deploy laboratory research data management processes procedures apply complex high dimensional data sets demonstrated ability understand translate high level scientific datasets results data curation management strategies underlying structures curation processes infrastructure required strong understanding lims systems systematic relational approaches data integration data processing workflows familiarity amazon web services aws excellent skills r programming experience additional computer languages perl python php plus java c c extensive practical experience working diverse highly connected scientific knowledge collections query interfaces enable research hypotheses around compound targets mechanisms action patient response proven ability work team environment clinical personnel study monitors computational biologists biostatisticians programmers medical writers knowledge fda ich guidelines industry standard practices regarding data management helpful required detailed knowledge experience case report form design central laboratories programming databases query resolution data validation computer skills detailed knowledge least one data management system oracle clinical clintrial preferred experience sas data sets conversion procedures required knowledge ms office program suite required knowledge distributed database design implementation lamp mysql etc capability perform direct assess implementation databases working knowledge windows linux operating systems required along programming proficiency must creativity show strong capacity independent thinking ability grasp underlying biological questions must thrive complex dynamic environment adapting dynamically changing priorities must excellent time management organizational skills", "years experience software engineering related field extensive experience working stack python postgresql aws something similar ba bs degree computer science engineering degree ability quickly learn understand work new emerging technologies methodologies solutions experience building managing production quality ml models plus growth company dna - thrives fast paced environment shares excitement lead change", "phd related discipline least years experience master' degree years experience bachelor' degree experience biomedical data management data engineering quality assurance assay development specimen data management related discipline demonstrated proficiency molecular biology concepts ability support develop deploy laboratory research data management processes procedures apply complex high dimensional data sets demonstrated ability understand translate high level scientific datasets results data curation management strategies underlying structures curation processes infrastructure required strong understanding lims systems systematic relational approaches data integration data processing workflows familiarity amazon web services aws excellent skills r programming experience additional computer languages perl python php plus java c c extensive practical experience working diverse highly connected scientific knowledge collections query interfaces enable research hypotheses around compound targets mechanisms action patient response proven ability work team environment clinical personnel study monitors computational biologists biostatisticians programmers medical writers knowledge fda ich guidelines industry standard practices regarding data management helpful required detailed knowledge experience case report form design central laboratories programming databases query resolution data validation computer skills detailed knowledge least one data management system oracle clinical clintrial preferred experience sas data sets conversion procedures required knowledge ms office program suite required knowledge distributed database design implementation lamp mysql etc capability perform direct assess implementation databases working knowledge windows linux operating systems required along programming proficiency must creativity show strong capacity independent thinking ability grasp underlying biological questions must thrive complex dynamic environment adapting dynamically changing priorities must excellent time management organizational skills", "experience integration data multiple data sources", "healthy wellness programs competitive medical benefit offerings happy - recognition programs confidential employee assistance program perkspot employee discount program potentially flexible work arrangements staggered start times enriched - tuition reimbursement training learning programs leadership development opportunities", "", "", "", "bs ba ms plus computer science information systems engineering related field years experience equivalent training experience working enterprise cloud database systems like mysql mongodb oracle mssql hadoop depth knowledge mac os x system strong proficiency php preferable years hands experience fluency bash python scripting languages significant automation experience proficient scripting languages php python ruby rails nodejs java etc experience designing developing programming api solutions integration various client end points including native applications desire build lead rapid development team project definition scoping estimating planning development testing launch strong preference someone done date current industry trends third party integration open source tools regard database design integration data storage analysis security implementation strong analytical skills excellent work ethic meticulous attention detail", "", "", "bachelors degree computer engineering computer science related discipline masters degree preferred years etl design development performance tuning microsoft ssis sql server preferable multi dimensional data warehousing environment years ssas design development maintenance performance tuning microsoft sql server preferable expert mdx dax skills years advanced sql programming pl sql sql u sql years enterprise data analytics solution architecture years power bi experience including mobile solutions years strong extensive hands experience azure preferably data heavy analytics applications leveraging relational nosql databases data warehouse big data experience azure data lake azure sql data warehouse data catalog azure analysis services data bricks storage account gen2 azure sql database azure dns virtual network documentdb azure app service data factory experience big data technologies hadoop sqoop hive kafka spark pyspark python scala pig experience big data management bdm relational non relational data formats like json xml avro parquet copybook etc experience setting operating data pipelines using python sql strong analytical abilities strong intellectual curiosity", "", "phd related discipline least years experience master' degree years experience bachelor' degree experience biomedical data management data engineering quality assurance assay development specimen data management related discipline demonstrated proficiency molecular biology concepts ability support develop deploy laboratory research data management processes procedures apply complex high dimensional data sets demonstrated ability understand translate high level scientific datasets results data curation management strategies underlying structures curation processes infrastructure required strong understanding lims systems systematic relational approaches data integration data processing workflows familiarity amazon web services aws excellent skills r programming experience additional computer languages perl python php plus java c c extensive practical experience working diverse highly connected scientific knowledge collections query interfaces enable research hypotheses around compound targets mechanisms action patient response proven ability work team environment clinical personnel study monitors computational biologists biostatisticians programmers medical writers knowledge fda ich guidelines industry standard practices regarding data management helpful required detailed knowledge experience case report form design central laboratories programming databases query resolution data validation computer skills detailed knowledge least one data management system oracle clinical clintrial preferred experience sas data sets conversion procedures required knowledge ms office program suite required knowledge distributed database design implementation lamp mysql etc capability perform direct assess implementation databases working knowledge windows linux operating systems required along programming proficiency must creativity show strong capacity independent thinking ability grasp underlying biological questions must thrive complex dynamic environment adapting dynamically changing priorities must excellent time management organizational skills", "", "", "", "", "", "", "", "", "live gohealth culture ensure represented within team design develop deploy optimal extraction transformation loading data various gohealth external data sources monitor execute report data pipeline tasks working appropriate teams take corrective action quickly case issues perform unit testing system integration testing assist user acceptance testing adapt data components accommodate changes source data new business requirements create maintain documentation technical detail design operational support maintenance procedures data pipeline tasks ensure data quality compliance development architecture reporting regulatory standards throughout entire data pipeline collaborate rest data engineering team subject matter experts department leaders understand analyze build deliver new data related processes reports ability work rest data engineering team cross train provide support various data engineering tasks bachelor' degree computer science equivalent experience required years experience design development data pipelines tasks strong analytical problem solving ability strong attention detail accuracy good understanding data warehousing concepts dimensional data modeling hands experience troubleshooting performance issues fine tuning sql queries experience python including modules libraries pandas numpy flask scikit learn sci py proven experience extracting data structured data sources sql excel csv files couchbase unstructured data sources splunk log files premise cloud experience consuming data web services rest soap html xml json knowledge version control systems using git bitbucket svn team foundation ability handle multiple tasks adapt evolving business technical environments self starter ability work independently take initiative learn new skills excellent written oral communication skills ability articulate complex processes individuals varying technical abilities experience software engineering practices required experience microsoft sql server ssis ssrs power bi azure preferred required familiar data warehouse platforms like aws redshift aws data pipeline join team daily meeting includes data engineers data scientists data analysts review open git pull requests jira tickets airflow dag runs work data engineering team various teams following analyzing designing implementing airflow dags operators restful services integration external internal restful services e g hubspot five9 etc troubleshooting issues resolving - airflow ssis tableau sql server ssrs aws mysql couchbase etc design implement evolve gohealth data pipelines tableau data sources data extracts build test automation jenkins python bash gradle etc participate lead demo wednesdays release thursdays join weekly data engineering team meeting review roadmap progress projects open vacation policy 401k match program medical life dental vision benefits flexible spending accounts subsidized gym memberships commuter transit benefits professional growth opportunities casual dress code generous employee referral bonuses happy hours ping pong tournaments company sponsored events gohealth equal opportunity employer", "track record successfully executing projects multiple partners", "", "", "", "knowledge sharing activities", "years data engineering backend development experience python experience technologies like hadoop spark hive presto nifi luigi aws experience preferred data modeling experience preferred scala java experience preferred ' pragmatic engineer help execute also provide strong voice technological direction systems avant passion data empowering business excited evaluating automating new technologies ' entrepreneurial self driven take pride improving user' experiences strong knowledge software engineering data fundamentals well devops best practices thrive collaborative environment involving different stakeholders subject matter experts enjoys working diverse group people different expertise", "bachelor' degree relevant discipline least years ' experience master' degree least years' experience phd least years' experience biomedical data management assay development specimen data management related discipline demonstrated proficiency molecular biology assay concepts ability support develop deploy laboratory research data management processes procedures apply complex high dimensional data sets extensive practical experience curating working diverse highly connected scientific knowledge collections query interfaces enable research hypotheses around compound targets mechanisms action patient response demonstrated ability understand translate high level scientific datasets results data curation management strategies proven ability work team environment clinical personnel operational personnel study monitors computational biologists biostatisticians programmers medical writers demonstrated proficiency current software engineering methodologies agile source control project management issue tracking working knowledge cloud computing preference given candidates aws experience working knowledge rest apis container strategies strongly preferred knowledge distributed database design implementation lamp mysql etc capability perform direct assess implementation databases strong understanding lims systems systematic relational approaches data integration data processing workflows excellent skills r programming experience additional computer languages perl python php plus java c c experience producing visualization data sets eg r shiny spotfire etc working knowledge windows linux operating systems required along programming proficiency must creativity show strong capacity independent thinking ability grasp underlying biological questions must thrive complex dynamic environment adapting dynamically changing priorities must excellent written verbal communication presentation skills must excellent time management organizational skills", "ingestion pipelines etl using technologies like kafka apache spark experience big data technologies hadoop kafka akka mesos similar highly desirable microservices design implementation rest json must experience elasticsearch highly desirable experience semantic web rdf owl sparql linked data highly desirable experience large scale production databases experience graph database highly desirable neo4j neptune experience commercial search engines may advantageous pair programming experience scrum agile experience kanban agile experience jira experience release search applications cloud environment ideally blue green deployments experience working across matrixed distributed international organization aws system administration release processes dev ops", "", "", "proficient understanding distributed computing principles management hadoop cluster included services unless going specific big data devops roles ability solve ongoing issues operating cluster unless going specific big data devops roles proficiency hadoop v2 mapreduce hdfs experience building stream processing systems using solutions storm spark streaming stream processing relevant role good knowledge big data querying tools pig hive impala experience spark including planning include experience integration data multiple data sources experience nosql databases hbase cassandra mongodb knowledge various etl techniques frameworks flume experience various messaging systems kafka rabbitmq experience big data ml toolkits mahout sparkml h2o going integrate machine learning big data infrastructure good understanding lambda architecture along advantages drawbacks experience cloudera mapr hortonworks specify distribution currently using planning use list technologies using planning use big data engineers know ones listed hadoop ecosystem table list education level certification require", "hadoop spark google cloud dataflow kafka kinesis aws google pub sub gcp elasticsearch bigquery distributed sql nossql databases python scala golang r year experience provisioned demand cloud computing platforms gcp aws azure year experience standard development tooling e g git jira etc knowledge distributed computing fundamentals ability design scalability linux platforms experience machine learning algorithms libraries e g scikit learn bachelor' degree relevant technical field computer science mathematics statistics similarly relevant engineering computational discipline data analytics modeling experience strong mathematical statistical analysis skills", "", "", "years experience data engineering constructing maintaining databases data pipelines year experience creating supporting production databases year experience working data lake environment experience collecting transforming storing large amounts data bachelor' degree experience working data engineering bachelor' degree master' degree computer science field computer science information sciences informatics experience creating data pipelines machine learning data management certifications strong knowledge sql nosql database technologies strong knowledge least one scripting language java python knowledge hadoop spark big data processing frameworks submit staff vacancy application submit voluntary self identification disability forms upload cover letter resume months years employment must included academic credentials unofficial transcripts diploma may acceptable names contact information three references", "", "relevant degree work experience", "", "", "", "full time exempt position", "", "", "", "", "", "bonus points bring real world experience aws emr e2 kinesis s3", "distributed computing principles legacy modern database architectures hadoop based technologies e g mapreduce hive pig sql based technologies e g postgresql mysql nosql technologies e g cassandra mongodb stream processing systems e g storm spark streaming etl tools apis optimizing data storage retrieval specific use cases testing validating accuracy data transformations cloud computing architectures preferably specific expertise microsoft azure aws creative problem solving sensitive available time resource constraints effective listening communication", "demonstrate strong understanding development processes agile methodologies", "", "undergraduate graduate degree computer science similar technical field sound understanding statistics years industry experience data engineer hands experience etl written data pipelines either spark mapreduce sound understanding sql cql worked data lakes s3 hdfs worked various databases postgres cassandra redshift understand pros cons working knowledge following technologies afraid picking fly mesos chronos cron marathon jenkins fluent least one scripting language preferably nodejs python one compiled language scala java c great communication skills ability work others", "", "", "", "", "data serialization json avro parquet", "building cube cube like products", "", "", "", "", "cluster managers eg docker apache mesos kubernetes", "401k retirement savings plan", "", "", "", "", "", "daily catered lunches la' best restaurants fully stocked kitchen", "indemnity according profile", "ability work successfully 3rd party vendors support application enhancements troubleshoot problems required meet business processes priorities", "", "excellent experience amazon web services aws strong coding skills python java demonstrable experience messaging queue technology apache kafka similar hands experience real time systems production stage years post academic experience data engineering capacity experience docker apache spark elasticsearch", "google analytics", "", "agile scrum working practices", "", "", "bachelors years experience masters degree years experience experience proficiency scala experience proficiency spark deep understanding machine learning interest applying scale experience data cleaning preparation feature building selection techniques experience working large data sets solve problems effective communication interpersonal teamwork skills ability handle multiple concurrent projects working independently teams ability work fast paced deadline driven environment experience hierarchical models random effect models online learning", "", "", "", "performs duties assigned", "", "", "", "", "", "bachelor degree years information technology experience technical certification college courses years information technology experience years information technology experience proficiency domain driven design domain modeling experience nosql solutions gemfire cassandra hbase distributed caching experience relational database system limited mysql sql server oracle postgresql communicate written verbal form effectively ci cd tools jenkins concourse ansible ca", "", "persons employed baltimore county public schools regular temporary required fingerprinted criminal background investigation state maryland senate bill effective october completed fee charged fingerprinting identification card issued must shown prior employment anyone offered employment required provide proper identification documentation eligibility employment us military experience asked provide copy dd214 official transcripts higher education must received prior contract signing positions require employees undergo physical examination drug testing newly hired personnel must attend badges benefits session additional job verification required salary credit", "", "fluent english years experience working java scala object oriented language strong object oriented experience true passion writing high quality code experience quick prototyping ability work dynamic environment experience nosql databases mongodb etc. . . experience api based architecture experience distributed framework parallel processing hadoop pig spark hive strong experience java scala plus experience large scale data distributed systems ability work independently strong part team experience real time applications plus attractive compensation package including equity real impact awesome work environment company huge vision right start success story relaxed fun work environment flexible working hours extras like catered lunch gym membership private health insurance. . .", "good understanding adherence data security standards", "", "", "years experience least one following google cloud dataflow google pubsub elasticsearch spark following plus hadoop kafka kinesis bigquery distributed sql nossql databases years experience least one following python golang clojure r year experience provisioned demand cloud computing platforms gcp aws azure year experience standard development tooling e g git jira etc knowledge distributed computing fundamentals ability design scalability linux platforms", "google analytics", "", "", "", "", "", "", "current tech stack includes python go aws spark graphdb docker kubernetes jenkins json git extensive experience developing data solutions aws experience building data vision strategy together upper management previous experience within fintech related area plus", "", "", "", "", "", "", "", "", "workplace recognized best consumer web company built chicago top company culture entrepreneur top workplace chicago tribune one chicago' best places work women crain' chicago business", "power bi skills", "excellent experience amazon web services aws strong coding skills python java demonstrable experience messaging queue technology apache kafka similar hands experience real time systems production stage years post academic experience data engineering capacity experience docker apache spark elasticsearch", "experience http rest ssl identity authentication", "", "", "comfortable building maintaining data infrastructure cloud", "", "years", "spontaneous nerf gun wars wake thursday happy hours wind", "", "excellent verbal written communication skills", "experience big data analytics systems hadoop storm spark etc. . . years back end web services programming java php years database work mysql postgres redshift experience using amazon web services plus experience scalable systems load balanced environment plus b computer science object oriented programming python java database technologies redshift postgres spark presto amazon web services s3 sqs kinesis ecs ecr emr", "bachelor degree mis computer science related field accredited college university equivalent year developing etl processes using enterprise tools sap data services talend informatica year working relational databases developing complex sql experience cloud native databases development tools familiarity memory databases sap hana proven ability collaborate cross functional teams system availability data availability data quality", "", "advanced degree computer science information systems closely related field years hands experience software engineering infrastructure role advanced knowledge hadoop stack prior experience hive pig hbase impala sqoop advanced knowledge object oriented programming distributed systems software design principles advanced knowledge database maintenance administration using ms sql server strong programming experience java python r hands experience microsoft azure amazon ec2 cloud platform demonstrated ability design implement etl workflows across windows linux environments highly motivated individual ability work effectively people across levels organization position requires ability travel nice programming experience c sas javascript experienced rdbms systems like oracle database ibm db2 mysql experienced nosql systems like mongodb redis cassandra experienced apache spark", "", "", "", "share success given hard promote change industry celebrate success along way", "", "google analytics", "experienced data modeling etl development data warehousing experience various aws services including rds redshift s3 experience consuming cleaning data third party apis sources experience big data technologies hive hadoop spark strong organizational analytical skills ability extract meaning data strong communication skills interact stakeholders customers strong experience python similar languages plus", "", "", "", "inspire offers competitive compensation equity packages plus benefits health vision life dental insurance mention unlimited vacation k plan lots cupcakes", "", "collaborative nature entrepreneurial spirit prior startup experience huge plus", "", "", "", "", "", "excited hear", "mssql server sql query microsoft excel embrace vba project develop internal tools utilities set unit test strategy develop improve code performance participate innovation process vba language microsoft excel unit test strategy object oriented programming software versioning revision control svn familiarity windows mac os cross platform development continuous integration environment jenkins embrace c project visual studio xcode develop internal tools utilities set unit test strategy develop improve c architecture performance lead statistical developers improving c code participate innovation process oop c c visual studio xcode higher software versioning revision control svn c unit test strategy hpc multi threading com object familiarity design pattern continuous integration environment jenkins m2 office space doctors data scientists developers specializing mathematics statistics people total spread floors place xlstat www xlstat com addinsoft www addinsoft com", "", "", "", "bachelor' degree accredited college university computer science computer engineering engineering related field seven years experience master' degree five years experience phd two years experience fluency several programming languages python scala java ability pick new languages technologies quickly understanding cloud distributed systems principles including load balancing networks scaling memory vs disk experience large scale big data methods mapreduce hadoop spark hive impala storm strong written verbal communication skills ability work dynamic team environments multi task effectively", "google analytics", "", "", "", "", "excited hear", "", "least three years working data engineer ideally involved setting data infrastructure storage cloud even better gcp experience building production level python applications data modelling using sql comfort delivering automated reports data visualisation solutions using tools like tableau 'full stack ness' comes play autonomous mindset given mandate design implement modern data solutions choosing challenge", "", "li pa1", "", "working truly global company offices countries", "monthly team outings ball games happy hours hikes etc", "", "", "least years experience developing business intelligence solutions using tableau experience tools ssrs cognos microstrategy qlikview spotfire etc plus familiarity least two different database platforms teradata oracle ms sql server platforms teradata ms sql strongly preferred extremely strong sql skills dimensional data modeling skills hands experience etl development plus excellent interpersonal team management facilitation communication skills must able communicate effectively levels client organization competitive salary performance based bonus opportunities single family health insurance plans including dental coverage short term long term disability matching k competitive paid time training certification opportunities eligible expense reimbursement team building social activities mentor program help develop career", ""], "meta": ["www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.google.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.google.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.nature.com", "bit.ly", "www.nature.com", "www.nature.com", "www.nature.com", "www.facebook.com", "www.linkedin.com", "www.google.com", "www.nature.com", "www.linkedin.com", "www.nature.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.nature.com", "www.google.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.nature.com", "www.nature.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.nature.com", "bit.ly", "www.linkedin.com", "www.nature.com", "www.nature.com", "www.nature.com", "www.nature.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.linkedin.com", "www.linkedin.com", "www.nature.com", "www.google.com", "www.mediabistro.com", "www.mediabistro.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.dlr.de", "www.glassdoor.com", "indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.accenture.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.glassdoor.com", "www.indeed.com", "www.dlr.de", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.cc.gatech.edu", "www.quora.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.verizon.com", "www.quora.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.glassdoor.com", "www.indeed.com", "www.accenture.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.accenture.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.mediabistro.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "lists.demog.berkeley.edu", "www.usajobs.gov", "militaryjobs.homedepot.com", "www.indeed.com", "www.usajobs.gov", "www.quora.com", "lists.demog.berkeley.edu", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.ssrn.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.glassdoor.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.verizon.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.mediabistro.com", "militaryjobs.homedepot.com", "www.quora.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.verizon.com", "cldb.ling.washington.edu", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "cldb.ling.washington.edu", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.usajobs.gov", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.verizon.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.quora.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.mediabistro.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.quora.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.glassdoor.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.glassdoor.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "cldb.ling.washington.edu", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "cldb.ling.washington.edu", "www.indeed.com", "www.accenture.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.glassdoor.com", "militaryjobs.homedepot.com", "cldb.ling.washington.edu", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.glassdoor.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.quora.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.accenture.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.llnl.gov", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "militaryjobs.homedepot.com", "www.yelp.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.mediabistro.com", "www.indeed.com", "www.glassdoor.com", "www.airbnb.com", "www.indeed.com", "www.indeed.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.verizon.com", "www.indeed.com", "www.llnl.gov", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.mckinsey.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.quora.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.mediabistro.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "lists.demog.berkeley.edu", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.mediabistro.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.quora.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.quora.com", "militaryjobs.homedepot.com", "www.llnl.gov", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.cc.gatech.edu", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.quora.com", "www.indeed.com", "www.quora.com", "www.indeed.com", "www.glassdoor.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "cldb.ling.washington.edu", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.verizon.com", "stackoverflow.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.cc.gatech.edu", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.quora.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.verizon.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.usajobs.gov", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.sitepoint.com", "www.verizon.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "stackoverflow.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.cc.gatech.edu", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.excite.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.mediabistro.com", "www.llnl.gov", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.llnl.gov", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.verizon.com", "militaryjobs.homedepot.com", "indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.glassdoor.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.quora.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.meetup.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.ucl.ac.uk", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "lists.demog.berkeley.edu", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.glassdoor.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.verizon.com", "www.indeed.com", "www.ucl.ac.uk", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "lists.demog.berkeley.edu", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.accenture.com", "www.indeed.com", "stackoverflow.com", "www.verizon.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.quora.com", "www.indeed.com", "www.quora.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.bath.ac.uk", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.sitepoint.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.quora.com", "www.monster.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.dlr.de", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "cldb.ling.washington.edu", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.mediabistro.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "stackoverflow.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "japan.careers.vmware.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "indeed.com", "cldb.ling.washington.edu", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "fr-jobs.about.ikea.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "militaryjobs.homedepot.com", "www.mediabistro.com", "www.mediabistro.com", "stackoverflow.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "karriere.nzz.ch", "www.quora.com", "www.verizon.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.glassdoor.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.verizon.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.verizon.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "cldb.ling.washington.edu", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.mediabistro.com", "www.indeed.com", "www.indeed.com", "www.indeed.com", "www.cc.gatech.edu", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "www.linkedin.com", "stackoverflow.com", "www.sitepoint.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.verizon.com", "militaryjobs.homedepot.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "japan.careers.vmware.com", "www.indeed.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "stackoverflow.com", "www.monster.com", "www.mediabistro.com", "www.sophos.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "www.sophos.com", "militaryjobs.homedepot.com", "www.indeed.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.wix.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.quora.com", "www.accenture.com", "militaryjobs.homedepot.com", "www.cc.gatech.edu", "stackoverflow.com", "www.wix.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.quora.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.sophos.com", "stackoverflow.com", "www.sophos.com", "militaryjobs.homedepot.com", "stackoverflow.com", "derstandard.at", "www.indeed.com", "stackoverflow.com", "www.sitepoint.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "www.verizon.com", "www.indeed.com", "stackoverflow.com", "www.sophos.com", "militaryjobs.homedepot.com", "www.glassdoor.com", "www.glassdoor.com", "nbacareers.nba.com", "militaryjobs.homedepot.com", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.indeed.com", "th-jobs.about.ikea.com", "stackoverflow.com", "www.sophos.com", "militaryjobs.homedepot.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.indeed.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.glassdoor.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "www.accenture.com", "militaryjobs.homedepot.com", "www.accenture.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.quora.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.accenture.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "japan.careers.vmware.com", "www.sophos.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.accenture.com", "www.wix.com", "www.glassdoor.com", "militaryjobs.homedepot.com", "www.mediabistro.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "japan.careers.vmware.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.quora.com", "www.sitepoint.com", "www.sitepoint.com", "stackoverflow.com", "www.mediabistro.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.indeed.com", "stackoverflow.com", "stackoverflow.com", "remoteok.io", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.mediabistro.com", "militaryjobs.homedepot.com", "www.sophos.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.sitepoint.com", "www.accenture.com", "www.verizon.com", "www.mediabistro.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.indeed.com", "www.wix.com", "www.indeed.com", "cldb.ling.washington.edu", "militaryjobs.homedepot.com", "stackoverflow.com", "www.idealist.org", "stackoverflow.com", "stackoverflow.com", "www.indeed.com", "stackoverflow.com", "militaryjobs.homedepot.com", "stackoverflow.com", "militaryjobs.homedepot.com", "www.glassdoor.com", "stackoverflow.com", "www.cia.gov", "militaryjobs.homedepot.com", "stackoverflow.com", "www.verizon.com", "dfwishiring.dallasnews.com", "stackoverflow.com", "stackoverflow.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "www.indeed.com", "militaryjobs.homedepot.com", "militaryjobs.homedepot.com", "stackoverflow.com", "stackoverflow.com", "www.accenture.com", "stackoverflow.com", "www.indeed.com", "militaryjobs.homedepot.com", "www.sophos.com", "www.indeed.com", "militaryjobs.homedepot.com", "stackoverflow.com", "www.careercast.com", "www.biospace.com", "www.gfk.com", "www.geekwire.com", "www.snagajob.com", "zapier.com", "www.themuse.com", "www.gene.com", "www.governmentjobs.com", "www.internships.com", "www.kdnuggets.com", "www.standardmedia.co.ke", "www.themuse.com", "www.finn.no", "tweakers.net", "www.insurancejournal.com", "www.google.com", "www.timesofmalta.com", "www.snagajob.com", "www.cdp.net", "careers.allstate.com", "angel.co", "www.devex.com", "www.biospace.com", "www.insurancejournal.com", "www.biospace.com", "www.biospace.com", "www.f6s.com", "www.careercast.com", "www.careercast.com", "www.rigzone.com", "www.careerjet.com", "www.themuse.com", "www.careercast.com", "www.careercast.com", "www.snagajob.com", "www.devex.com", "www.careercast.com", "www.careercast.com", "usa.visa.com", "www.careercast.com", "www.internships.com", "www.f6s.com", "www.devex.com", "www.themuse.com", "www.devex.com", "in.linkedin.com", "www.biospace.com", "www.cio.com.au", "www.careercast.com", "www.geekwire.com", "www.biospace.com", "www.themuse.com", "www.insurancejournal.com", "feedproxy.google.com", "www.cdp.net", "www.careercast.com", "www.devex.com", "www.kdnuggets.com", "www.rigzone.com", "www.careercast.com", "hh.ru", "www.careercast.com", "www.careercast.com", "www.themuse.com", "www.careerjet.com", "www.airweb.org", "www.devex.com", "www.gene.com", "www.ziprecruiter.com", "slack.com", "www.biospace.com", "www.gene.com", "www.insurancejournal.com", "www.careercast.com", "www.careercast.com", "www.biospace.com", "feedproxy.google.com", "www.internships.com", "www.gene.com", "www.internships.com", "www.careercast.com", "www.geekwire.com", "www.biospace.com", "www.biospace.com", "www.cdp.net", "www.f6s.com", "www.careercast.com", "stripe.com", "www.insurancejournal.com", "www.devex.com", "www.biospace.com", "tweakers.net", "hh.ru", "www.insurancejournal.com", "elifesciences.org", "www.devex.com", "www.careercast.com", "www.devex.com", "angel.co", "www.careercast.com", "www.cdp.net", "www.rigzone.com", "angel.co", "www.careercast.com", "www.telekom.com", "angel.co", "www.insurancejournal.com", "www.themuse.com", "www.biospace.com", "www.themuse.com", "www.gene.com", "www.gene.com", "careers.trimble.com", "vtk.ugent.be", "www.f6s.com", "www.dice.com", "feedproxy.google.com", "www.cdp.net", "www.zynga.com", "www.avjobs.com", "newyork.craigslist.org", "www.reed.co.uk", "www.bizcommunity.com", "jobs.apple.com", "www.flexjobs.com", "www.reed.co.uk", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.flexjobs.com", "www.latpro.com", "www.avjobs.com", "www.indeed.co.uk", "www.bizcommunity.com", "www.reed.co.uk", "www.shakeshack.com", "www.computerworld.dk", "www.reed.co.uk", "technical.ly", "www.indeed.co.uk", "www.reed.co.uk", "www.flexjobs.com", "www.reed.co.uk", "www.flexjobs.com", "jobs.apple.com", "www.careerjet.co.uk", "www.reed.co.uk", "www.reed.co.uk", "www.reed.co.uk", "www.ziprecruiter.com", "jobs.apple.com", "technical.ly", "www.reed.co.uk", "www.flexjobs.com", "www.bizcommunity.com", "www.reed.co.uk", "www.reed.co.uk", "www.flexjobs.com", "www.ziprecruiter.com", "www.careerjet.co.uk", "jobs.apple.com", "www.reed.co.uk", "jobs.apple.com", "jobs.apple.com", "www.avjobs.com", "technical.ly", "www.indeed.co.uk", "www.reed.co.uk", "www.smartrecruiters.com", "www.indeed.co.uk", "www.bizcommunity.com", "www.computerworld.co.nz", "www.reed.co.uk", "jobs.apple.com", "www.reed.co.uk", "www.avjobs.com", "jobs.apple.com", "jobs.apple.com", "www.ziprecruiter.com", "www.avjobs.com", "www.reed.co.uk", "www.flexjobs.com", "www.reed.co.uk", "www.nytco.com", "jobs.apple.com", "www.careerbliss.com", "www.reed.co.uk", "www.indeed.co.uk", "www.totaljobs.com", "www.reed.co.uk", "www.careerjet.co.uk", "www.indeed.co.uk", "www.reed.co.uk", "www.flexjobs.com", "www.reed.co.uk", "www.splunk.com", "jobs.apple.com", "www.flexjobs.com", "www.ziprecruiter.com", "jobs.apple.com", "www.reed.co.uk", "www.indeed.co.uk", "www.cs.mcgill.ca", "www.infomine.com", "www.ziprecruiter.com", "www.computerworld.co.nz", "www.reed.co.uk", "www.zynga.com", "www.reed.co.uk", "www.reed.co.uk", "www.gettinghired.com", "www.gettinghired.com", "www.avjobs.com", "www.gettinghired.com", "www.ziprecruiter.com", "www.indeed.co.uk", "optics.org", "jobs.apple.com", "jobs.apple.com", "www.ziprecruiter.com", "www.upwork.com", "www.gettinghired.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.reed.co.uk", "www.cgi.com", "www.internweb.com", "jobs.apple.com", "www.ziprecruiter.com", "www.reed.co.uk", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.indeed.co.uk", "www.flexjobs.com", "www.reed.co.uk", "www.ziprecruiter.com", "jobs.apple.com", "www.flexjobs.com", "www.ziprecruiter.com", "www.bizcommunity.com", "www.reed.co.uk", "www.reed.co.uk", "jobs.apple.com", "www.careerjet.co.uk", "hiring.monster.com", "www.reed.co.uk", "www.gettinghired.com", "www.ziprecruiter.com", "www.flexjobs.com", "www.indeed.co.uk", "www.avjobs.com", "seattle.craigslist.org", "jobs.apple.com", "jobs.apple.com", "www.flexjobs.com", "www.sportsbusinessdaily.com", "jobs.apple.com", "www.reed.co.uk", "www.bizcommunity.com", "jobs.apple.com", "jobs.apple.com", "www.reed.co.uk", "www.ziprecruiter.com", "theoceancleanup.com", "www.gettinghired.com", "www.reed.co.uk", "www.bizcommunity.com", "www.techworld.com.au", "www.avjobs.com", "www.techworld.com.au", "www.reed.co.uk", "jobs.apple.com", "www.computerworld.co.nz", "www.ziprecruiter.com", "www.infomine.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.diglib.org", "www.ziprecruiter.com", "www.ziprecruiter.com", "www.careerjet.co.uk", "jobs.apple.com", "newyork.craigslist.org", "jobs.apple.com", "www.flexjobs.com", "www.careerjet.co.uk", "www.reed.co.uk", "www.reed.co.uk", "jobs.apple.com", "jobs.apple.com", "www.bizcommunity.com", "www.techworld.com.au", "www.avjobs.com", "www.indeed.co.uk", "www.reed.co.uk", "www.bizcommunity.com", "jobs.apple.com", "jobs.apple.com", "www.reed.co.uk", "www.reed.co.uk", "www.reed.co.uk", "www.ziprecruiter.com", "www.reed.co.uk", "jobs.apple.com", "www.flexjobs.com", "www.ziprecruiter.com", "jobs.apple.com", "jobs.apple.com", "technical.ly", "www.reed.co.uk", "www.gettinghired.com", "jobs.apple.com", "www.reed.co.uk", "www.careerjet.co.uk", "technical.ly", "jobs.apple.com", "www.reed.co.uk", "www.reed.co.uk", "www.reed.co.uk", "www.gettinghired.com", "jobs.apple.com", "careers.peopleclick.com", "jobs.apple.com", "www.reed.co.uk", "www.careerjet.co.uk", "www.reed.co.uk", "www.cs.mcgill.ca", "www.reed.co.uk", "jobs.apple.com", "www.ziprecruiter.com", "www.flexjobs.com", "www.reed.co.uk", "jobs.apple.com", "www.reed.co.uk", "www.reed.co.uk", "www.ziprecruiter.com", "www.flexjobs.com", "www.bizcommunity.com", "www.ziprecruiter.com", "www.flexjobs.com", "www.computerworld.dk", "www.efinancialcareers.com", "www.arkansasbusiness.com", "www.computerworld.dk", "jobs.apple.com", "jobs.apple.com", "www.reed.co.uk", "jobs.apple.com", "technical.ly", "www.reed.co.uk", "www.computerworld.co.nz", "www.indeed.co.uk", "jobs.apple.com", "jobs.apple.com", "www.reed.co.uk", "www.flexjobs.com", "www.ziprecruiter.com", "jobs.apple.com", "www.reed.co.uk", "jobs.apple.com", "technical.ly", "jobs.apple.com", "www.reed.co.uk", "www.reed.co.uk", "www.ziprecruiter.com", "www.flexjobs.com", "www.flexjobs.com", "jobs.apple.com", "www.reed.co.uk", "www.smartrecruiters.com", "jobs.apple.com", "jobs.apple.com", "www.zynga.com", "www.infomine.com", "www.jobbnorge.no", "www.reed.co.uk", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.gettinghired.com", "www.reed.co.uk", "www.bizcommunity.com", "www.henkel.com", "jobs.apple.com", "jobs.apple.com", "www.reed.co.uk", "www.reed.co.uk", "www.careerjet.co.uk", "technical.ly", "www.reed.co.uk", "jobs.apple.com", "www.indeed.co.uk", "technical.ly", "www.avjobs.com", "jobs.apple.com", "www.ziprecruiter.com", "www.totaljobs.com", "www.careerjet.co.uk", "www.reed.co.uk", "www.ziprecruiter.com", "www.gettinghired.com", "www.reed.co.uk", "www.entertainmentcareers.net", "www.flexjobs.com", "www.ziprecruiter.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.gettinghired.com", "www.reed.co.uk", "www.flexjobs.com", "jobs.apple.com", "www.flexjobs.com", "www.flexjobs.com", "jobs.apple.com", "www.indeed.co.uk", "www.axa.com", "www.bizcommunity.com", "www.reed.co.uk", "www.ziprecruiter.com", "www.flexjobs.com", "www.reed.co.uk", "www.reed.co.uk", "www.reed.co.uk", "www.reed.co.uk", "www.totaljobs.com", "www.ziprecruiter.com", "www.guru.com", "www.reed.co.uk", "www.cgi.com", "www.flexjobs.com", "www.bizcommunity.com", "www.ziprecruiter.com", "www.reed.co.uk", "www.reed.co.uk", "jobs.apple.com", "www.computerworld.dk", "www.reed.co.uk", "www.ziprecruiter.com", "www.ziprecruiter.com", "www.jobbnorge.no", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "jobs.apple.com", "www.reed.co.uk", "www.flexjobs.com", "www.computerworld.co.nz", "www.reed.co.uk", "www.reed.co.uk", "www.ziprecruiter.com", "jobs.apple.com", "www.ziprecruiter.com", "www.careerbliss.com", "ipa.co.uk", "www.reed.co.uk", "www.indeed.co.uk", "www.net-temps.com", "www.ziprecruiter.com", "www.reed.co.uk", "www.computerworld.dk", "www.bizcommunity.com", "www.flexjobs.com", "www.flexjobs.com", "www.builtincolorado.com", "jobs.newscientist.com", "jobs.theguardian.com", "jobs.lever.co", "www.clearancejobs.com", "www.builtincolorado.com", "www.clearancejobs.com", "www.clearancejobs.com", "jobs.lever.co", "www.builtinchicago.org", "www.clearancejobs.com", "jobs.theguardian.com", "www.builtinchicago.org", "jobs.theguardian.com", "jobs.lever.co", "www.builtinchicago.org", "jobs.lever.co", "www.builtinchicago.org", "www.builtinchicago.org", "www.clearancejobs.com", "www.cv-library.co.uk", "www.cambridgenetwork.co.uk", "moikrug.ru", "careers.insidehighered.com", "www.builtinchicago.org", "www.builtinchicago.org", "www.builtinchicago.org", "jobs.newscientist.com", "www.builtinchicago.org", "spb.hh.ru", "jobs.theguardian.com", "oilvoice.com", "www.builtincolorado.com", "jobs.theguardian.com", "www.cambridgenetwork.co.uk", "www.clearancejobs.com", "www.cv-library.co.uk", "www.pracuj.pl", "www.epmag.com", "boards.greenhouse.io", "boards.greenhouse.io", "spb.hh.ru", "jobs.theguardian.com", "jobs.theguardian.com", "www.clearancejobs.com", "jobs.newscientist.com", "krb-sjobs.brassring.com", "ca.indeed.com", "www.builtincolorado.com", "jobs.theguardian.com", "jobs.theguardian.com", "jobs.theguardian.com", "jobs.theguardian.com", "www.clearancejobs.com", "jobs.theguardian.com", "www.oilvoice.com", "www.builtinchicago.org", "jobs.theguardian.com", "www.thinkspain.com", "jobs.theguardian.com", "jobs.theguardian.com", "jobs.lever.co", "jobs.theguardian.com", "jobs.newscientist.com", "www.builtinchicago.org", "www.oilvoice.com", "www.myvisajobs.com", "jobs.newscientist.com", "www.builtinchicago.org", "jobs.newscientist.com", "www.builtinchicago.org", "www.builtinchicago.org", "www.clearancejobs.com", "www.careerjet.co.uk", "jobs.theguardian.com", "careers.insidehighered.com", "jobs.theguardian.com", "careers.homedepot.com", "jobs.newscientist.com", "www.clearancejobs.com", "jobs.newscientist.com", "jobs.theguardian.com", "www.oilvoice.com", "www.profesia.sk", "jobs.lever.co", "jobs.theguardian.com", "jobs.theguardian.com", "jobs.theguardian.com", "www.builtinchicago.org", "www.builtinchicago.org", "www.builtinchicago.org", "jobs.theguardian.com", "www.clearancejobs.com", "krb-sjobs.brassring.com", "www.level39.co", "www.builtinchicago.org", "jobs.newscientist.com", "jobs.theguardian.com", "jobs.theguardian.com", "www.cambridgenetwork.co.uk", "www.toptal.com", "www.builtincolorado.com", "www.analytictalent.datasciencecentral.com", "www.clearancejobs.com", "careers.insidehighered.com", "jobs.theguardian.com", "jobs.theguardian.com", "www.clearancejobs.com", "jobs.lever.co", "jobs.theguardian.com", "www.builtinchicago.org", "jobs.newscientist.com", "www.clearancejobs.com", "www.oilvoice.com", "jobs.lever.co", "jobs.newscientist.com", "www.builtinchicago.org", "careers.insidehighered.com", "www.builtinchicago.org", "www.clearancejobs.com", "jobs.lever.co", "www.oilvoice.com", "www.clearancejobs.com", "jobs.theguardian.com", "ca.indeed.com", "www.builtinchicago.org", "www.builtinchicago.org", "www.irishjobs.ie", "krb-sjobs.brassring.com", "jobs.theguardian.com", "careers.insidehighered.com", "www.jobserve.com", "boards.greenhouse.io", "www.clearancejobs.com", "www.clearancejobs.com", "www.cv-library.co.uk", "www.classifiedads.com", "diversityjobs.com", "www.builtinla.com", "www.parking-net.com", "jobs.seattletimes.com", "www.nuon.com", "jobs.telegraph.co.uk", "www.careerjet.co.uk", "www.aplitrak.com", "www.careerjet.co.uk", "www.cybercoders.com", "www.godubai.com", "careers.walmart.com", "www.careerjet.co.uk", "www.godubai.com", "www.builtinla.com", "electricenergyonline.com", "careers.neoris.com", "secure2.sophos.com", "www.builtincolorado.com", "jobs.telegraph.co.uk", "www.godubai.com", "illinoisjoblink.illinois.gov", "www.godubai.com", "www.topschooljobs.org", "illinoisjoblink.illinois.gov", "www.indeed.es", "illinoisjoblink.illinois.gov", "www.indeed.es", "www.godubai.com", "www.builtincolorado.com", "www.careerjet.co.uk", "www.builtinla.com", "www.classifiedads.com", "www.godubai.com", "secure2.sophos.com", "secure2.sophos.com", "www.classifiedads.com", "www.iamexpat.nl", "www.hipo.ro", "careers.walmart.com", "www.godubai.com", "www.godubai.com", "www.indeed.es", "www.classifiedads.com", "www.godubai.com", "www.aplitrak.com", "www.parking-net.com", "www.randstad.co.uk", "jobs.telegraph.co.uk", "www.connecticum.de", "www.startupjobs.cz", "www.godubai.com", "www.builtinla.com", "illinoisjoblink.illinois.gov", "ejob.bz", "www.parking-net.com", "www.godubai.com", "www.builtinla.com", "jobs.gamasutra.com", "electricenergyonline.com", "www.indeed.es", "www.indeed.es", "www.godubai.com", "www.randstad.co.uk", "illinoisjoblink.illinois.gov", "marketingevolution.theresumator.com", "www.builtincolorado.com", "www.careerjet.co.uk", "www.builtinla.com", "www.godubai.com", "illinoisjoblink.illinois.gov", "jobs.telegraph.co.uk", "www.builtinla.com", "www.godubai.com", "www.parking-net.com", "www.classifiedads.com", "www.iamexpat.nl", "www.godubai.com", "dasauge.de", "www.classifiedads.com", "careers.walmart.com", "www.xlstat.com", "www.godubai.com", "jobs.telegraph.co.uk", "www.iamexpat.nl", "www.classifiedads.com", "www.careerjet.co.uk", "www.careerjet.co.uk", "www.godubai.com", "jobs.telegraph.co.uk", "www.godubai.com", "careers.walmart.com", "www.godubai.com", "www.iamexpat.nl", "illinoisjoblink.illinois.gov", "www.indeed.es", "diversityjobs.com", "join.irdeto.com", "www.builtinla.com", "illinoisjoblink.illinois.gov", "www.godubai.com", "justjobs.com", "www.builtincolorado.com"]}}; }
plotInterface = buildViz(1000,
600,
null,
null,
false,
false,
false,
false,
false,
true,
false,
false,
true,
0.1,
false,
undefined,
undefined,
getDataAndInfo(),
true,
false,
null,
null,
null,
null,
true,
false,
true,
false,
null,
null,
10,
null,
null,
null,
false,
true,
true,
undefined,
null,
false,
false,
".3f",
".3f",
false,
-1,
true,
false,
true,
false,
false,
false,
true,
null,
null,
null,
false,
null,
undefined,
undefined,
undefined,
undefined,
undefined,
undefined,
undefined,
14,
0);


autocomplete(
    document.getElementById('searchInput'),
    plotInterface.data.map(x => x.term).sort(),
    plotInterface
);

</script>
